name: "no-retriever-hotpotqa"
dataset:
  dataset_dir: partitioned
  description: This dataset is a vast collection of all kind of information that you
    can find on Wikipedia. It can be used, for instance, to retrieve straightforward
    facts from one or more documents, compare two entities based on shared attributes,
    identify relationships, roles, or attributes of entities, reason about dates,
    timelines, or chronological order, determine geographical relationships or locations,
    explain causes or sequences of events or processes, synthesize facts from multiple
    documents to infer answers, and validate or refute premises in the context of
    the question.
  examples_data_path: examples
  grounding_data_path: grounding_data
  load_examples_timeout_s: 3600
  load_grounding_data_timeout_s: 3600
  partition_map:
    holdout: holdout
    sample: sample
    test: test
    train: train
  path_root: octo-syftr-benchmarking-data
  storage_options:
    cache_check: 60
    cache_storage: benchmarking/data/cache
    check_files: true
    expiry_time: 2592000
    protocol: filecache
    same_names: true
    target_protocol: s3
  storage_partitions:
  - sample
  - train
  - test
  - holdout
  subset: train_hard
  xname: hotpotqa_hf
reuse_study: false
optimization:
  baselines: []
  use_individual_baselines: false
  use_agent_baselines: false
  use_variations_of_baselines: false
  shuffle_baselines: false
  blocks:
  - components:
    - few_shot_retriever
    - reranker
    - rag_mode
    - sub_question_rag
    - critique_rag_agent
    - lats_rag_agent
    - react_rag_agent
    - response_synthesizer_llm
    - template_name
    name: no-retriever
    num_trials: 500
    custom_defaults:
      hyde_enabled: false
      additional_context_enabled: true
      rag_method: hybrid
      rag_query_decomposition_enabled: false
      rag_top_k: 19
      rag_embedding_model: avsolatorio/GIST-large-Embedding-v0
      rag_hybrid_bm25_weight: 0.6
      rag_query_decomposition_num_queries: 6
      rag_fusion_mode: dist_based_score
      splitter_method: html
      splitter_chunk_exp: 10
      splitter_chunk_overlap_frac: 0.25
      hyde_llm_name: llama-33-70B
      additional_context_num_nodes: 7
  cpus_per_trial: 5
  # embedding_device: cuda
  # use_hf_embedding_models: true
  gpus_per_trial: 0.2
  max_concurrent_trials: 50
  num_eval_samples: 100
  num_eval_batch: 10  
  num_retries_unique_params: 10
  num_trials: 500

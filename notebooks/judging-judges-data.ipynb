{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getcwd().endswith(\"syftr\"):\n",
    "    os.chdir(os.path.dirname(os.getcwd()))\n",
    "    print(f\"Changed working directory to: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"mode.chained_assignment\", \"raise\")\n",
    "pd.set_option(\"display.max_rows\", 200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from syftr.plotting.insights import load_studies\n",
    "\n",
    "study_names = [\n",
    "    \"judge-data1--test--crag_hf-music--music\",\n",
    "    \"judge-data1--test--financebench_hf\",\n",
    "    \"judge-data1--test--hotpotqa_hf-train_hard--train_hard\",\n",
    "    \"judge-data1--test--multihoprag_hf\",\n",
    "]\n",
    "df, study_stats_table, exceptions_table = load_studies(study_names, only_successful_trials=False)\n",
    "display(df.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0]['user_attrs_metric_eval_results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the QAR triplet and judge scores from each QA pair from each row\n",
    "records = [\n",
    "    record\n",
    "    for row in df['user_attrs_metric_eval_results']\n",
    "    for record in row.values()\n",
    "]\n",
    "\n",
    "new_df = pd.DataFrame(records)\n",
    "\n",
    "new_df = new_df.rename(columns={\"passing\": \"gpt-4o-passing\", \"raw_score\": \"gpt-4o-raw-score\"})[[\"question\", \"answer\", \"response\", \"gpt-4o-passing\", \"gpt-4o-raw-score\"]]\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "new_df = shuffle(new_df).reset_index(drop=True)\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv(\"/tmp/qars.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_df = pd.read_csv(\"/home/alex/Downloads/Human Eval Redux - qars (5).csv\")\n",
    "labeled_df = labeled_df[~pd.isna(labeled_df['Human Score'])]\n",
    "labeled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "qar_template = \"\"\"\n",
    "## User Query\n",
    "{query}\n",
    "\n",
    "## Reference Answer\n",
    "{reference_answer}\n",
    "\n",
    "## Generated Answer\n",
    "{generated_answer}\n",
    "\"\"\"\n",
    "labeled_df['qar_prompt'] = labeled_df.apply(\n",
    "    lambda row: qar_template.format(\n",
    "        query=row.question,\n",
    "        reference_answer=row.answer,\n",
    "        generated_answer=row.response,\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "labeled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_df.to_csv(\"qars_labeled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.Dataset.from_pandas(labeled_df)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.push_to_hub(repo_id='DataRobot-Research/judge-eval', commit_message=\"create datasets dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

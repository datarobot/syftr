{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# QA Dataset Generation\n",
    "Given a raw text, the notebook helps to generate a custom HuggingFace QA dataset based on the given information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.core import ultratb\n",
    "\n",
    "ultratb.VerboseTB.tb_highlight = \"bg:#3e0054\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed working directory to: /Users/debadeepta.dey/sources/syftr\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if not os.getcwd().endswith(\"syftr\"):\n",
    "    os.chdir(os.path.dirname(os.getcwd()))\n",
    "    print(f\"Changed working directory to: {os.getcwd()}\")\n",
    "\n",
    "from syftr.configuration import cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Hugging Face token: hf_a...SUmb\n"
     ]
    }
   ],
   "source": [
    "DATA_FILEPATH = \"/Users/debadeepta.dey/datasets/barclays/rise-insights-report-making-data-count-with-ai-DIGITAL.md\"  # Path to the raw text file\n",
    "QA_PAIRS_FILEPATH = \"/Users/debadeepta.dey/datasets/barclays/rise-insights-report-making-data-count-with-ai-DIGITAL-qapairs.json\"  # Path to the QA pairs file\n",
    "CHUNK_SIZE = 8148  # Size of each text chunk\n",
    "\n",
    "# Provide a valid dataset name\n",
    "DATASET_NAME = \"making-data-count-with-ai-2\"\n",
    "assert DATASET_NAME, \"Please set the DATASET_NAME variable to a valid dataset name.\"\n",
    "# -------------------------------------------------------------------------------------------\n",
    "\n",
    "DATASET_IS_PRIVATE = True  # Set to False if you want to share the dataset publicly\n",
    "\n",
    "HF_DATASET_NAME = f\"DataRobot-Research/{DATASET_NAME}\"  # Adjust name of the dataset on Hugging Face Hub\n",
    "HF_TOKEN = cfg.hf_datasets.api_key.get_secret_value()  # Get Hugging Face token from configuration\n",
    "\n",
    "assert HF_TOKEN, \"Please set the HF_TOKEN environment variable with your Hugging Face token.\"\n",
    "\n",
    "print(f\"Using Hugging Face token: {HF_TOKEN[:4]}...{HF_TOKEN[-4:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text(file_path: str) -> str:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        return file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 122287 characters from /Users/debadeepta.dey/datasets/barclays/rise-insights-report-making-data-count-with-ai-DIGITAL.md\n"
     ]
    }
   ],
   "source": [
    "raw_text = load_text(DATA_FILEPATH)\n",
    "print(f\"Loaded {len(raw_text)} characters from {DATA_FILEPATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text: str, chunk_size: int = 1000) -> list:\n",
    "    return [text[i : i + chunk_size] for i in range(0, len(text), chunk_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 60 chunks of size 2048 characters.\n"
     ]
    }
   ],
   "source": [
    "chunks = chunk_text(raw_text, CHUNK_SIZE)\n",
    "print(f\"Created {len(chunks)} chunks of size {CHUNK_SIZE} characters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56dff09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'question': 'What is the title of the article written by the CEO of ProGrad?', 'answer': 'The title of the article is \"Improved credit decisions for the unbanked and Gen-Z\". The author, Ethan Fraenkel, is the Co-founder and CEO of ProGrad.'}, {'question': 'Which company, featured in a case study, uses a proprietary Spoken Language Understanding (SLU) engine?', 'answer': 'PolyAI, featured in a case study, used a proprietary Spoken Language Understanding (SLU) engine to help BP with its contact centers.'}, {'question': 'What is the full name of the author who is the CEO & Director of Level E Research?', 'answer': 'Dr. Sonia Schulenburg is the CEO & Director of Level E Research.'}]\n"
     ]
    }
   ],
   "source": [
    "# Load QA pairs from the JSON file\n",
    "import json\n",
    "def load_qa_pairs(file_path: str) -> list:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        return json.load(file)\n",
    "\n",
    "qa_pairs = load_qa_pairs(QA_PAIRS_FILEPATH)\n",
    "print(qa_pairs[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "**Adjust the parameters to make a custom split based on your needs and the amount of data generated.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import typing as T\n",
    "\n",
    "def get_context(chunks: T.List[str]) -> str:\n",
    "    full_context = \"\\n\".join(chunks)\n",
    "    return full_context\n",
    "\n",
    "def prepare_hf_data(\n",
    "        qa_pairs: T.List[T.Dict[str, str]], \n",
    "        chunks: T.List[str] | None = None, \n",
    "        all_grounding_data_for_each_partition = True,\n",
    ") -> T.Tuple[datasets.DatasetDict, datasets.DatasetDict]:\n",
    "    if all_grounding_data_for_each_partition:\n",
    "        grounding_data_train = chunks\n",
    "        grounding_data_test = chunks\n",
    "        grounding_data_holdout = chunks\n",
    "        grounding_data_sample = chunks[:5]\n",
    "    elif chunks:\n",
    "        grounding_data_train = get_context(chunks[:100])\n",
    "        grounding_data_test = get_context(chunks[100:200])\n",
    "        grounding_data_holdout = get_context(chunks[200:])\n",
    "        grounding_data_sample = get_context(chunks[:5])\n",
    "    else:\n",
    "        raise ValueError(\"Either chunks or raw_text must be provided.\")\n",
    "    \n",
    "    qa_data = datasets.DatasetDict(\n",
    "        {\n",
    "            \"train\": datasets.Dataset.from_list(qa_pairs[:50]),\n",
    "            \"test\": datasets.Dataset.from_list(qa_pairs[50:180]),\n",
    "            \"holdout\": datasets.Dataset.from_list(qa_pairs[180:]),\n",
    "            \"sample\": datasets.Dataset.from_list(qa_pairs[:5]),  # for quick testing\n",
    "        }\n",
    "    )\n",
    "    grounding_data = datasets.DatasetDict(\n",
    "        {\n",
    "            \"train\": datasets.Dataset.from_dict({\"text\": grounding_data_train}),\n",
    "            \"test\": datasets.Dataset.from_dict({\"text\": grounding_data_test}),\n",
    "            \"holdout\": datasets.Dataset.from_dict({\"text\": grounding_data_holdout}),\n",
    "            \"sample\": datasets.Dataset.from_dict({\"text\": grounding_data_sample}),\n",
    "        }\n",
    "    )\n",
    "    return qa_data, grounding_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m qa_data, grounding_data = prepare_hf_data(qa_pairs, chunks=chunks)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[48;5;53mqa_data\u001b[49m.push_to_hub(\n\u001b[32m      4\u001b[39m     repo_id=HF_DATASET_NAME, \n\u001b[32m      5\u001b[39m     data_dir=\u001b[33m\"\u001b[39m\u001b[33mexamples\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      6\u001b[39m     private=DATASET_IS_PRIVATE, \n\u001b[32m      7\u001b[39m     token=HF_TOKEN,\n\u001b[32m      8\u001b[39m     config_name=\u001b[33m\"\u001b[39m\u001b[33mqa\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      9\u001b[39m )\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mQA data pushed to Hugging Face Hub.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m grounding_data.push_to_hub(\n\u001b[32m     13\u001b[39m     repo_id=HF_DATASET_NAME,\n\u001b[32m     14\u001b[39m     data_dir=\u001b[33m\"\u001b[39m\u001b[33mgrounding_data\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m     config_name=\u001b[33m\"\u001b[39m\u001b[33mgrounding\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     18\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<stringsource>:69\u001b[39m, in \u001b[36mcfunc.to_py.__Pyx_CFunc_b0409f__29_pydevd_sys_monitoring_cython_object__lParen__etc_to_py_4code_4line.wrap\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_pydevd_sys_monitoring\\\\_pydevd_sys_monitoring_cython.pyx:1470\u001b[39m, in \u001b[36m_pydevd_sys_monitoring_cython._line_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_pydevd_sys_monitoring\\\\_pydevd_sys_monitoring_cython.pyx:1512\u001b[39m, in \u001b[36m_pydevd_sys_monitoring_cython._internal_line_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_pydevd_sys_monitoring\\\\_pydevd_sys_monitoring_cython.pyx:1313\u001b[39m, in \u001b[36m_pydevd_sys_monitoring_cython._stop_on_breakpoint\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_pydevd_sys_monitoring\\\\_pydevd_sys_monitoring_cython.pyx:1950\u001b[39m, in \u001b[36m_pydevd_sys_monitoring_cython._do_wait_suspend\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sources/syftr/.venv/lib/python3.12/site-packages/debugpy/_vendored/pydevd/pydevd.py:2188\u001b[39m, in \u001b[36mPyDB.do_wait_suspend\u001b[39m\u001b[34m(self, thread, frame, event, arg, exception_type)\u001b[39m\n\u001b[32m   2185\u001b[39m             from_this_thread.append(frame_custom_thread_id)\n\u001b[32m   2187\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._threads_suspended_single_notification.notify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[32m-> \u001b[39m\u001b[32m2188\u001b[39m         keep_suspended = \u001b[38;5;28;48;5;53mself\u001b[39;49m\u001b[48;5;53m.\u001b[49m\u001b[48;5;53m_do_wait_suspend\u001b[49m\u001b[48;5;53m(\u001b[49m\u001b[48;5;53mthread\u001b[49m\u001b[48;5;53m,\u001b[49m\u001b[48;5;53m \u001b[49m\u001b[48;5;53mframe\u001b[49m\u001b[48;5;53m,\u001b[49m\u001b[48;5;53m \u001b[49m\u001b[48;5;53mevent\u001b[49m\u001b[48;5;53m,\u001b[49m\u001b[48;5;53m \u001b[49m\u001b[48;5;53marg\u001b[49m\u001b[48;5;53m,\u001b[49m\u001b[48;5;53m \u001b[49m\u001b[48;5;53mtrace_suspend_type\u001b[49m\u001b[48;5;53m,\u001b[49m\u001b[48;5;53m \u001b[49m\u001b[48;5;53mfrom_this_thread\u001b[49m\u001b[48;5;53m,\u001b[49m\u001b[48;5;53m \u001b[49m\u001b[48;5;53mframes_tracker\u001b[49m\u001b[48;5;53m)\u001b[49m\n\u001b[32m   2190\u001b[39m frames_list = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2192\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[32m   2193\u001b[39m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sources/syftr/.venv/lib/python3.12/site-packages/debugpy/_vendored/pydevd/pydevd.py:2257\u001b[39m, in \u001b[36mPyDB._do_wait_suspend\u001b[39m\u001b[34m(self, thread, frame, event, arg, trace_suspend_type, from_this_thread, frames_tracker)\u001b[39m\n\u001b[32m   2254\u001b[39m                 queue.put(internal_cmd)\n\u001b[32m   2255\u001b[39m                 wait_timeout = TIMEOUT_FAST\n\u001b[32m-> \u001b[39m\u001b[32m2257\u001b[39m         \u001b[48;5;53mnotify_event\u001b[49m\u001b[48;5;53m.\u001b[49m\u001b[48;5;53mwait\u001b[49m\u001b[48;5;53m(\u001b[49m\u001b[48;5;53mwait_timeout\u001b[49m\u001b[48;5;53m)\u001b[49m\n\u001b[32m   2258\u001b[39m         notify_event.clear()\n\u001b[32m   2260\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.7-macos-aarch64-none/lib/python3.12/threading.py:655\u001b[39m, in \u001b[36mEvent.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    653\u001b[39m signaled = \u001b[38;5;28mself\u001b[39m._flag\n\u001b[32m    654\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[32m--> \u001b[39m\u001b[32m655\u001b[39m     signaled = \u001b[38;5;28;48;5;53mself\u001b[39;49m\u001b[48;5;53m.\u001b[49m\u001b[48;5;53m_cond\u001b[49m\u001b[48;5;53m.\u001b[49m\u001b[48;5;53mwait\u001b[49m\u001b[48;5;53m(\u001b[49m\u001b[48;5;53mtimeout\u001b[49m\u001b[48;5;53m)\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.7-macos-aarch64-none/lib/python3.12/threading.py:359\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m         gotit = \u001b[48;5;53mwaiter\u001b[49m\u001b[48;5;53m.\u001b[49m\u001b[48;5;53macquire\u001b[49m\u001b[48;5;53m(\u001b[49m\u001b[38;5;28;48;5;53;01mTrue\u001b[39;49;00m\u001b[48;5;53m,\u001b[49m\u001b[48;5;53m \u001b[49m\u001b[48;5;53mtimeout\u001b[49m\u001b[48;5;53m)\u001b[49m\n\u001b[32m    360\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    361\u001b[39m         gotit = waiter.acquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "qa_data, grounding_data = prepare_hf_data(qa_pairs, chunks=chunks)\n",
    "\n",
    "qa_data.push_to_hub(\n",
    "    repo_id=HF_DATASET_NAME, \n",
    "    data_dir=\"examples\",\n",
    "    private=DATASET_IS_PRIVATE, \n",
    "    token=HF_TOKEN,\n",
    "    config_name=\"qa\"\n",
    ")\n",
    "print(f\"QA data pushed to Hugging Face Hub.\")\n",
    "\n",
    "grounding_data.push_to_hub(\n",
    "    repo_id=HF_DATASET_NAME,\n",
    "    data_dir=\"grounding_data\",\n",
    "    private=DATASET_IS_PRIVATE,\n",
    "    token=HF_TOKEN,\n",
    "    config_name=\"grounding\"\n",
    ")\n",
    "print(f\"Grounding data pushed to Hugging Face Hub.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "syftr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

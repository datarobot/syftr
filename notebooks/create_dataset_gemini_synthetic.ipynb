{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# QA Dataset Generation\n",
    "Given a raw text, the notebook helps to generate a custom HuggingFace QA dataset based on the given information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.core import ultratb\n",
    "\n",
    "ultratb.VerboseTB.tb_highlight = \"bg:#3e0054\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getcwd().endswith(\"syftr\"):\n",
    "    os.chdir(os.path.dirname(os.getcwd()))\n",
    "    print(f\"Changed working directory to: {os.getcwd()}\")\n",
    "\n",
    "from syftr.configuration import cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILEPATH = \"/Users/debadeepta.dey/datasets/barclays/rise-insights-report-making-data-count-with-ai-DIGITAL.md\"  # Path to the raw text file\n",
    "QA_PAIRS_FILEPATH = \"/Users/debadeepta.dey/datasets/barclays/rise-insights-report-making-data-count-with-ai-DIGITAL-qapairs.json\"  # Path to the QA pairs file\n",
    "CHUNK_SIZE = 8148  # Size of each text chunk\n",
    "\n",
    "# Provide a valid dataset name\n",
    "DATASET_NAME = \"making-data-count-with-ai-2\"\n",
    "assert DATASET_NAME, \"Please set the DATASET_NAME variable to a valid dataset name.\"\n",
    "# -------------------------------------------------------------------------------------------\n",
    "\n",
    "DATASET_IS_PRIVATE = True  # Set to False if you want to share the dataset publicly\n",
    "\n",
    "HF_DATASET_NAME = f\"DataRobot-Research/{DATASET_NAME}\"  # Adjust name of the dataset on Hugging Face Hub\n",
    "HF_TOKEN = cfg.hf_datasets.api_key.get_secret_value()  # Get Hugging Face token from configuration\n",
    "\n",
    "assert HF_TOKEN, \"Please set the HF_TOKEN environment variable with your Hugging Face token.\"\n",
    "\n",
    "print(f\"Using Hugging Face token: {HF_TOKEN[:4]}...{HF_TOKEN[-4:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text(file_path: str) -> str:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        return file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = load_text(DATA_FILEPATH)\n",
    "print(f\"Loaded {len(raw_text)} characters from {DATA_FILEPATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text: str, chunk_size: int = 1000) -> list:\n",
    "    return [text[i : i + chunk_size] for i in range(0, len(text), chunk_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = chunk_text(raw_text, CHUNK_SIZE)\n",
    "print(f\"Created {len(chunks)} chunks of size {CHUNK_SIZE} characters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load QA pairs from the JSON file\n",
    "import json\n",
    "def load_qa_pairs(file_path: str) -> list:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        return json.load(file)\n",
    "\n",
    "qa_pairs = load_qa_pairs(QA_PAIRS_FILEPATH)\n",
    "print(qa_pairs[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "**Adjust the parameters to make a custom split based on your needs and the amount of data generated.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import typing as T\n",
    "\n",
    "def get_context(chunks: T.List[str]) -> str:\n",
    "    full_context = \"\\n\".join(chunks)\n",
    "    return full_context\n",
    "\n",
    "def prepare_hf_data(\n",
    "        qa_pairs: T.List[T.Dict[str, str]], \n",
    "        chunks: T.List[str] | None = None, \n",
    "        all_grounding_data_for_each_partition = True,\n",
    ") -> T.Tuple[datasets.DatasetDict, datasets.DatasetDict]:\n",
    "    if all_grounding_data_for_each_partition:\n",
    "        grounding_data_train = chunks\n",
    "        grounding_data_test = chunks\n",
    "        grounding_data_holdout = chunks\n",
    "        grounding_data_sample = chunks[:5]\n",
    "    elif chunks:\n",
    "        grounding_data_train = get_context(chunks[:100])\n",
    "        grounding_data_test = get_context(chunks[100:200])\n",
    "        grounding_data_holdout = get_context(chunks[200:])\n",
    "        grounding_data_sample = get_context(chunks[:5])\n",
    "    else:\n",
    "        raise ValueError(\"Either chunks or raw_text must be provided.\")\n",
    "    \n",
    "    qa_data = datasets.DatasetDict(\n",
    "        {\n",
    "            \"train\": datasets.Dataset.from_list(qa_pairs[:50]),\n",
    "            \"test\": datasets.Dataset.from_list(qa_pairs[50:180]),\n",
    "            \"holdout\": datasets.Dataset.from_list(qa_pairs[180:]),\n",
    "            \"sample\": datasets.Dataset.from_list(qa_pairs[:5]),  # for quick testing\n",
    "        }\n",
    "    )\n",
    "    grounding_data = datasets.DatasetDict(\n",
    "        {\n",
    "            \"train\": datasets.Dataset.from_dict({\"text\": grounding_data_train}),\n",
    "            \"test\": datasets.Dataset.from_dict({\"text\": grounding_data_test}),\n",
    "            \"holdout\": datasets.Dataset.from_dict({\"text\": grounding_data_holdout}),\n",
    "            \"sample\": datasets.Dataset.from_dict({\"text\": grounding_data_sample}),\n",
    "        }\n",
    "    )\n",
    "    return qa_data, grounding_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_data, grounding_data = prepare_hf_data(qa_pairs, chunks=chunks)\n",
    "\n",
    "qa_data.push_to_hub(\n",
    "    repo_id=HF_DATASET_NAME, \n",
    "    data_dir=\"examples\",\n",
    "    private=DATASET_IS_PRIVATE, \n",
    "    token=HF_TOKEN,\n",
    "    config_name=\"qa\"\n",
    ")\n",
    "print(f\"QA data pushed to Hugging Face Hub.\")\n",
    "\n",
    "grounding_data.push_to_hub(\n",
    "    repo_id=HF_DATASET_NAME,\n",
    "    data_dir=\"grounding_data\",\n",
    "    private=DATASET_IS_PRIVATE,\n",
    "    token=HF_TOKEN,\n",
    "    config_name=\"grounding\"\n",
    ")\n",
    "print(f\"Grounding data pushed to Hugging Face Hub.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "syftr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

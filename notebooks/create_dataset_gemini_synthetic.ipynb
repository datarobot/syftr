{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# QA Dataset Generation\n",
    "Given a raw text, the notebook helps to generate a custom HuggingFace QA dataset based on the given information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.core import ultratb\n",
    "\n",
    "ultratb.VerboseTB.tb_highlight = \"bg:#3e0054\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed working directory to: /Users/debadeepta.dey/sources/syftr\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if not os.getcwd().endswith(\"syftr\"):\n",
    "    os.chdir(os.path.dirname(os.getcwd()))\n",
    "    print(f\"Changed working directory to: {os.getcwd()}\")\n",
    "\n",
    "from syftr.configuration import cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Hugging Face token: hf_a...SUmb\n"
     ]
    }
   ],
   "source": [
    "DATA_FILEPATH = \"/Users/debadeepta.dey/datasets/barclays/rise-insights-report-making-data-count-with-ai-DIGITAL.md\"  # Path to the raw text file\n",
    "QA_PAIRS_FILEPATH = \"/Users/debadeepta.dey/datasets/barclays/rise-insights-report-making-data-count-with-ai-DIGITAL-qapairs.json\"  # Path to the QA pairs file\n",
    "CHUNK_SIZE = 8148  # Size of each text chunk\n",
    "\n",
    "# Provide a valid dataset name\n",
    "DATASET_NAME = \"making-data-count-with-ai-2\"\n",
    "assert DATASET_NAME, \"Please set the DATASET_NAME variable to a valid dataset name.\"\n",
    "# -------------------------------------------------------------------------------------------\n",
    "\n",
    "DATASET_IS_PRIVATE = True  # Set to False if you want to share the dataset publicly\n",
    "\n",
    "HF_DATASET_NAME = f\"DataRobot-Research/{DATASET_NAME}\"  # Adjust name of the dataset on Hugging Face Hub\n",
    "HF_TOKEN = cfg.hf_datasets.api_key.get_secret_value()  # Get Hugging Face token from configuration\n",
    "\n",
    "assert HF_TOKEN, \"Please set the HF_TOKEN environment variable with your Hugging Face token.\"\n",
    "\n",
    "print(f\"Using Hugging Face token: {HF_TOKEN[:4]}...{HF_TOKEN[-4:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text(file_path: str) -> str:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        return file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 122287 characters from /Users/debadeepta.dey/datasets/barclays/rise-insights-report-making-data-count-with-ai-DIGITAL.md\n"
     ]
    }
   ],
   "source": [
    "raw_text = load_text(DATA_FILEPATH)\n",
    "print(f\"Loaded {len(raw_text)} characters from {DATA_FILEPATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text: str, chunk_size: int = 1000) -> list:\n",
    "    return [text[i : i + chunk_size] for i in range(0, len(text), chunk_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 16 chunks of size 8148 characters.\n"
     ]
    }
   ],
   "source": [
    "chunks = chunk_text(raw_text, CHUNK_SIZE)\n",
    "print(f\"Created {len(chunks)} chunks of size {CHUNK_SIZE} characters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56dff09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'question': 'What is the title of the article written by the CEO of ProGrad?', 'answer': 'The title of the article is \"Improved credit decisions for the unbanked and Gen-Z\". The author, Ethan Fraenkel, is the Co-founder and CEO of ProGrad.'}, {'question': 'Which company, featured in a case study, uses a proprietary Spoken Language Understanding (SLU) engine?', 'answer': 'PolyAI, featured in a case study, used a proprietary Spoken Language Understanding (SLU) engine to help BP with its contact centers.'}, {'question': 'What is the full name of the author who is the CEO & Director of Level E Research?', 'answer': 'Dr. Sonia Schulenburg is the CEO & Director of Level E Research.'}]\n"
     ]
    }
   ],
   "source": [
    "# Load QA pairs from the JSON file\n",
    "import json\n",
    "def load_qa_pairs(file_path: str) -> list:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        return json.load(file)\n",
    "\n",
    "qa_pairs = load_qa_pairs(QA_PAIRS_FILEPATH)\n",
    "print(qa_pairs[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "**Adjust the parameters to make a custom split based on your needs and the amount of data generated.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import typing as T\n",
    "\n",
    "def get_context(chunks: T.List[str]) -> str:\n",
    "    full_context = \"\\n\".join(chunks)\n",
    "    return full_context\n",
    "\n",
    "def prepare_hf_data(\n",
    "        qa_pairs: T.List[T.Dict[str, str]], \n",
    "        chunks: T.List[str] | None = None, \n",
    "        all_grounding_data_for_each_partition = True,\n",
    ") -> T.Tuple[datasets.DatasetDict, datasets.DatasetDict]:\n",
    "    if all_grounding_data_for_each_partition:\n",
    "        grounding_data_train = chunks\n",
    "        grounding_data_test = chunks\n",
    "        grounding_data_holdout = chunks\n",
    "        grounding_data_sample = chunks[:5]\n",
    "    elif chunks:\n",
    "        grounding_data_train = get_context(chunks[:100])\n",
    "        grounding_data_test = get_context(chunks[100:200])\n",
    "        grounding_data_holdout = get_context(chunks[200:])\n",
    "        grounding_data_sample = get_context(chunks[:5])\n",
    "    else:\n",
    "        raise ValueError(\"Either chunks or raw_text must be provided.\")\n",
    "    \n",
    "    qa_data = datasets.DatasetDict(\n",
    "        {\n",
    "            \"train\": datasets.Dataset.from_list(qa_pairs[:50]),\n",
    "            \"test\": datasets.Dataset.from_list(qa_pairs[50:180]),\n",
    "            \"holdout\": datasets.Dataset.from_list(qa_pairs[180:]),\n",
    "            \"sample\": datasets.Dataset.from_list(qa_pairs[:5]),  # for quick testing\n",
    "        }\n",
    "    )\n",
    "    grounding_data = datasets.DatasetDict(\n",
    "        {\n",
    "            \"train\": datasets.Dataset.from_dict({\"text\": grounding_data_train}),\n",
    "            \"test\": datasets.Dataset.from_dict({\"text\": grounding_data_test}),\n",
    "            \"holdout\": datasets.Dataset.from_dict({\"text\": grounding_data_holdout}),\n",
    "            \"sample\": datasets.Dataset.from_dict({\"text\": grounding_data_sample}),\n",
    "        }\n",
    "    )\n",
    "    return qa_data, grounding_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "163382a15bc74f389b383460ab3ad5ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01273d828e034edeb8005d54723d0e9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c44d108ad41647c88e6a62c4e330e191",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c510335edc143e0b136991f0bb3099f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd20d424cc7946eb88474c9bd200cf03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b14a987ac36f4fffa00631c7e1f73fa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2362d5163844a68839ab7db04b358e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10d7b585c5914b248081f7adfcbc6429",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QA data pushed to Hugging Face Hub.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "739cb09ef5ca4f959345d1354a9051a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "006ea5e737f24f44a878f7a4dfddce41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4650839d8d44fb58692ac81bf3310fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79b6e4cd875e4505ad76b096103cde75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20761e24b25b4c358f9563f78ca70244",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ffa4501bcb74dec907ccdc44046ee37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58bce0491b174daba447a26ace54323c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54382533fb07411a95e3b9452e2e2791",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "657e7ad8cada4312bbc4f5539f94f477",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/630 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grounding data pushed to Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "qa_data, grounding_data = prepare_hf_data(qa_pairs, chunks=chunks)\n",
    "\n",
    "qa_data.push_to_hub(\n",
    "    repo_id=HF_DATASET_NAME, \n",
    "    data_dir=\"examples\",\n",
    "    private=DATASET_IS_PRIVATE, \n",
    "    token=HF_TOKEN,\n",
    "    config_name=\"qa\"\n",
    ")\n",
    "print(f\"QA data pushed to Hugging Face Hub.\")\n",
    "\n",
    "grounding_data.push_to_hub(\n",
    "    repo_id=HF_DATASET_NAME,\n",
    "    data_dir=\"grounding_data\",\n",
    "    private=DATASET_IS_PRIVATE,\n",
    "    token=HF_TOKEN,\n",
    "    config_name=\"grounding\"\n",
    ")\n",
    "print(f\"Grounding data pushed to Hugging Face Hub.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "syftr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

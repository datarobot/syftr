{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ee3a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, MarkdownHeaderTextSplitter\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "\n",
    "path = \"/Users/debadeepta.dey/datasets/barclays\"\n",
    "loader = DirectoryLoader(path, glob=\"**/*.md\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61bc9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Split by Markdown Headers (Most intelligent for markdown)\n",
    "# This preserves the document structure and creates logical chunks\n",
    "\n",
    "def split_markdown_by_headers(document_content):\n",
    "    \"\"\"\n",
    "    Split markdown document by headers, preserving document structure\n",
    "    \"\"\"\n",
    "    # Define headers to split on (from h1 to h3)\n",
    "    headers_to_split_on = [\n",
    "        (\"#\", \"Header 1\"),\n",
    "        (\"##\", \"Header 2\"), \n",
    "        (\"###\", \"Header 3\"),\n",
    "    ]\n",
    "    \n",
    "    # Create the markdown header text splitter\n",
    "    markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "        headers_to_split_on=headers_to_split_on,\n",
    "        strip_headers=False  # Keep headers in the chunks\n",
    "    )\n",
    "    \n",
    "    # Split the document\n",
    "    md_header_splits = markdown_splitter.split_text(document_content)\n",
    "    \n",
    "    return md_header_splits\n",
    "\n",
    "# Example usage with your loaded documents\n",
    "if docs:\n",
    "    # Take the first document as example\n",
    "    first_doc = docs[0]\n",
    "    header_splits = split_markdown_by_headers(first_doc.page_content)\n",
    "    \n",
    "    print(f\"Original document split into {len(header_splits)} chunks based on headers\")\n",
    "    \n",
    "    # Display first few chunks\n",
    "    for i, chunk in enumerate(header_splits[:3]):\n",
    "        print(f\"\\n--- Chunk {i+1} ---\")\n",
    "        print(f\"Content: {chunk.page_content[:200]}...\")\n",
    "        print(f\"Metadata: {chunk.metadata}\")\n",
    "        print(f\"Full length: {len(chunk.page_content)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c057368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Recursive Character Text Splitter (Good fallback)\n",
    "# This method is useful when documents don't have clear header structure\n",
    "\n",
    "def split_markdown_recursive(document_content, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Split markdown using recursive character splitter with markdown-aware separators\n",
    "    \"\"\"\n",
    "    # Define separators that work well for markdown\n",
    "    markdown_separators = [\n",
    "        \"\\n\\n\",  # Double newline (paragraph breaks)\n",
    "        \"\\n\",    # Single newline\n",
    "        \" \",     # Space\n",
    "        \"\"       # Character level\n",
    "    ]\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=markdown_separators,\n",
    "        length_function=len,\n",
    "    )\n",
    "    \n",
    "    # Split the document\n",
    "    chunks = text_splitter.split_text(document_content)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Example usage\n",
    "if docs:\n",
    "    first_doc = docs[0]\n",
    "    recursive_chunks = split_markdown_recursive(\n",
    "        first_doc.page_content, \n",
    "        chunk_size=2048,  # Adjust based on your needs\n",
    "        chunk_overlap=200\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nRecursive splitting created {len(recursive_chunks)} chunks\")\n",
    "    \n",
    "    # Display first few chunks\n",
    "    for i, chunk in enumerate(recursive_chunks[:3]):\n",
    "        print(f\"\\n--- Recursive Chunk {i+1} ---\")\n",
    "        print(f\"Content: {chunk[:200]}...\")\n",
    "        print(f\"Length: {len(chunk)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fc45fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 3: Hybrid Approach (Recommended)\n",
    "# Combine header-based splitting with recursive splitting for optimal results\n",
    "\n",
    "def smart_markdown_split(document_content, max_chunk_size=1500, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Smart markdown splitting that combines header-based and recursive approaches\n",
    "    \"\"\"\n",
    "    from langchain.schema import Document\n",
    "    \n",
    "    # First, try to split by headers\n",
    "    headers_to_split_on = [\n",
    "        (\"#\", \"Header 1\"),\n",
    "        (\"##\", \"Header 2\"), \n",
    "        (\"###\", \"Header 3\"),\n",
    "        (\"####\", \"Header 4\"),\n",
    "    ]\n",
    "    \n",
    "    markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "        headers_to_split_on=headers_to_split_on,\n",
    "        strip_headers=False\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Split by headers first\n",
    "        header_splits = markdown_splitter.split_text(document_content)\n",
    "        \n",
    "        # If header splits are too large, further split them recursively\n",
    "        final_chunks = []\n",
    "        \n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=max_chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "            length_function=len,\n",
    "        )\n",
    "        \n",
    "        for doc in header_splits:\n",
    "            if len(doc.page_content) > max_chunk_size:\n",
    "                # Split large chunks further\n",
    "                sub_chunks = text_splitter.split_text(doc.page_content)\n",
    "                for i, sub_chunk in enumerate(sub_chunks):\n",
    "                    # Preserve metadata from header splitting\n",
    "                    new_metadata = doc.metadata.copy()\n",
    "                    new_metadata['sub_chunk'] = i\n",
    "                    final_chunks.append(Document(\n",
    "                        page_content=sub_chunk,\n",
    "                        metadata=new_metadata\n",
    "                    ))\n",
    "            else:\n",
    "                final_chunks.append(doc)\n",
    "                \n",
    "        return final_chunks\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Header splitting failed: {e}\")\n",
    "        # Fallback to recursive splitting\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=max_chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "        )\n",
    "        chunks = text_splitter.split_text(document_content)\n",
    "        return [Document(page_content=chunk, metadata={}) for chunk in chunks]\n",
    "\n",
    "# Example usage with the hybrid approach\n",
    "if docs:\n",
    "    first_doc = docs[0]\n",
    "    smart_chunks = smart_markdown_split(\n",
    "        first_doc.page_content,\n",
    "        max_chunk_size=1200,\n",
    "        chunk_overlap=150\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nSmart splitting created {len(smart_chunks)} chunks\")\n",
    "    \n",
    "    # Display statistics\n",
    "    chunk_lengths = [len(chunk.page_content) for chunk in smart_chunks]\n",
    "    print(f\"Average chunk length: {sum(chunk_lengths) / len(chunk_lengths):.0f} characters\")\n",
    "    print(f\"Min chunk length: {min(chunk_lengths)} characters\")\n",
    "    print(f\"Max chunk length: {max(chunk_lengths)} characters\")\n",
    "    \n",
    "    # Display first few chunks with metadata\n",
    "    for i, chunk in enumerate(smart_chunks[:3]):\n",
    "        print(f\"\\n--- Smart Chunk {i+1} ---\")\n",
    "        print(f\"Metadata: {chunk.metadata}\")\n",
    "        print(f\"Content preview: {chunk.page_content[:200]}...\")\n",
    "        print(f\"Length: {len(chunk.page_content)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7956566",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "# Utility function to process all your documents\n",
    "def process_all_documents(docs, output_method='smart', **kwargs):\n",
    "    \"\"\"\n",
    "    Process all loaded documents and return chunks\n",
    "    \n",
    "    Args:\n",
    "        docs: List of loaded documents\n",
    "        output_method: 'header', 'recursive', or 'smart'\n",
    "        **kwargs: Additional parameters for the splitting methods\n",
    "    \n",
    "    Returns:\n",
    "        List of all chunks with source document information\n",
    "    \"\"\"\n",
    "    all_chunks = []\n",
    "    \n",
    "    for doc_idx, doc in enumerate(docs):\n",
    "        print(f\"Processing document {doc_idx + 1}/{len(docs)}: {doc.metadata.get('source', 'unknown')}\")\n",
    "        \n",
    "        if output_method == 'header':\n",
    "            chunks = split_markdown_by_headers(doc.page_content)\n",
    "        elif output_method == 'recursive':\n",
    "            chunk_texts = split_markdown_recursive(doc.page_content, **kwargs)\n",
    "            chunks = [Document(page_content=text, metadata=doc.metadata.copy()) for text in chunk_texts]\n",
    "        elif output_method == 'smart':\n",
    "            chunks = smart_markdown_split(doc.page_content, **kwargs)\n",
    "        else:\n",
    "            raise ValueError(\"output_method must be 'header', 'recursive', or 'smart'\")\n",
    "        \n",
    "        # Add source document information to each chunk\n",
    "        for chunk_idx, chunk in enumerate(chunks):\n",
    "            chunk.metadata['source_doc_index'] = doc_idx\n",
    "            chunk.metadata['chunk_index'] = chunk_idx\n",
    "            chunk.metadata['original_source'] = doc.metadata.get('source', 'unknown')\n",
    "            all_chunks.append(chunk)\n",
    "    \n",
    "    return all_chunks\n",
    "\n",
    "# Process all your documents using the smart method\n",
    "all_processed_chunks = process_all_documents(\n",
    "    docs, \n",
    "    output_method='smart',  # Change to 'header' or 'recursive' if preferred\n",
    "    max_chunk_size=2000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "\n",
    "print(f\"\\nTotal chunks created from all documents: {len(all_processed_chunks)}\")\n",
    "\n",
    "# Show summary statistics\n",
    "if all_processed_chunks:\n",
    "    chunk_lengths = [len(chunk.page_content) for chunk in all_processed_chunks]\n",
    "    print(f\"Average chunk length: {sum(chunk_lengths) / len(chunk_lengths):.0f} characters\")\n",
    "    print(f\"Chunk length range: {min(chunk_lengths)} - {max(chunk_lengths)} characters\")\n",
    "    \n",
    "    # Show distribution by source document\n",
    "    source_counts = {}\n",
    "    for chunk in all_processed_chunks:\n",
    "        source = chunk.metadata.get('original_source', 'unknown')\n",
    "        source_counts[source] = source_counts.get(source, 0) + 1\n",
    "    \n",
    "    print(f\"\\nChunks per source document:\")\n",
    "    for source, count in source_counts.items():\n",
    "        print(f\"  {source}: {count} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fef83a8",
   "metadata": {},
   "source": [
    "# Initialize knowledge graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7aa909",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.graph import KnowledgeGraph\n",
    "from ragas.testset.graph import Node, NodeType\n",
    "\n",
    "\n",
    "kg = KnowledgeGraph()\n",
    "for doc in all_processed_chunks:\n",
    "    kg.nodes.append(\n",
    "        Node(\n",
    "            type=NodeType.DOCUMENT,\n",
    "            properties={\n",
    "                \"page_content\": doc.page_content,\n",
    "                \"document_metadata\": doc.metadata,\n",
    "            },\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373120f0",
   "metadata": {},
   "source": [
    "## VLLM Configuration for RAGAS\n",
    "\n",
    "The configuration above connects RAGAS to your vLLM server. Here are some key points:\n",
    "\n",
    "1. **Base URL**: `http://localhost:8003/v1` - your vLLM endpoint\n",
    "2. **API Key**: Set to \"not-needed\" since vLLM typically doesn't require authentication\n",
    "3. **Model Name**: Replace `\"your-model-name\"` with the actual model you're serving\n",
    "4. **Temperature**: Controls randomness (0.1 is relatively deterministic)\n",
    "5. **Max Tokens**: Maximum response length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b55eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings.base import embedding_factory\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Configure vLLM hosted LLM\n",
    "vllm_llm = ChatOpenAI(\n",
    "    base_url=\"http://localhost:8003/v1\",\n",
    "    api_key=\"asdf\",\n",
    "    model=\"Qwen/Qwen2.5\",  # Replace with your actual model name\n",
    "    temperature=0.1,\n",
    "    max_tokens=4096,\n",
    ")\n",
    "\n",
    "# Wrap for RAGAS\n",
    "llm = LangchainLLMWrapper(vllm_llm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7108734",
   "metadata": {},
   "source": [
    "# vLLM Hosted Embedding Model Configuration\n",
    "\n",
    "Here's how to configure a vLLM hosted embedding model for use with RAGAS:\n",
    "\n",
    "## Option 1: Using OpenAI-compatible embedding endpoint\n",
    "If your vLLM server hosts an embedding model with OpenAI-compatible API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b31482f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing query embedding...\n",
      "❌ Failed with: Connection error....\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "import asyncio\n",
    "\n",
    "# Try each configuration until one works\n",
    "\n",
    "async def test_embedding_model():\n",
    "    \"\"\"Async function to test the embedding model\"\"\"\n",
    "    vllm_embeddings = OpenAIEmbeddings(\n",
    "        base_url=\"http://localhost:8001/v1\",\n",
    "        api_key=\"asdf\",\n",
    "        model='thenlper/gte-large',\n",
    "        tiktoken_enabled=False,  # Disable tiktoken for vLLM\n",
    "    )\n",
    "    \n",
    "    # Test with a simple text first\n",
    "    print(\"testing query embedding...\")\n",
    "    test_result = vllm_embeddings.embed_query(\"Risk management is crucial for financial institutions.\")\n",
    "    print(f\"query embedding dimensions: {len(test_result)}\")\n",
    "\n",
    "    # Test with texts\n",
    "    print(\"testing text embedding...\")\n",
    "    test_texts = [\n",
    "        \"This is a test document about financial analysis.\",\n",
    "        \"Machine learning models are used in banking.\",\n",
    "        \"Risk management is crucial for financial institutions.\"\n",
    "    ]\n",
    "    test_results = vllm_embeddings.embed_documents(test_texts)\n",
    "    print(f\"Text embedding dimensions: {len(test_results[0])} for {len(test_results)} texts\")\n",
    "    \n",
    "    # If successful, wrap for RAGAS and test it through the wrapper\n",
    "    embedding_model = LangchainEmbeddingsWrapper(vllm_embeddings)\n",
    "\n",
    "    print(\"Testing wrapped embedding model query ...\")\n",
    "    embedding_result = await embedding_model.embed_query(\"Risk management is crucial for financial institutions.\")\n",
    "    print(f\"Wrapped query embedding dimensions: {len(embedding_result)}\")\n",
    "\n",
    "    print(\"Testing wrapped embedding model text ...\")\n",
    "    embedding_results = await embedding_model.embed_texts(test_texts, is_async=True)\n",
    "    print(f\"Wrapped text embedding dimensions: {len(embedding_results[0])} for {len(embedding_results)} texts\")\n",
    "\n",
    "    print(f\"✅ Successfully configured vLLM embedding model\")\n",
    "    return embedding_model\n",
    "\n",
    "# Run the async function\n",
    "try:\n",
    "    embedding_model = asyncio.run(test_embedding_model())\n",
    "    print(f\"🎉 Using vLLM embedding model successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed with: {str(e)[:100]}...\")\n",
    "    embedding_model = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fef3c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the vLLM connection\n",
    "print(\"Testing vLLM connection...\")\n",
    "\n",
    "try:\n",
    "    # Test the LLM directly\n",
    "    test_response = vllm_llm.invoke(\"Hello, this is a test. Please respond briefly.\")\n",
    "    print(f\"✅ vLLM connection successful!\")\n",
    "    print(f\"Response: {test_response.content}\")\n",
    "    \n",
    "    # Test with RAGAS wrapper\n",
    "    from ragas.llms.base import BaseRagasLLM\n",
    "    if isinstance(llm, BaseRagasLLM):\n",
    "        print(\"✅ RAGAS LLM wrapper configured correctly\")\n",
    "    else:\n",
    "        print(\"⚠️  RAGAS LLM wrapper might need adjustment\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error connecting to vLLM: {e}\")\n",
    "    print(\"Please check:\")\n",
    "    print(\"1. vLLM server is running at http://localhost:8003\")\n",
    "    print(\"2. Model name is correct\")\n",
    "    print(\"3. No firewall blocking the connection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251cc400",
   "metadata": {},
   "source": [
    "# Default synthetic data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac392bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.transforms import default_transforms, apply_transforms\n",
    "\n",
    "\n",
    "# define your LLM and Embedding Model\n",
    "# here we are using the same LLM and the configured embedding model\n",
    "transformer_llm = llm\n",
    "\n",
    "# Use the embedding model we configured above instead of None\n",
    "# This will enable better document similarity calculations and knowledge graph relationships\n",
    "# embedding_model = None  # Old way - commented out\n",
    "\n",
    "# Use the configured embedding model\n",
    "# Make sure you've run the embedding model configuration cells above first!\n",
    "try:\n",
    "    # This should be set from the embedding configuration cells above\n",
    "    if 'embedding_model' in locals() and embedding_model is not None:\n",
    "        print(f\"✅ Using configured embedding model\")\n",
    "    else:\n",
    "        print(\"⚠️  No embedding model configured, using None (limited functionality)\")\n",
    "        embedding_model = None\n",
    "except NameError:\n",
    "    print(\"⚠️  Embedding model not found, using None (limited functionality)\")\n",
    "    embedding_model = None\n",
    "\n",
    "trans = default_transforms(documents=docs, llm=transformer_llm, embedding_model=embedding_model)\n",
    "apply_transforms(kg, trans)\n",
    "print(kg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de8aa7b",
   "metadata": {},
   "source": [
    "# Custom multi-hop question answer generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004c9bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.transforms import Parallel, apply_transforms\n",
    "from ragas.testset.transforms import (\n",
    "    HeadlinesExtractor,\n",
    "    HeadlineSplitter,\n",
    "    KeyphrasesExtractor,\n",
    "    OverlapScoreBuilder,\n",
    ")\n",
    "\n",
    "\n",
    "headline_extractor = HeadlinesExtractor(llm=llm)\n",
    "headline_splitter = HeadlineSplitter(min_tokens=300, max_tokens=1000)\n",
    "keyphrase_extractor = KeyphrasesExtractor(\n",
    "    llm=llm, property_name=\"keyphrases\", max_num=10\n",
    ")\n",
    "relation_builder = OverlapScoreBuilder(\n",
    "    property_name=\"keyphrases\",\n",
    "    new_property_name=\"overlap_score\",\n",
    "    threshold=0.01,\n",
    "    distance_threshold=0.9,\n",
    ")\n",
    "\n",
    "transforms = [\n",
    "    headline_extractor,\n",
    "    headline_splitter,\n",
    "    keyphrase_extractor,\n",
    "    relation_builder,\n",
    "]\n",
    "\n",
    "apply_transforms(kg, transforms=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac06f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.persona import Persona\n",
    "\n",
    "person1 = Persona(\n",
    "    name=\"financial analyst at Barclays\",\n",
    "    role_description=\"A junior financial analyst at Barclays curious on workings on financial systems at Barclays \",\n",
    ")\n",
    "persona2 = Persona(\n",
    "    name=\"\",\n",
    "    role_description=\"AI developer building a financial system for Barclays\",\n",
    ")\n",
    "persona_list = [person1, persona2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95074d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import typing as t\n",
    "from ragas.testset.synthesizers.multi_hop.base import (\n",
    "    MultiHopQuerySynthesizer,\n",
    "    MultiHopScenario,\n",
    ")\n",
    "from ragas.testset.synthesizers.prompts import (\n",
    "    ThemesPersonasInput,\n",
    "    ThemesPersonasMatchingPrompt,\n",
    ")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MyMultiHopQuery(MultiHopQuerySynthesizer):\n",
    "\n",
    "    theme_persona_matching_prompt = ThemesPersonasMatchingPrompt()\n",
    "\n",
    "    async def _generate_scenarios(\n",
    "        self,\n",
    "        n: int,\n",
    "        knowledge_graph,\n",
    "        persona_list,\n",
    "        callbacks,\n",
    "    ) -> t.List[MultiHopScenario]:\n",
    "\n",
    "        # query and get (node_a, rel, node_b) to create multi-hop queries\n",
    "        results = knowledge_graph.find_two_nodes_single_rel(\n",
    "            relationship_condition=lambda rel: (\n",
    "                True if rel.type == \"keyphrases_overlap\" else False\n",
    "            )\n",
    "        )\n",
    "\n",
    "        num_sample_per_triplet = max(1, n // len(results))\n",
    "\n",
    "        scenarios = []\n",
    "        for triplet in results:\n",
    "            if len(scenarios) < n:\n",
    "                node_a, node_b = triplet[0], triplet[-1]\n",
    "                overlapped_keywords = triplet[1].properties[\"overlapped_items\"]\n",
    "                if overlapped_keywords:\n",
    "\n",
    "                    # match the keyword with a persona for query creation\n",
    "                    themes = list(dict(overlapped_keywords).keys())\n",
    "                    prompt_input = ThemesPersonasInput(\n",
    "                        themes=themes, personas=persona_list\n",
    "                    )\n",
    "                    persona_concepts = (\n",
    "                        await self.theme_persona_matching_prompt.generate(\n",
    "                            data=prompt_input, llm=self.llm, callbacks=callbacks\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                    overlapped_keywords = [list(item) for item in overlapped_keywords]\n",
    "\n",
    "                    # prepare and sample possible combinations\n",
    "                    base_scenarios = self.prepare_combinations(\n",
    "                        [node_a, node_b],\n",
    "                        overlapped_keywords,\n",
    "                        personas=persona_list,\n",
    "                        persona_item_mapping=persona_concepts.mapping,\n",
    "                        property_name=\"keyphrases\",\n",
    "                    )\n",
    "\n",
    "                    # get number of required samples from this triplet\n",
    "                    base_scenarios = self.sample_diverse_combinations(\n",
    "                        base_scenarios, num_sample_per_triplet\n",
    "                    )\n",
    "\n",
    "                    scenarios.extend(base_scenarios)\n",
    "\n",
    "        return scenarios\n",
    "\n",
    "query = MyMultiHopQuery(llm=llm)\n",
    "scenarios = await query.generate_scenarios(\n",
    "    n=10, knowledge_graph=kg, persona_list=persona_list\n",
    ")\n",
    "\n",
    "scenarios[4]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "syftr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

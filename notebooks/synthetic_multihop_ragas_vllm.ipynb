{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ee3a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, MarkdownHeaderTextSplitter\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "\n",
    "path = \"/Users/debadeepta.dey/datasets/barclays\"\n",
    "loader = DirectoryLoader(path, glob=\"**/*.md\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61bc9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Split by Markdown Headers (Most intelligent for markdown)\n",
    "# This preserves the document structure and creates logical chunks\n",
    "\n",
    "def split_markdown_by_headers(document_content):\n",
    "    \"\"\"\n",
    "    Split markdown document by headers, preserving document structure\n",
    "    \"\"\"\n",
    "    # Define headers to split on (from h1 to h3)\n",
    "    headers_to_split_on = [\n",
    "        (\"#\", \"Header 1\"),\n",
    "        (\"##\", \"Header 2\"), \n",
    "        (\"###\", \"Header 3\"),\n",
    "    ]\n",
    "    \n",
    "    # Create the markdown header text splitter\n",
    "    markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "        headers_to_split_on=headers_to_split_on,\n",
    "        strip_headers=False  # Keep headers in the chunks\n",
    "    )\n",
    "    \n",
    "    # Split the document\n",
    "    md_header_splits = markdown_splitter.split_text(document_content)\n",
    "    \n",
    "    return md_header_splits\n",
    "\n",
    "# # Example usage with your loaded documents\n",
    "# if docs:\n",
    "#     # Take the first document as example\n",
    "#     first_doc = docs[0]\n",
    "#     header_splits = split_markdown_by_headers(first_doc.page_content)\n",
    "    \n",
    "#     print(f\"Original document split into {len(header_splits)} chunks based on headers\")\n",
    "    \n",
    "#     # Display first few chunks\n",
    "#     for i, chunk in enumerate(header_splits[:3]):\n",
    "#         print(f\"\\n--- Chunk {i+1} ---\")\n",
    "#         print(f\"Content: {chunk.page_content[:200]}...\")\n",
    "#         print(f\"Metadata: {chunk.metadata}\")\n",
    "#         print(f\"Full length: {len(chunk.page_content)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c057368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Recursive Character Text Splitter (Good fallback)\n",
    "# This method is useful when documents don't have clear header structure\n",
    "\n",
    "def split_markdown_recursive(document_content, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Split markdown using recursive character splitter with markdown-aware separators\n",
    "    \"\"\"\n",
    "    # Define separators that work well for markdown\n",
    "    markdown_separators = [\n",
    "        \"\\n\\n\",  # Double newline (paragraph breaks)\n",
    "        \"\\n\",    # Single newline\n",
    "        \" \",     # Space\n",
    "        \"\"       # Character level\n",
    "    ]\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=markdown_separators,\n",
    "        length_function=len,\n",
    "    )\n",
    "    \n",
    "    # Split the document\n",
    "    chunks = text_splitter.split_text(document_content)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# # Example usage\n",
    "# if docs:\n",
    "#     first_doc = docs[0]\n",
    "#     recursive_chunks = split_markdown_recursive(\n",
    "#         first_doc.page_content, \n",
    "#         chunk_size=2048,  # Adjust based on your needs\n",
    "#         chunk_overlap=200\n",
    "#     )\n",
    "    \n",
    "#     print(f\"\\nRecursive splitting created {len(recursive_chunks)} chunks\")\n",
    "    \n",
    "#     # Display first few chunks\n",
    "#     for i, chunk in enumerate(recursive_chunks[:3]):\n",
    "#         print(f\"\\n--- Recursive Chunk {i+1} ---\")\n",
    "#         print(f\"Content: {chunk[:200]}...\")\n",
    "#         print(f\"Length: {len(chunk)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fc45fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 3: Hybrid Approach (Recommended)\n",
    "# Combine header-based splitting with recursive splitting for optimal results\n",
    "\n",
    "def smart_markdown_split(document_content, max_chunk_size=1500, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Smart markdown splitting that combines header-based and recursive approaches\n",
    "    \"\"\"\n",
    "    from langchain.schema import Document\n",
    "    \n",
    "    # First, try to split by headers\n",
    "    headers_to_split_on = [\n",
    "        (\"#\", \"Header 1\"),\n",
    "        (\"##\", \"Header 2\"), \n",
    "        (\"###\", \"Header 3\"),\n",
    "        (\"####\", \"Header 4\"),\n",
    "    ]\n",
    "    \n",
    "    markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "        headers_to_split_on=headers_to_split_on,\n",
    "        strip_headers=False\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Split by headers first\n",
    "        header_splits = markdown_splitter.split_text(document_content)\n",
    "        \n",
    "        # If header splits are too large, further split them recursively\n",
    "        final_chunks = []\n",
    "        \n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=max_chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "            length_function=len,\n",
    "        )\n",
    "        \n",
    "        for doc in header_splits:\n",
    "            if len(doc.page_content) > max_chunk_size:\n",
    "                # Split large chunks further\n",
    "                sub_chunks = text_splitter.split_text(doc.page_content)\n",
    "                for i, sub_chunk in enumerate(sub_chunks):\n",
    "                    # Preserve metadata from header splitting\n",
    "                    new_metadata = doc.metadata.copy()\n",
    "                    new_metadata['sub_chunk'] = i\n",
    "                    final_chunks.append(Document(\n",
    "                        page_content=sub_chunk,\n",
    "                        metadata=new_metadata\n",
    "                    ))\n",
    "            else:\n",
    "                final_chunks.append(doc)\n",
    "                \n",
    "        return final_chunks\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Header splitting failed: {e}\")\n",
    "        # Fallback to recursive splitting\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=max_chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "        )\n",
    "        chunks = text_splitter.split_text(document_content)\n",
    "        return [Document(page_content=chunk, metadata={}) for chunk in chunks]\n",
    "\n",
    "# # Example usage with the hybrid approach\n",
    "# if docs:\n",
    "#     first_doc = docs[0]\n",
    "#     smart_chunks = smart_markdown_split(\n",
    "#         first_doc.page_content,\n",
    "#         max_chunk_size=1200,\n",
    "#         chunk_overlap=150\n",
    "#     )\n",
    "    \n",
    "#     print(f\"\\nSmart splitting created {len(smart_chunks)} chunks\")\n",
    "    \n",
    "#     # Display statistics\n",
    "#     chunk_lengths = [len(chunk.page_content) for chunk in smart_chunks]\n",
    "#     print(f\"Average chunk length: {sum(chunk_lengths) / len(chunk_lengths):.0f} characters\")\n",
    "#     print(f\"Min chunk length: {min(chunk_lengths)} characters\")\n",
    "#     print(f\"Max chunk length: {max(chunk_lengths)} characters\")\n",
    "    \n",
    "#     # Display first few chunks with metadata\n",
    "#     for i, chunk in enumerate(smart_chunks[:3]):\n",
    "#         print(f\"\\n--- Smart Chunk {i+1} ---\")\n",
    "#         print(f\"Metadata: {chunk.metadata}\")\n",
    "#         print(f\"Content preview: {chunk.page_content[:200]}...\")\n",
    "#         print(f\"Length: {len(chunk.page_content)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7956566",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "# Utility function to process all your documents\n",
    "def process_all_documents(docs, output_method='smart', **kwargs):\n",
    "    \"\"\"\n",
    "    Process all loaded documents and return chunks\n",
    "    \n",
    "    Args:\n",
    "        docs: List of loaded documents\n",
    "        output_method: 'header', 'recursive', or 'smart'\n",
    "        **kwargs: Additional parameters for the splitting methods\n",
    "    \n",
    "    Returns:\n",
    "        List of all chunks with source document information\n",
    "    \"\"\"\n",
    "    all_chunks = []\n",
    "    \n",
    "    for doc_idx, doc in enumerate(docs):\n",
    "        print(f\"Processing document {doc_idx + 1}/{len(docs)}: {doc.metadata.get('source', 'unknown')}\")\n",
    "        \n",
    "        if output_method == 'header':\n",
    "            chunks = split_markdown_by_headers(doc.page_content)\n",
    "        elif output_method == 'recursive':\n",
    "            chunk_texts = split_markdown_recursive(doc.page_content, **kwargs)\n",
    "            chunks = [Document(page_content=text, metadata=doc.metadata.copy()) for text in chunk_texts]\n",
    "        elif output_method == 'smart':\n",
    "            chunks = smart_markdown_split(doc.page_content, **kwargs)\n",
    "        else:\n",
    "            raise ValueError(\"output_method must be 'header', 'recursive', or 'smart'\")\n",
    "        \n",
    "        # Add source document information to each chunk\n",
    "        for chunk_idx, chunk in enumerate(chunks):\n",
    "            chunk.metadata['source_doc_index'] = doc_idx\n",
    "            chunk.metadata['chunk_index'] = chunk_idx\n",
    "            chunk.metadata['original_source'] = doc.metadata.get('source', 'unknown')\n",
    "            all_chunks.append(chunk)\n",
    "    \n",
    "    return all_chunks\n",
    "\n",
    "# Process all your documents using the smart method\n",
    "all_processed_chunks = process_all_documents(\n",
    "    docs, \n",
    "    output_method='smart',  # Change to 'header' or 'recursive' if preferred\n",
    "    max_chunk_size=10000,\n",
    "    chunk_overlap=0 # deliberately set to 0\n",
    ")\n",
    "\n",
    "print(f\"\\nTotal chunks created from all documents: {len(all_processed_chunks)}\")\n",
    "\n",
    "# Show summary statistics\n",
    "if all_processed_chunks:\n",
    "    chunk_lengths = [len(chunk.page_content) for chunk in all_processed_chunks]\n",
    "    print(f\"Average chunk length: {sum(chunk_lengths) / len(chunk_lengths):.0f} characters\")\n",
    "    print(f\"Chunk length range: {min(chunk_lengths)} - {max(chunk_lengths)} characters\")\n",
    "    \n",
    "    # Show distribution by source document\n",
    "    source_counts = {}\n",
    "    for chunk in all_processed_chunks:\n",
    "        source = chunk.metadata.get('original_source', 'unknown')\n",
    "        source_counts[source] = source_counts.get(source, 0) + 1\n",
    "    \n",
    "    print(f\"\\nChunks per source document:\")\n",
    "    for source, count in source_counts.items():\n",
    "        print(f\"  {source}: {count} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c98727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use gpt-4o-mini Azure OpenAI model\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "# Specify the directory containing your .env file\n",
    "env_directory = \"/Users/debadeepta.dey/sources/syftr/runtime-secrets\"  # Change this to your desired directory\n",
    "env_file_path = Path(env_directory) / \"azure_openai_gpt_4o_mini.env\"\n",
    "\n",
    "# Load environment variables from the specified directory\n",
    "load_dotenv(dotenv_path=env_file_path)\n",
    "\n",
    "# Verify the .env file was found and loaded\n",
    "if env_file_path.exists():\n",
    "    print(f\"âœ… Loaded .env from: {env_file_path}\")\n",
    "else:\n",
    "    print(f\"âš ï¸  .env file not found at: {env_file_path}\")\n",
    "    print(\"Please create the .env file with your Azure OpenAI credentials\")\n",
    "\n",
    "# Configure Azure OpenAI GPT-4o-mini\n",
    "azure_llm = AzureChatOpenAI(\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    azure_deployment=os.getenv(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "    temperature=0.0,\n",
    "    max_tokens=16384,\n",
    ")\n",
    "\n",
    "# Test the model \n",
    "print(azure_llm.invoke(\"Who are you?\"))\n",
    "\n",
    "# Wrap for RAGAS\n",
    "llm = LangchainLLMWrapper(azure_llm)\n",
    "\n",
    "print(\"Azure OpenAI LLM configured successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373120f0",
   "metadata": {},
   "source": [
    "## VLLM Configuration for RAGAS\n",
    "\n",
    "The configuration above connects RAGAS to your vLLM server. Here are some key points:\n",
    "\n",
    "1. **Base URL**: `http://localhost:8003/v1` - your vLLM endpoint\n",
    "2. **API Key**: Set to \"not-needed\" since vLLM typically doesn't require authentication\n",
    "3. **Model Name**: Replace `\"your-model-name\"` with the actual model you're serving\n",
    "4. **Temperature**: Controls randomness (0.1 is relatively deterministic)\n",
    "5. **Max Tokens**: Maximum response length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b55eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ragas.llms import LangchainLLMWrapper\n",
    "# from ragas.embeddings.base import embedding_factory\n",
    "# from langchain_openai import ChatOpenAI\n",
    "\n",
    "# # Configure vLLM hosted LLM\n",
    "# vllm_llm = ChatOpenAI(\n",
    "#     base_url=\"http://localhost:8014/v1\",\n",
    "#     api_key=\"asdf\",\n",
    "#     model=\"nvidia/Llama-3_3-Nemotron-Super-49B\",  # Replace with your actual model name\n",
    "#     temperature=0.0,\n",
    "#     max_tokens=32768,\n",
    "# )\n",
    "\n",
    "# # Wrap for RAGAS\n",
    "# llm = LangchainLLMWrapper(vllm_llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fe900c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test the vLLM connection\n",
    "# print(\"Testing vLLM connection...\")\n",
    "\n",
    "# try:\n",
    "#     # Test the LLM directly\n",
    "#     test_response = vllm_llm.invoke(\"Hello, this is a test. Please respond briefly.\")\n",
    "#     print(f\"âœ… vLLM connection successful!\")\n",
    "#     print(f\"Response: {test_response.content}\")\n",
    "    \n",
    "#     # Test with RAGAS wrapper\n",
    "#     from ragas.llms.base import BaseRagasLLM\n",
    "#     if isinstance(llm, BaseRagasLLM):\n",
    "#         print(\"âœ… RAGAS LLM wrapper configured correctly\")\n",
    "#     else:\n",
    "#         print(\"âš ï¸  RAGAS LLM wrapper might need adjustment\")\n",
    "        \n",
    "# except Exception as e:\n",
    "#     print(f\"âŒ Error connecting to vLLM: {e}\")\n",
    "#     print(\"Please check:\")\n",
    "#     print(\"1. vLLM server is running\")\n",
    "#     print(\"2. Model name is correct\")\n",
    "#     print(\"3. No firewall blocking the connection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7108734",
   "metadata": {},
   "source": [
    "# vLLM Hosted Embedding Model Configuration\n",
    "\n",
    "Here's how to configure a vLLM hosted embedding model for use with RAGAS:\n",
    "\n",
    "## Option 1: Using OpenAI-compatible embedding endpoint\n",
    "If your vLLM server hosts an embedding model with OpenAI-compatible API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31482f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_openai import OpenAIEmbeddings\n",
    "# from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "# import asyncio\n",
    "\n",
    "# # Try each configuration until one works\n",
    "# vllm_embeddings = OpenAIEmbeddings(\n",
    "#         base_url=\"http://localhost:8001/v1\",\n",
    "#         api_key=\"asdf\",\n",
    "#         model='thenlper/gte-large',\n",
    "#         tiktoken_enabled=False,  # Disable tiktoken for vLLM\n",
    "#     )\n",
    "\n",
    "# embedding_model = LangchainEmbeddingsWrapper(vllm_embeddings)\n",
    "\n",
    "# async def test_embedding_model(vllm_embeddings: OpenAIEmbeddings):\n",
    "#     \"\"\"Async function to test the embedding model\"\"\"\n",
    "    \n",
    "#     # Test with a simple text first\n",
    "#     print(\"testing query embedding...\")\n",
    "#     test_result = vllm_embeddings.embed_query(\"Risk management is crucial for financial institutions.\")\n",
    "#     print(f\"query embedding dimensions: {len(test_result)}\")\n",
    "\n",
    "#     # Test with texts\n",
    "#     print(\"testing text embedding...\")\n",
    "#     test_texts = [\n",
    "#         \"This is a test document about financial analysis.\",\n",
    "#         \"Machine learning models are used in banking.\",\n",
    "#         \"Risk management is crucial for financial institutions.\"\n",
    "#     ]\n",
    "#     test_results = vllm_embeddings.embed_documents(test_texts)\n",
    "#     print(f\"Text embedding dimensions: {len(test_results[0])} for {len(test_results)} texts\")\n",
    "    \n",
    "#     # If successful, wrap for RAGAS and test it through the wrapper\n",
    "#     embedding_model = LangchainEmbeddingsWrapper(vllm_embeddings)\n",
    "\n",
    "#     print(\"Testing wrapped embedding model query ...\")\n",
    "#     embedding_result = await embedding_model.embed_query(\"Risk management is crucial for financial institutions.\")\n",
    "#     print(f\"Wrapped query embedding dimensions: {len(embedding_result)}\")\n",
    "\n",
    "#     print(\"Testing wrapped embedding model text ...\")\n",
    "#     embedding_results = await embedding_model.embed_texts(test_texts, is_async=True)\n",
    "#     print(f\"Wrapped text embedding dimensions: {len(embedding_results[0])} for {len(embedding_results)} texts\")\n",
    "\n",
    "#     print(f\"âœ… Successfully configured vLLM embedding model\")\n",
    "#     return embedding_model\n",
    "\n",
    "# # Run the async function\n",
    "# try:\n",
    "#     embedding_model = asyncio.run(test_embedding_model(vllm_embeddings))\n",
    "#     print(f\"ðŸŽ‰ Using vLLM embedding model successfully!\")\n",
    "# except Exception as e:\n",
    "#     print(f\"âŒ Failed with: {str(e)[:100]}...\")\n",
    "#     embedding_model = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66685102",
   "metadata": {},
   "source": [
    "# Option 2: local embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9123be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "\n",
    "local_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"WhereIsAI/UAE-Large-V1\",\n",
    "    model_kwargs={\"device\": \"mps\"} # Or \"cuda\" for GPU, \"mps\" for Mac \n",
    ")\n",
    "local_embeddings = LangchainEmbeddingsWrapper(local_embeddings)\n",
    "\n",
    "res = local_embeddings.embed_query(\"Who is this?\")  # Test local embedding model\n",
    "print(res)\n",
    "\n",
    "res = local_embeddings.embed_text(\"Who is this?\")\n",
    "print(res)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251cc400",
   "metadata": {},
   "source": [
    "# Default synthetic data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac392bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.transforms import (\n",
    "    default_transforms, \n",
    "    apply_transforms, \n",
    "    EmbeddingExtractor, \n",
    "    SummaryExtractor, \n",
    "    TitleExtractor,\n",
    "    HeadlinesExtractor,\n",
    "    HeadlineSplitter,\n",
    "    KeyphrasesExtractor,\n",
    "    HeadlineSplitter,\n",
    "    OverlapScoreBuilder,\n",
    ")\n",
    "from ragas.testset.graph import KnowledgeGraph\n",
    "from ragas.testset.graph import Node, NodeType\n",
    "\n",
    "\n",
    "# initialize your knowledge graph\n",
    "kg = KnowledgeGraph()\n",
    "\n",
    "for chunk in all_processed_chunks:\n",
    "    kg.nodes.append(\n",
    "        Node(\n",
    "            type=NodeType.DOCUMENT,\n",
    "            properties={\"page_content\": chunk.page_content, \"metadata\": chunk.metadata},\n",
    "        )\n",
    "    )\n",
    "print(kg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d54a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# headline extractor\n",
    "headline_extractor = HeadlinesExtractor(llm=llm)\n",
    "apply_transforms(kg, headline_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074e15eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the default transforms as well\n",
    "print(\"Before\")\n",
    "print(kg)\n",
    "trans = default_transforms(documents=docs, llm=llm, embedding_model=local_embeddings)\n",
    "for tran in trans:\n",
    "    print(f\"Applying transform: {tran}\")\n",
    "    apply_transforms(kg, tran)\n",
    "print(\"After\")\n",
    "print(kg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd287d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get summary embeddings required by automatic persona generator\n",
    "summary_embedding_extractor = EmbeddingExtractor(embedding_model=local_embeddings,\n",
    "                                         property_name=\"summary_embedding\",\n",
    "                                         embed_property_name=\"summary\")\n",
    "apply_transforms(kg, summary_embedding_extractor)\n",
    "print(\"Embedding extraction complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7397628e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get regular embeddings\n",
    "regular_embedding_extractor = EmbeddingExtractor(\n",
    "    embedding_model=local_embeddings,\n",
    ")\n",
    "apply_transforms(kg, regular_embedding_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc93f285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# headline extractor\n",
    "headline_extractor = HeadlinesExtractor(llm=llm)\n",
    "apply_transforms(kg, headline_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab1195b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the default transforms as well\n",
    "print(\"Before\")\n",
    "print(kg)\n",
    "trans = default_transforms(documents=docs, llm=llm, embedding_model=local_embeddings)\n",
    "apply_transforms(kg, trans)\n",
    "print(\"After\")\n",
    "print(kg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd1e47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this debugging code to see what relationships exist\n",
    "print(\"=== Knowledge Graph Debug Info ===\")\n",
    "print(f\"Total nodes: {len(kg.nodes)}\")\n",
    "print(f\"Total relationships: {len(kg.relationships)}\")\n",
    "\n",
    "# Check relationship types\n",
    "rel_types = set()\n",
    "for rel in kg.relationships:\n",
    "    rel_types.add(rel.type)\n",
    "    \n",
    "print(f\"Relationship types found: {rel_types}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41592f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(kg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee375d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset import TestsetGenerator\n",
    "from ragas.testset.synthesizers import default_query_distribution\n",
    "\n",
    "generator = TestsetGenerator(llm=llm, embedding_model=local_embeddings, knowledge_graph=kg)\n",
    "query_distribution = default_query_distribution(llm)\n",
    "testset = generator.generate(testset_size=10, query_distribution=query_distribution)\n",
    "testset_pd = testset.to_pandas()\n",
    "testset_pd.to_json('barclays_synthetic_multihop.json', orient='records', indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c852e6d1",
   "metadata": {},
   "source": [
    "### Custom persona "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7271ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.graph import KnowledgeGraph\n",
    "from ragas.testset.graph import Node, NodeType\n",
    "\n",
    "\n",
    "kg = KnowledgeGraph()\n",
    "for doc in docs:\n",
    "    kg.nodes.append(\n",
    "        Node(\n",
    "            type=NodeType.DOCUMENT,\n",
    "            properties={\n",
    "                \"page_content\": doc.page_content,\n",
    "                \"document_metadata\": doc.metadata,\n",
    "            },\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0417345",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.transforms import Parallel, apply_transforms\n",
    "from ragas.testset.transforms import (\n",
    "    HeadlinesExtractor,\n",
    "    HeadlineSplitter,\n",
    "    KeyphrasesExtractor,\n",
    "    OverlapScoreBuilder,\n",
    ")\n",
    "\n",
    "\n",
    "headline_extractor = HeadlinesExtractor(llm=llm)\n",
    "headline_splitter = HeadlineSplitter(min_tokens=300, max_tokens=1000)\n",
    "keyphrase_extractor = KeyphrasesExtractor(\n",
    "    llm=llm, property_name=\"keyphrases\", max_num=10\n",
    ")\n",
    "relation_builder = OverlapScoreBuilder(\n",
    "    property_name=\"keyphrases\",\n",
    "    new_property_name=\"overlap_score\",\n",
    "    threshold=0.01,\n",
    "    distance_threshold=0.9,\n",
    ")\n",
    "\n",
    "transforms = [\n",
    "    headline_extractor,\n",
    "    headline_splitter,\n",
    "    keyphrase_extractor,\n",
    "    relation_builder,\n",
    "]\n",
    "\n",
    "apply_transforms(kg, transforms=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ddddc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's generate using custom personas\n",
    "from ragas.testset.persona import Persona\n",
    "\n",
    "person1 = Persona(\n",
    "    name=\"Banking Executive\",\n",
    "    role_description=\"Explore AI and data strategies to modernize financial services and unlock new revenue streams. Asks only one question at a time.\",\n",
    ")\n",
    "persona2 = Persona(\n",
    "    name=\"FinTech Founder\",\n",
    "    role_description=\"Leverage AI and data innovation to build competitive, regulation-aware financial products. Asks only one question at a time.\",\n",
    ")\n",
    "persona_list = [person1, persona2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e818b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import typing as t\n",
    "from ragas.testset.synthesizers.multi_hop.base import (\n",
    "    MultiHopQuerySynthesizer,\n",
    "    MultiHopScenario,\n",
    ")\n",
    "from ragas.testset.synthesizers.prompts import (\n",
    "    ThemesPersonasInput,\n",
    "    ThemesPersonasMatchingPrompt,\n",
    ")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MyMultiHopQuery(MultiHopQuerySynthesizer):\n",
    "\n",
    "    theme_persona_matching_prompt = ThemesPersonasMatchingPrompt()\n",
    "\n",
    "    async def _generate_scenarios(\n",
    "        self,\n",
    "        n: int,\n",
    "        knowledge_graph,\n",
    "        persona_list,\n",
    "        callbacks,\n",
    "    ) -> t.List[MultiHopScenario]:\n",
    "\n",
    "        # query and get (node_a, rel, node_b) to create multi-hop queries\n",
    "        results = kg.find_two_nodes_single_rel(\n",
    "            relationship_condition=lambda rel: (\n",
    "                True if rel.type == \"keyphrases_overlap\" else False\n",
    "            )\n",
    "        )\n",
    "\n",
    "        num_sample_per_triplet = max(1, n // len(results))\n",
    "\n",
    "        scenarios = []\n",
    "        for triplet in results:\n",
    "            if len(scenarios) < n:\n",
    "                node_a, node_b = triplet[0], triplet[-1]\n",
    "                overlapped_keywords = triplet[1].properties[\"overlapped_items\"]\n",
    "                if overlapped_keywords:\n",
    "\n",
    "                    # match the keyword with a persona for query creation\n",
    "                    themes = list(dict(overlapped_keywords).keys())\n",
    "                    prompt_input = ThemesPersonasInput(\n",
    "                        themes=themes, personas=persona_list\n",
    "                    )\n",
    "                    persona_concepts = (\n",
    "                        await self.theme_persona_matching_prompt.generate(\n",
    "                            data=prompt_input, llm=self.llm, callbacks=callbacks\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                    overlapped_keywords = [list(item) for item in overlapped_keywords]\n",
    "\n",
    "                    # prepare and sample possible combinations\n",
    "                    base_scenarios = self.prepare_combinations(\n",
    "                        [node_a, node_b],\n",
    "                        overlapped_keywords,\n",
    "                        personas=persona_list,\n",
    "                        persona_item_mapping=persona_concepts.mapping,\n",
    "                        property_name=\"keyphrases\",\n",
    "                    )\n",
    "\n",
    "                    # get number of required samples from this triplet\n",
    "                    base_scenarios = self.sample_diverse_combinations(\n",
    "                        base_scenarios, num_sample_per_triplet\n",
    "                    )\n",
    "\n",
    "                    scenarios.extend(base_scenarios)\n",
    "\n",
    "        return scenarios\n",
    "\n",
    "query = MyMultiHopQuery(llm=llm)\n",
    "scenarios = await query.generate_scenarios(\n",
    "    n=200, knowledge_graph=kg, persona_list=persona_list\n",
    ")\n",
    "\n",
    "scenarios[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaa981c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await query.generate_sample(scenario=scenarios[1])\n",
    "print(result.user_input)\n",
    "print(result.reference)\n",
    "print(result.reference_contexts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72b1e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for scenario in scenarios:\n",
    "    result = await query.generate_sample(scenario=scenario)\n",
    "    print(f\"Scenario: {result.user_input}\")\n",
    "    print(\"-\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "syftr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

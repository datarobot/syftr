{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ee3a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, MarkdownHeaderTextSplitter\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "\n",
    "path = \"/Users/debadeepta.dey/datasets/barclays\"\n",
    "loader = DirectoryLoader(path, glob=\"**/*.md\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61bc9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Split by Markdown Headers (Most intelligent for markdown)\n",
    "# This preserves the document structure and creates logical chunks\n",
    "\n",
    "def split_markdown_by_headers(document_content):\n",
    "    \"\"\"\n",
    "    Split markdown document by headers, preserving document structure\n",
    "    \"\"\"\n",
    "    # Define headers to split on (from h1 to h3)\n",
    "    headers_to_split_on = [\n",
    "        (\"#\", \"Header 1\"),\n",
    "        (\"##\", \"Header 2\"), \n",
    "        (\"###\", \"Header 3\"),\n",
    "    ]\n",
    "    \n",
    "    # Create the markdown header text splitter\n",
    "    markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "        headers_to_split_on=headers_to_split_on,\n",
    "        strip_headers=False  # Keep headers in the chunks\n",
    "    )\n",
    "    \n",
    "    # Split the document\n",
    "    md_header_splits = markdown_splitter.split_text(document_content)\n",
    "    \n",
    "    return md_header_splits\n",
    "\n",
    "# # Example usage with your loaded documents\n",
    "# if docs:\n",
    "#     # Take the first document as example\n",
    "#     first_doc = docs[0]\n",
    "#     header_splits = split_markdown_by_headers(first_doc.page_content)\n",
    "    \n",
    "#     print(f\"Original document split into {len(header_splits)} chunks based on headers\")\n",
    "    \n",
    "#     # Display first few chunks\n",
    "#     for i, chunk in enumerate(header_splits[:3]):\n",
    "#         print(f\"\\n--- Chunk {i+1} ---\")\n",
    "#         print(f\"Content: {chunk.page_content[:200]}...\")\n",
    "#         print(f\"Metadata: {chunk.metadata}\")\n",
    "#         print(f\"Full length: {len(chunk.page_content)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c057368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Recursive Character Text Splitter (Good fallback)\n",
    "# This method is useful when documents don't have clear header structure\n",
    "\n",
    "def split_markdown_recursive(document_content, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Split markdown using recursive character splitter with markdown-aware separators\n",
    "    \"\"\"\n",
    "    # Define separators that work well for markdown\n",
    "    markdown_separators = [\n",
    "        \"\\n\\n\",  # Double newline (paragraph breaks)\n",
    "        \"\\n\",    # Single newline\n",
    "        \" \",     # Space\n",
    "        \"\"       # Character level\n",
    "    ]\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=markdown_separators,\n",
    "        length_function=len,\n",
    "    )\n",
    "    \n",
    "    # Split the document\n",
    "    chunks = text_splitter.split_text(document_content)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# # Example usage\n",
    "# if docs:\n",
    "#     first_doc = docs[0]\n",
    "#     recursive_chunks = split_markdown_recursive(\n",
    "#         first_doc.page_content, \n",
    "#         chunk_size=2048,  # Adjust based on your needs\n",
    "#         chunk_overlap=200\n",
    "#     )\n",
    "    \n",
    "#     print(f\"\\nRecursive splitting created {len(recursive_chunks)} chunks\")\n",
    "    \n",
    "#     # Display first few chunks\n",
    "#     for i, chunk in enumerate(recursive_chunks[:3]):\n",
    "#         print(f\"\\n--- Recursive Chunk {i+1} ---\")\n",
    "#         print(f\"Content: {chunk[:200]}...\")\n",
    "#         print(f\"Length: {len(chunk)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fc45fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 3: Hybrid Approach (Recommended)\n",
    "# Combine header-based splitting with recursive splitting for optimal results\n",
    "\n",
    "def smart_markdown_split(document_content, max_chunk_size=1500, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Smart markdown splitting that combines header-based and recursive approaches\n",
    "    \"\"\"\n",
    "    from langchain.schema import Document\n",
    "    \n",
    "    # First, try to split by headers\n",
    "    headers_to_split_on = [\n",
    "        (\"#\", \"Header 1\"),\n",
    "        (\"##\", \"Header 2\"), \n",
    "        (\"###\", \"Header 3\"),\n",
    "        (\"####\", \"Header 4\"),\n",
    "    ]\n",
    "    \n",
    "    markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "        headers_to_split_on=headers_to_split_on,\n",
    "        strip_headers=False\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Split by headers first\n",
    "        header_splits = markdown_splitter.split_text(document_content)\n",
    "        \n",
    "        # If header splits are too large, further split them recursively\n",
    "        final_chunks = []\n",
    "        \n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=max_chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "            length_function=len,\n",
    "        )\n",
    "        \n",
    "        for doc in header_splits:\n",
    "            if len(doc.page_content) > max_chunk_size:\n",
    "                # Split large chunks further\n",
    "                sub_chunks = text_splitter.split_text(doc.page_content)\n",
    "                for i, sub_chunk in enumerate(sub_chunks):\n",
    "                    # Preserve metadata from header splitting\n",
    "                    new_metadata = doc.metadata.copy()\n",
    "                    new_metadata['sub_chunk'] = i\n",
    "                    final_chunks.append(Document(\n",
    "                        page_content=sub_chunk,\n",
    "                        metadata=new_metadata\n",
    "                    ))\n",
    "            else:\n",
    "                final_chunks.append(doc)\n",
    "                \n",
    "        return final_chunks\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Header splitting failed: {e}\")\n",
    "        # Fallback to recursive splitting\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=max_chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "        )\n",
    "        chunks = text_splitter.split_text(document_content)\n",
    "        return [Document(page_content=chunk, metadata={}) for chunk in chunks]\n",
    "\n",
    "# # Example usage with the hybrid approach\n",
    "# if docs:\n",
    "#     first_doc = docs[0]\n",
    "#     smart_chunks = smart_markdown_split(\n",
    "#         first_doc.page_content,\n",
    "#         max_chunk_size=1200,\n",
    "#         chunk_overlap=150\n",
    "#     )\n",
    "    \n",
    "#     print(f\"\\nSmart splitting created {len(smart_chunks)} chunks\")\n",
    "    \n",
    "#     # Display statistics\n",
    "#     chunk_lengths = [len(chunk.page_content) for chunk in smart_chunks]\n",
    "#     print(f\"Average chunk length: {sum(chunk_lengths) / len(chunk_lengths):.0f} characters\")\n",
    "#     print(f\"Min chunk length: {min(chunk_lengths)} characters\")\n",
    "#     print(f\"Max chunk length: {max(chunk_lengths)} characters\")\n",
    "    \n",
    "#     # Display first few chunks with metadata\n",
    "#     for i, chunk in enumerate(smart_chunks[:3]):\n",
    "#         print(f\"\\n--- Smart Chunk {i+1} ---\")\n",
    "#         print(f\"Metadata: {chunk.metadata}\")\n",
    "#         print(f\"Content preview: {chunk.page_content[:200]}...\")\n",
    "#         print(f\"Length: {len(chunk.page_content)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7956566",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "# Utility function to process all your documents\n",
    "def process_all_documents(docs, output_method='smart', **kwargs):\n",
    "    \"\"\"\n",
    "    Process all loaded documents and return chunks\n",
    "    \n",
    "    Args:\n",
    "        docs: List of loaded documents\n",
    "        output_method: 'header', 'recursive', or 'smart'\n",
    "        **kwargs: Additional parameters for the splitting methods\n",
    "    \n",
    "    Returns:\n",
    "        List of all chunks with source document information\n",
    "    \"\"\"\n",
    "    all_chunks = []\n",
    "    \n",
    "    for doc_idx, doc in enumerate(docs):\n",
    "        print(f\"Processing document {doc_idx + 1}/{len(docs)}: {doc.metadata.get('source', 'unknown')}\")\n",
    "        \n",
    "        if output_method == 'header':\n",
    "            chunks = split_markdown_by_headers(doc.page_content)\n",
    "        elif output_method == 'recursive':\n",
    "            chunk_texts = split_markdown_recursive(doc.page_content, **kwargs)\n",
    "            chunks = [Document(page_content=text, metadata=doc.metadata.copy()) for text in chunk_texts]\n",
    "        elif output_method == 'smart':\n",
    "            chunks = smart_markdown_split(doc.page_content, **kwargs)\n",
    "        else:\n",
    "            raise ValueError(\"output_method must be 'header', 'recursive', or 'smart'\")\n",
    "        \n",
    "        # Add source document information to each chunk\n",
    "        for chunk_idx, chunk in enumerate(chunks):\n",
    "            chunk.metadata['source_doc_index'] = doc_idx\n",
    "            chunk.metadata['chunk_index'] = chunk_idx\n",
    "            chunk.metadata['original_source'] = doc.metadata.get('source', 'unknown')\n",
    "            all_chunks.append(chunk)\n",
    "    \n",
    "    return all_chunks\n",
    "\n",
    "# Process all your documents using the smart method\n",
    "all_processed_chunks = process_all_documents(\n",
    "    docs, \n",
    "    output_method='smart',  # Change to 'header' or 'recursive' if preferred\n",
    "    max_chunk_size=2000,\n",
    "    chunk_overlap=0 # deliberately set to 0\n",
    ")\n",
    "\n",
    "print(f\"\\nTotal chunks created from all documents: {len(all_processed_chunks)}\")\n",
    "\n",
    "# Show summary statistics\n",
    "if all_processed_chunks:\n",
    "    chunk_lengths = [len(chunk.page_content) for chunk in all_processed_chunks]\n",
    "    print(f\"Average chunk length: {sum(chunk_lengths) / len(chunk_lengths):.0f} characters\")\n",
    "    print(f\"Chunk length range: {min(chunk_lengths)} - {max(chunk_lengths)} characters\")\n",
    "    \n",
    "    # Show distribution by source document\n",
    "    source_counts = {}\n",
    "    for chunk in all_processed_chunks:\n",
    "        source = chunk.metadata.get('original_source', 'unknown')\n",
    "        source_counts[source] = source_counts.get(source, 0) + 1\n",
    "    \n",
    "    print(f\"\\nChunks per source document:\")\n",
    "    for source, count in source_counts.items():\n",
    "        print(f\"  {source}: {count} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373120f0",
   "metadata": {},
   "source": [
    "## VLLM Configuration for RAGAS\n",
    "\n",
    "The configuration above connects RAGAS to your vLLM server. Here are some key points:\n",
    "\n",
    "1. **Base URL**: `http://localhost:8003/v1` - your vLLM endpoint\n",
    "2. **API Key**: Set to \"not-needed\" since vLLM typically doesn't require authentication\n",
    "3. **Model Name**: Replace `\"your-model-name\"` with the actual model you're serving\n",
    "4. **Temperature**: Controls randomness (0.1 is relatively deterministic)\n",
    "5. **Max Tokens**: Maximum response length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b55eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings.base import embedding_factory\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Configure vLLM hosted LLM\n",
    "vllm_llm = ChatOpenAI(\n",
    "    base_url=\"http://localhost:8002/v1\",\n",
    "    api_key=\"asdf\",\n",
    "    model=\"deepseek-ai/DeepSeek-R1-Distill-Llama-70B\",  # Replace with your actual model name\n",
    "    temperature=0.0,\n",
    "    max_tokens=32768,\n",
    ")\n",
    "\n",
    "# Wrap for RAGAS\n",
    "llm = LangchainLLMWrapper(vllm_llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fe900c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the vLLM connection\n",
    "print(\"Testing vLLM connection...\")\n",
    "\n",
    "try:\n",
    "    # Test the LLM directly\n",
    "    test_response = vllm_llm.invoke(\"Hello, this is a test. Please respond briefly.\")\n",
    "    print(f\"‚úÖ vLLM connection successful!\")\n",
    "    print(f\"Response: {test_response.content}\")\n",
    "    \n",
    "    # Test with RAGAS wrapper\n",
    "    from ragas.llms.base import BaseRagasLLM\n",
    "    if isinstance(llm, BaseRagasLLM):\n",
    "        print(\"‚úÖ RAGAS LLM wrapper configured correctly\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  RAGAS LLM wrapper might need adjustment\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error connecting to vLLM: {e}\")\n",
    "    print(\"Please check:\")\n",
    "    print(\"1. vLLM server is running\")\n",
    "    print(\"2. Model name is correct\")\n",
    "    print(\"3. No firewall blocking the connection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7108734",
   "metadata": {},
   "source": [
    "# vLLM Hosted Embedding Model Configuration\n",
    "\n",
    "Here's how to configure a vLLM hosted embedding model for use with RAGAS:\n",
    "\n",
    "## Option 1: Using OpenAI-compatible embedding endpoint\n",
    "If your vLLM server hosts an embedding model with OpenAI-compatible API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31482f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_openai import OpenAIEmbeddings\n",
    "# from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "# import asyncio\n",
    "\n",
    "# # Try each configuration until one works\n",
    "# vllm_embeddings = OpenAIEmbeddings(\n",
    "#         base_url=\"http://localhost:8001/v1\",\n",
    "#         api_key=\"asdf\",\n",
    "#         model='thenlper/gte-large',\n",
    "#         tiktoken_enabled=False,  # Disable tiktoken for vLLM\n",
    "#     )\n",
    "\n",
    "# embedding_model = LangchainEmbeddingsWrapper(vllm_embeddings)\n",
    "\n",
    "# async def test_embedding_model(vllm_embeddings: OpenAIEmbeddings):\n",
    "#     \"\"\"Async function to test the embedding model\"\"\"\n",
    "    \n",
    "#     # Test with a simple text first\n",
    "#     print(\"testing query embedding...\")\n",
    "#     test_result = vllm_embeddings.embed_query(\"Risk management is crucial for financial institutions.\")\n",
    "#     print(f\"query embedding dimensions: {len(test_result)}\")\n",
    "\n",
    "#     # Test with texts\n",
    "#     print(\"testing text embedding...\")\n",
    "#     test_texts = [\n",
    "#         \"This is a test document about financial analysis.\",\n",
    "#         \"Machine learning models are used in banking.\",\n",
    "#         \"Risk management is crucial for financial institutions.\"\n",
    "#     ]\n",
    "#     test_results = vllm_embeddings.embed_documents(test_texts)\n",
    "#     print(f\"Text embedding dimensions: {len(test_results[0])} for {len(test_results)} texts\")\n",
    "    \n",
    "#     # If successful, wrap for RAGAS and test it through the wrapper\n",
    "#     embedding_model = LangchainEmbeddingsWrapper(vllm_embeddings)\n",
    "\n",
    "#     print(\"Testing wrapped embedding model query ...\")\n",
    "#     embedding_result = await embedding_model.embed_query(\"Risk management is crucial for financial institutions.\")\n",
    "#     print(f\"Wrapped query embedding dimensions: {len(embedding_result)}\")\n",
    "\n",
    "#     print(\"Testing wrapped embedding model text ...\")\n",
    "#     embedding_results = await embedding_model.embed_texts(test_texts, is_async=True)\n",
    "#     print(f\"Wrapped text embedding dimensions: {len(embedding_results[0])} for {len(embedding_results)} texts\")\n",
    "\n",
    "#     print(f\"‚úÖ Successfully configured vLLM embedding model\")\n",
    "#     return embedding_model\n",
    "\n",
    "# # Run the async function\n",
    "# try:\n",
    "#     embedding_model = asyncio.run(test_embedding_model(vllm_embeddings))\n",
    "#     print(f\"üéâ Using vLLM embedding model successfully!\")\n",
    "# except Exception as e:\n",
    "#     print(f\"‚ùå Failed with: {str(e)[:100]}...\")\n",
    "#     embedding_model = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66685102",
   "metadata": {},
   "source": [
    "# Option 2: local embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9123be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "\n",
    "local_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"WhereIsAI/UAE-Large-V1\",\n",
    "    model_kwargs={\"device\": \"mps\"} # Or \"cuda\" for GPU, \"mps\" for Mac \n",
    ")\n",
    "local_embeddings = LangchainEmbeddingsWrapper(local_embeddings)\n",
    "\n",
    "res = local_embeddings.embed_query(\"Who is this?\")  # Test local embedding model\n",
    "print(res)\n",
    "\n",
    "res = local_embeddings.embed_text(\"Who is this?\")\n",
    "print(res)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251cc400",
   "metadata": {},
   "source": [
    "# Default synthetic data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac392bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.transforms import (\n",
    "    default_transforms, \n",
    "    apply_transforms, \n",
    "    EmbeddingExtractor, \n",
    "    SummaryExtractor, \n",
    "    TitleExtractor,\n",
    "    HeadlinesExtractor,\n",
    "    KeyphrasesExtractor,\n",
    "    HeadlineSplitter,\n",
    "    OverlapScoreBuilder,\n",
    ")\n",
    "from ragas.testset.graph import KnowledgeGraph\n",
    "from ragas.testset.graph import Node, NodeType\n",
    "\n",
    "\n",
    "# initialize your knowledge graph\n",
    "kg = KnowledgeGraph()\n",
    "\n",
    "for chunk in all_processed_chunks:\n",
    "    kg.nodes.append(\n",
    "        Node(\n",
    "            type=NodeType.DOCUMENT,\n",
    "            properties={\"page_content\": chunk.page_content, \"metadata\": chunk.metadata},\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0860265f",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_extractor = SummaryExtractor(llm=llm)\n",
    "apply_transforms(kg, summary_extractor)\n",
    "print(\"Summary extraction complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f5013a",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_extractor = TitleExtractor(llm=llm)\n",
    "apply_transforms(kg, title_extractor)\n",
    "print(\"Title extraction complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778a461a",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyphrases_extractor = KeyphrasesExtractor(llm=llm)\n",
    "apply_transforms(kg, keyphrases_extractor)\n",
    "print(\"Keyphrases extraction complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd287d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get summary embeddings required by automatic persona generator\n",
    "summary_embedding_extractor = EmbeddingExtractor(embedding_model=local_embeddings,\n",
    "                                         property_name=\"summary_embedding\",\n",
    "                                         embed_property_name=\"summary\")\n",
    "apply_transforms(kg, summary_embedding_extractor)\n",
    "print(\"Embedding extraction complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7397628e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get regular embeddings\n",
    "regular_embedding_extractor = EmbeddingExtractor(\n",
    "    embedding_model=local_embeddings,\n",
    ")\n",
    "apply_transforms(kg, regular_embedding_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc93f285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# headline extractor\n",
    "headline_extractor = HeadlinesExtractor(llm=llm)\n",
    "apply_transforms(kg, headline_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9212a33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.transforms import ( \n",
    "    CosineSimilarityBuilder,\n",
    ")\n",
    "print(\"Before\")\n",
    "print(kg)\n",
    "cosine_similarity_builder = CosineSimilarityBuilder(threshold=0.5)\n",
    "apply_transforms(kg, cosine_similarity_builder)\n",
    "print(\"After\")\n",
    "print(kg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566c47a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.transforms import ( \n",
    "    OverlapScoreBuilder,\n",
    ")\n",
    "\n",
    "print(f\"Before\")\n",
    "print(kg)\n",
    "overlap_score_builder = OverlapScoreBuilder(property_name=\"keyphrases\", \n",
    "                                            threshold=0.3)\n",
    "apply_transforms(kg, overlap_score_builder)\n",
    "print(kg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab1195b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the default transforms as well\n",
    "print(\"Before\")\n",
    "print(kg)\n",
    "trans = default_transforms(documents=docs, llm=llm, embedding_model=local_embeddings)\n",
    "apply_transforms(kg, trans)\n",
    "print(\"After\")\n",
    "print(kg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd1e47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this debugging code to see what relationships exist\n",
    "print(\"=== Knowledge Graph Debug Info ===\")\n",
    "print(f\"Total nodes: {len(kg.nodes)}\")\n",
    "print(f\"Total relationships: {len(kg.relationships)}\")\n",
    "\n",
    "# Check relationship types\n",
    "rel_types = set()\n",
    "for rel in kg.relationships:\n",
    "    rel_types.add(rel.type)\n",
    "    \n",
    "print(f\"Relationship types found: {rel_types}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41592f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(kg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee375d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset import TestsetGenerator\n",
    "from ragas.testset.synthesizers import default_query_distribution\n",
    "\n",
    "generator = TestsetGenerator(llm=llm, embedding_model=local_embeddings, knowledge_graph=kg)\n",
    "query_distribution = default_query_distribution(llm)\n",
    "testset = generator.generate(testset_size=10, query_distribution=query_distribution)\n",
    "testset.to_pandas()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "syftr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75ee3a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, MarkdownHeaderTextSplitter\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "\n",
    "path = \"/Users/debadeepta.dey/datasets/barclays\"\n",
    "loader = DirectoryLoader(path, glob=\"**/*.md\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a61bc9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Split by Markdown Headers (Most intelligent for markdown)\n",
    "# This preserves the document structure and creates logical chunks\n",
    "\n",
    "def split_markdown_by_headers(document_content):\n",
    "    \"\"\"\n",
    "    Split markdown document by headers, preserving document structure\n",
    "    \"\"\"\n",
    "    # Define headers to split on (from h1 to h3)\n",
    "    headers_to_split_on = [\n",
    "        (\"#\", \"Header 1\"),\n",
    "        (\"##\", \"Header 2\"), \n",
    "        (\"###\", \"Header 3\"),\n",
    "    ]\n",
    "    \n",
    "    # Create the markdown header text splitter\n",
    "    markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "        headers_to_split_on=headers_to_split_on,\n",
    "        strip_headers=False  # Keep headers in the chunks\n",
    "    )\n",
    "    \n",
    "    # Split the document\n",
    "    md_header_splits = markdown_splitter.split_text(document_content)\n",
    "    \n",
    "    return md_header_splits\n",
    "\n",
    "# # Example usage with your loaded documents\n",
    "# if docs:\n",
    "#     # Take the first document as example\n",
    "#     first_doc = docs[0]\n",
    "#     header_splits = split_markdown_by_headers(first_doc.page_content)\n",
    "    \n",
    "#     print(f\"Original document split into {len(header_splits)} chunks based on headers\")\n",
    "    \n",
    "#     # Display first few chunks\n",
    "#     for i, chunk in enumerate(header_splits[:3]):\n",
    "#         print(f\"\\n--- Chunk {i+1} ---\")\n",
    "#         print(f\"Content: {chunk.page_content[:200]}...\")\n",
    "#         print(f\"Metadata: {chunk.metadata}\")\n",
    "#         print(f\"Full length: {len(chunk.page_content)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c057368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Recursive Character Text Splitter (Good fallback)\n",
    "# This method is useful when documents don't have clear header structure\n",
    "\n",
    "def split_markdown_recursive(document_content, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Split markdown using recursive character splitter with markdown-aware separators\n",
    "    \"\"\"\n",
    "    # Define separators that work well for markdown\n",
    "    markdown_separators = [\n",
    "        \"\\n\\n\",  # Double newline (paragraph breaks)\n",
    "        \"\\n\",    # Single newline\n",
    "        \" \",     # Space\n",
    "        \"\"       # Character level\n",
    "    ]\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=markdown_separators,\n",
    "        length_function=len,\n",
    "    )\n",
    "    \n",
    "    # Split the document\n",
    "    chunks = text_splitter.split_text(document_content)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# # Example usage\n",
    "# if docs:\n",
    "#     first_doc = docs[0]\n",
    "#     recursive_chunks = split_markdown_recursive(\n",
    "#         first_doc.page_content, \n",
    "#         chunk_size=2048,  # Adjust based on your needs\n",
    "#         chunk_overlap=200\n",
    "#     )\n",
    "    \n",
    "#     print(f\"\\nRecursive splitting created {len(recursive_chunks)} chunks\")\n",
    "    \n",
    "#     # Display first few chunks\n",
    "#     for i, chunk in enumerate(recursive_chunks[:3]):\n",
    "#         print(f\"\\n--- Recursive Chunk {i+1} ---\")\n",
    "#         print(f\"Content: {chunk[:200]}...\")\n",
    "#         print(f\"Length: {len(chunk)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39fc45fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 3: Hybrid Approach (Recommended)\n",
    "# Combine header-based splitting with recursive splitting for optimal results\n",
    "\n",
    "def smart_markdown_split(document_content, max_chunk_size=1500, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Smart markdown splitting that combines header-based and recursive approaches\n",
    "    \"\"\"\n",
    "    from langchain.schema import Document\n",
    "    \n",
    "    # First, try to split by headers\n",
    "    headers_to_split_on = [\n",
    "        (\"#\", \"Header 1\"),\n",
    "        (\"##\", \"Header 2\"), \n",
    "        (\"###\", \"Header 3\"),\n",
    "        (\"####\", \"Header 4\"),\n",
    "    ]\n",
    "    \n",
    "    markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "        headers_to_split_on=headers_to_split_on,\n",
    "        strip_headers=False\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Split by headers first\n",
    "        header_splits = markdown_splitter.split_text(document_content)\n",
    "        \n",
    "        # If header splits are too large, further split them recursively\n",
    "        final_chunks = []\n",
    "        \n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=max_chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "            length_function=len,\n",
    "        )\n",
    "        \n",
    "        for doc in header_splits:\n",
    "            if len(doc.page_content) > max_chunk_size:\n",
    "                # Split large chunks further\n",
    "                sub_chunks = text_splitter.split_text(doc.page_content)\n",
    "                for i, sub_chunk in enumerate(sub_chunks):\n",
    "                    # Preserve metadata from header splitting\n",
    "                    new_metadata = doc.metadata.copy()\n",
    "                    new_metadata['sub_chunk'] = i\n",
    "                    final_chunks.append(Document(\n",
    "                        page_content=sub_chunk,\n",
    "                        metadata=new_metadata\n",
    "                    ))\n",
    "            else:\n",
    "                final_chunks.append(doc)\n",
    "                \n",
    "        return final_chunks\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Header splitting failed: {e}\")\n",
    "        # Fallback to recursive splitting\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=max_chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "        )\n",
    "        chunks = text_splitter.split_text(document_content)\n",
    "        return [Document(page_content=chunk, metadata={}) for chunk in chunks]\n",
    "\n",
    "# # Example usage with the hybrid approach\n",
    "# if docs:\n",
    "#     first_doc = docs[0]\n",
    "#     smart_chunks = smart_markdown_split(\n",
    "#         first_doc.page_content,\n",
    "#         max_chunk_size=1200,\n",
    "#         chunk_overlap=150\n",
    "#     )\n",
    "    \n",
    "#     print(f\"\\nSmart splitting created {len(smart_chunks)} chunks\")\n",
    "    \n",
    "#     # Display statistics\n",
    "#     chunk_lengths = [len(chunk.page_content) for chunk in smart_chunks]\n",
    "#     print(f\"Average chunk length: {sum(chunk_lengths) / len(chunk_lengths):.0f} characters\")\n",
    "#     print(f\"Min chunk length: {min(chunk_lengths)} characters\")\n",
    "#     print(f\"Max chunk length: {max(chunk_lengths)} characters\")\n",
    "    \n",
    "#     # Display first few chunks with metadata\n",
    "#     for i, chunk in enumerate(smart_chunks[:3]):\n",
    "#         print(f\"\\n--- Smart Chunk {i+1} ---\")\n",
    "#         print(f\"Metadata: {chunk.metadata}\")\n",
    "#         print(f\"Content preview: {chunk.page_content[:200]}...\")\n",
    "#         print(f\"Length: {len(chunk.page_content)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7956566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing document 1/1: /Users/debadeepta.dey/datasets/barclays/rise-insights-report-making-data-count-with-ai-DIGITAL.md\n",
      "\n",
      "Total chunks created from all documents: 13\n",
      "Average chunk length: 9404 characters\n",
      "Chunk length range: 2316 - 10000 characters\n",
      "\n",
      "Chunks per source document:\n",
      "  /Users/debadeepta.dey/datasets/barclays/rise-insights-report-making-data-count-with-ai-DIGITAL.md: 13 chunks\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "# Utility function to process all your documents\n",
    "def process_all_documents(docs, output_method='smart', **kwargs):\n",
    "    \"\"\"\n",
    "    Process all loaded documents and return chunks\n",
    "    \n",
    "    Args:\n",
    "        docs: List of loaded documents\n",
    "        output_method: 'header', 'recursive', or 'smart'\n",
    "        **kwargs: Additional parameters for the splitting methods\n",
    "    \n",
    "    Returns:\n",
    "        List of all chunks with source document information\n",
    "    \"\"\"\n",
    "    all_chunks = []\n",
    "    \n",
    "    for doc_idx, doc in enumerate(docs):\n",
    "        print(f\"Processing document {doc_idx + 1}/{len(docs)}: {doc.metadata.get('source', 'unknown')}\")\n",
    "        \n",
    "        if output_method == 'header':\n",
    "            chunks = split_markdown_by_headers(doc.page_content)\n",
    "        elif output_method == 'recursive':\n",
    "            chunk_texts = split_markdown_recursive(doc.page_content, **kwargs)\n",
    "            chunks = [Document(page_content=text, metadata=doc.metadata.copy()) for text in chunk_texts]\n",
    "        elif output_method == 'smart':\n",
    "            chunks = smart_markdown_split(doc.page_content, **kwargs)\n",
    "        else:\n",
    "            raise ValueError(\"output_method must be 'header', 'recursive', or 'smart'\")\n",
    "        \n",
    "        # Add source document information to each chunk\n",
    "        for chunk_idx, chunk in enumerate(chunks):\n",
    "            chunk.metadata['source_doc_index'] = doc_idx\n",
    "            chunk.metadata['chunk_index'] = chunk_idx\n",
    "            chunk.metadata['original_source'] = doc.metadata.get('source', 'unknown')\n",
    "            all_chunks.append(chunk)\n",
    "    \n",
    "    return all_chunks\n",
    "\n",
    "# Process all your documents using the smart method\n",
    "all_processed_chunks = process_all_documents(\n",
    "    docs, \n",
    "    output_method='smart',  # Change to 'header' or 'recursive' if preferred\n",
    "    max_chunk_size=10000,\n",
    "    chunk_overlap=0 # deliberately set to 0\n",
    ")\n",
    "\n",
    "print(f\"\\nTotal chunks created from all documents: {len(all_processed_chunks)}\")\n",
    "\n",
    "# Show summary statistics\n",
    "if all_processed_chunks:\n",
    "    chunk_lengths = [len(chunk.page_content) for chunk in all_processed_chunks]\n",
    "    print(f\"Average chunk length: {sum(chunk_lengths) / len(chunk_lengths):.0f} characters\")\n",
    "    print(f\"Chunk length range: {min(chunk_lengths)} - {max(chunk_lengths)} characters\")\n",
    "    \n",
    "    # Show distribution by source document\n",
    "    source_counts = {}\n",
    "    for chunk in all_processed_chunks:\n",
    "        source = chunk.metadata.get('original_source', 'unknown')\n",
    "        source_counts[source] = source_counts.get(source, 0) + 1\n",
    "    \n",
    "    print(f\"\\nChunks per source document:\")\n",
    "    for source, count in source_counts.items():\n",
    "        print(f\"  {source}: {count} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85c98727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded .env from: /Users/debadeepta.dey/sources/syftr/runtime-secrets/azure_openai_gpt_4o_mini.env\n",
      "content='I am an AI language model created by OpenAI, designed to assist with a wide range of questions and tasks by providing information, answering queries, and engaging in conversation. How can I help you today?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 42, 'prompt_tokens': 11, 'total_tokens': 53, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_efad92c60b', 'id': 'chatcmpl-C1KqBbH5rfDvFNFEhK8ihUh13bCTR', 'service_tier': None, 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}} id='run--83f67356-8e27-46fd-9796-a26797ab7fa8-0' usage_metadata={'input_tokens': 11, 'output_tokens': 42, 'total_tokens': 53, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "Azure OpenAI LLM configured successfully!\n"
     ]
    }
   ],
   "source": [
    "# Use gpt-4o-mini Azure OpenAI model\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "# Specify the directory containing your .env file\n",
    "env_directory = \"/Users/debadeepta.dey/sources/syftr/runtime-secrets\"  # Change this to your desired directory\n",
    "env_file_path = Path(env_directory) / \"azure_openai_gpt_4o_mini.env\"\n",
    "\n",
    "# Load environment variables from the specified directory\n",
    "load_dotenv(dotenv_path=env_file_path)\n",
    "\n",
    "# Verify the .env file was found and loaded\n",
    "if env_file_path.exists():\n",
    "    print(f\"✅ Loaded .env from: {env_file_path}\")\n",
    "else:\n",
    "    print(f\"⚠️  .env file not found at: {env_file_path}\")\n",
    "    print(\"Please create the .env file with your Azure OpenAI credentials\")\n",
    "\n",
    "# Configure Azure OpenAI GPT-4o-mini\n",
    "azure_llm = AzureChatOpenAI(\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    azure_deployment=os.getenv(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "    temperature=0.0,\n",
    "    max_tokens=16384,\n",
    ")\n",
    "\n",
    "# Test the model \n",
    "print(azure_llm.invoke(\"Who are you?\"))\n",
    "\n",
    "# Wrap for RAGAS\n",
    "llm = LangchainLLMWrapper(azure_llm)\n",
    "\n",
    "print(\"Azure OpenAI LLM configured successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373120f0",
   "metadata": {},
   "source": [
    "## VLLM Configuration for RAGAS\n",
    "\n",
    "The configuration above connects RAGAS to your vLLM server. Here are some key points:\n",
    "\n",
    "1. **Base URL**: `http://localhost:8003/v1` - your vLLM endpoint\n",
    "2. **API Key**: Set to \"not-needed\" since vLLM typically doesn't require authentication\n",
    "3. **Model Name**: Replace `\"your-model-name\"` with the actual model you're serving\n",
    "4. **Temperature**: Controls randomness (0.1 is relatively deterministic)\n",
    "5. **Max Tokens**: Maximum response length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5b55eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ragas.llms import LangchainLLMWrapper\n",
    "# from ragas.embeddings.base import embedding_factory\n",
    "# from langchain_openai import ChatOpenAI\n",
    "\n",
    "# # Configure vLLM hosted LLM\n",
    "# vllm_llm = ChatOpenAI(\n",
    "#     base_url=\"http://localhost:8014/v1\",\n",
    "#     api_key=\"asdf\",\n",
    "#     model=\"nvidia/Llama-3_3-Nemotron-Super-49B\",  # Replace with your actual model name\n",
    "#     temperature=0.0,\n",
    "#     max_tokens=32768,\n",
    "# )\n",
    "\n",
    "# # Wrap for RAGAS\n",
    "# llm = LangchainLLMWrapper(vllm_llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58fe900c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test the vLLM connection\n",
    "# print(\"Testing vLLM connection...\")\n",
    "\n",
    "# try:\n",
    "#     # Test the LLM directly\n",
    "#     test_response = vllm_llm.invoke(\"Hello, this is a test. Please respond briefly.\")\n",
    "#     print(f\"✅ vLLM connection successful!\")\n",
    "#     print(f\"Response: {test_response.content}\")\n",
    "    \n",
    "#     # Test with RAGAS wrapper\n",
    "#     from ragas.llms.base import BaseRagasLLM\n",
    "#     if isinstance(llm, BaseRagasLLM):\n",
    "#         print(\"✅ RAGAS LLM wrapper configured correctly\")\n",
    "#     else:\n",
    "#         print(\"⚠️  RAGAS LLM wrapper might need adjustment\")\n",
    "        \n",
    "# except Exception as e:\n",
    "#     print(f\"❌ Error connecting to vLLM: {e}\")\n",
    "#     print(\"Please check:\")\n",
    "#     print(\"1. vLLM server is running\")\n",
    "#     print(\"2. Model name is correct\")\n",
    "#     print(\"3. No firewall blocking the connection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7108734",
   "metadata": {},
   "source": [
    "# vLLM Hosted Embedding Model Configuration\n",
    "\n",
    "Here's how to configure a vLLM hosted embedding model for use with RAGAS:\n",
    "\n",
    "## Option 1: Using OpenAI-compatible embedding endpoint\n",
    "If your vLLM server hosts an embedding model with OpenAI-compatible API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b31482f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_openai import OpenAIEmbeddings\n",
    "# from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "# import asyncio\n",
    "\n",
    "# # Try each configuration until one works\n",
    "# vllm_embeddings = OpenAIEmbeddings(\n",
    "#         base_url=\"http://localhost:8001/v1\",\n",
    "#         api_key=\"asdf\",\n",
    "#         model='thenlper/gte-large',\n",
    "#         tiktoken_enabled=False,  # Disable tiktoken for vLLM\n",
    "#     )\n",
    "\n",
    "# embedding_model = LangchainEmbeddingsWrapper(vllm_embeddings)\n",
    "\n",
    "# async def test_embedding_model(vllm_embeddings: OpenAIEmbeddings):\n",
    "#     \"\"\"Async function to test the embedding model\"\"\"\n",
    "    \n",
    "#     # Test with a simple text first\n",
    "#     print(\"testing query embedding...\")\n",
    "#     test_result = vllm_embeddings.embed_query(\"Risk management is crucial for financial institutions.\")\n",
    "#     print(f\"query embedding dimensions: {len(test_result)}\")\n",
    "\n",
    "#     # Test with texts\n",
    "#     print(\"testing text embedding...\")\n",
    "#     test_texts = [\n",
    "#         \"This is a test document about financial analysis.\",\n",
    "#         \"Machine learning models are used in banking.\",\n",
    "#         \"Risk management is crucial for financial institutions.\"\n",
    "#     ]\n",
    "#     test_results = vllm_embeddings.embed_documents(test_texts)\n",
    "#     print(f\"Text embedding dimensions: {len(test_results[0])} for {len(test_results)} texts\")\n",
    "    \n",
    "#     # If successful, wrap for RAGAS and test it through the wrapper\n",
    "#     embedding_model = LangchainEmbeddingsWrapper(vllm_embeddings)\n",
    "\n",
    "#     print(\"Testing wrapped embedding model query ...\")\n",
    "#     embedding_result = await embedding_model.embed_query(\"Risk management is crucial for financial institutions.\")\n",
    "#     print(f\"Wrapped query embedding dimensions: {len(embedding_result)}\")\n",
    "\n",
    "#     print(\"Testing wrapped embedding model text ...\")\n",
    "#     embedding_results = await embedding_model.embed_texts(test_texts, is_async=True)\n",
    "#     print(f\"Wrapped text embedding dimensions: {len(embedding_results[0])} for {len(embedding_results)} texts\")\n",
    "\n",
    "#     print(f\"✅ Successfully configured vLLM embedding model\")\n",
    "#     return embedding_model\n",
    "\n",
    "# # Run the async function\n",
    "# try:\n",
    "#     embedding_model = asyncio.run(test_embedding_model(vllm_embeddings))\n",
    "#     print(f\"🎉 Using vLLM embedding model successfully!\")\n",
    "# except Exception as e:\n",
    "#     print(f\"❌ Failed with: {str(e)[:100]}...\")\n",
    "#     embedding_model = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66685102",
   "metadata": {},
   "source": [
    "# Option 2: local embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e9123be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2c/yssyvkk54_b21htl90_2g_080000gp/T/ipykernel_6993/807525259.py:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  local_embeddings = HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.22206830978393555, 0.051233645528554916, -0.491749107837677, -0.4322299361228943, 0.5002784132957458, -0.2561419606208801, -0.22091081738471985, 0.11627580225467682, 0.4728822112083435, 0.08462154865264893, 0.3242437243461609, -0.19702470302581787, -0.36262261867523193, -0.2812041640281677, -0.36116212606430054, 0.08653219789266586, 0.08922451734542847, -0.16965575516223907, -0.39888814091682434, -0.2973982095718384, 0.6447439789772034, 0.7650296092033386, -0.6067692637443542, -0.3339347541332245, 0.3801654875278473, -0.3341101408004761, -0.16917163133621216, -0.010672621428966522, 0.13589155673980713, 0.7128159999847412, -0.6217780113220215, -0.042145416140556335, -0.07324082404375076, -1.0624228715896606, -0.01020888052880764, -0.7508808374404907, 0.5139471292495728, -0.9261746406555176, -0.4445686340332031, -0.7835230827331543, -0.4164794087409973, 0.0833824947476387, 0.30117857456207275, -0.637333333492279, -0.7911930084228516, 0.09582684934139252, -0.3917580246925354, 0.08683580905199051, 0.6413971185684204, -0.8161498308181763, -0.02510622888803482, 0.3480791747570038, 0.5102754831314087, 0.010825647972524166, 0.024440117180347443, -0.49215203523635864, 0.45990410447120667, 0.020236387848854065, -0.056113459169864655, 0.620966911315918, -0.263802707195282, 0.2514188885688782, 0.43374067544937134, -1.5778640508651733, -0.3432634174823761, -0.10514787584543228, 0.4252501130104065, -0.7035882472991943, 0.7603548169136047, -0.24159115552902222, -0.12211216986179352, 0.5205444097518921, 0.10612176358699799, -0.29616260528564453, -0.9526011347770691, 0.1380445957183838, -0.9346005916595459, -0.12144991010427475, 0.32955944538116455, 0.6389495730400085, -0.11909645795822144, 0.80478435754776, -1.0629146099090576, 0.09499762952327728, -0.2559436857700348, -0.3419335186481476, -0.041599392890930176, 0.16329579055309296, -0.7427644729614258, 0.4264370799064636, 1.4789419174194336, 0.953029215335846, 0.1947574019432068, -0.40775495767593384, 0.7413784265518188, -0.07727555930614471, 0.04623107612133026, 0.4576510488986969, 0.16363339126110077, 0.38255757093429565, 0.6922783851623535, 0.3602486848831177, -0.07548847049474716, 0.8179692625999451, -0.23670029640197754, -0.1635613739490509, -0.18866169452667236, -0.8018100261688232, -0.96413254737854, -0.3531401455402374, 0.00995999202132225, -0.05385060980916023, 0.579920768737793, 0.06017041206359863, -0.49051687121391296, 0.7909472584724426, 0.15073327720165253, 0.6010760068893433, -0.5779573321342468, -0.17494668066501617, 0.0852322056889534, 0.5269415378570557, -0.1334744244813919, -0.31674081087112427, 0.4833444654941559, -0.030565230175852776, -0.5749952793121338, 0.33473968505859375, 0.0004319101572036743, 0.45592737197875977, -0.18767769634723663, -0.8602790236473083, 0.3829115033149719, -0.0802556723356247, 0.09994492679834366, 0.7468171715736389, 0.11103043705224991, 0.6189877390861511, 0.49376949667930603, -0.24391292035579681, 0.9451703429222107, 0.13011273741722107, 0.5095144510269165, 1.255920648574829, -0.7403191924095154, 0.6105400919914246, -0.2897707521915436, 0.18060539662837982, -0.6257873773574829, -0.23152831196784973, -0.31635817885398865, 0.42520201206207275, -0.6913508176803589, 0.7072970867156982, -0.5001314878463745, 0.03753262013196945, -0.17148220539093018, 0.42012453079223633, 0.392981618642807, 0.1883641481399536, -0.611476480960846, 0.30196577310562134, 0.43094030022621155, 0.11945705115795135, -0.39281782507896423, 0.21264547109603882, 0.3656338155269623, 0.24542608857154846, 0.0846385806798935, 0.2507622241973877, 0.33082717657089233, 0.9564147591590881, -0.9108942747116089, 0.2504575550556183, -0.08718903362751007, 0.11290307343006134, -0.18700942397117615, -0.1247681975364685, 0.5808479189872742, 0.7902886867523193, -0.40303218364715576, -0.2600286602973938, -0.16773200035095215, 0.43643248081207275, -0.43649181723594666, 0.3036612272262573, 0.20779652893543243, -0.4620950520038605, -0.5630983710289001, -0.06565248221158981, -0.1545879989862442, 0.5325674414634705, -0.030392250046133995, 0.11382918059825897, 0.3787766993045807, 0.7037674188613892, -0.7097041606903076, 0.012877002358436584, -0.07430759817361832, -0.7057828307151794, -0.25471949577331543, 0.9276160597801208, -0.05897224694490433, -0.13378897309303284, 0.10329968482255936, -0.2711450755596161, 0.590972900390625, 0.7145387530326843, -0.9214826226234436, -0.5122234225273132, 0.7867801189422607, 0.19255970418453217, -0.5133690237998962, 0.33079585433006287, 0.04017431288957596, -0.574737548828125, -0.47399821877479553, 0.5004295110702515, -0.20272919535636902, 0.13311152160167694, 0.015913158655166626, 0.6636849045753479, 0.5761948227882385, 1.1156107187271118, 0.3925614356994629, -0.056861184537410736, -0.22298766672611237, 0.713234007358551, -0.09035839140415192, -0.6036624312400818, -0.17358636856079102, 0.006314091384410858, -0.4152955114841461, 1.1463758945465088, -0.031577859073877335, 0.5993724465370178, 0.7650524377822876, 0.9349228739738464, -0.1082908883690834, 0.06031068041920662, -0.010153815150260925, 0.23446504771709442, 1.292172908782959, 0.5910046100616455, 0.22069312632083893, 0.018930751830339432, 0.8708239793777466, -0.06955023854970932, -0.4228111803531647, 0.34676605463027954, -0.08359949290752411, 0.4039374887943268, 0.5903169512748718, 0.5580243468284607, -0.1924194097518921, 0.14943927526474, 0.018999025225639343, 0.3507327437400818, -0.6487327814102173, -0.02605743706226349, 0.018284596502780914, -0.710110068321228, 0.24816735088825226, -0.568193256855011, -0.28102582693099976, -0.20176838338375092, 0.6107132434844971, 0.4522159993648529, -0.2823016047477722, -1.3116886615753174, -0.05152880400419235, -1.0032899379730225, -0.2586948573589325, 0.25646743178367615, -0.30640578269958496, 0.1625765562057495, 0.8328151106834412, -0.31409648060798645, 0.5652180910110474, -0.8017776012420654, 0.21610838174819946, -0.04218677431344986, -1.3392473459243774, 0.29432472586631775, -0.27536314725875854, 0.13420593738555908, -0.08812867105007172, 0.2345127910375595, 0.302409827709198, 0.6412661671638489, 0.02387409098446369, 0.23204033076763153, -0.3081594407558441, -0.19499143958091736, -0.19224198162555695, -0.1255355179309845, -0.6842144727706909, -0.6973284482955933, -0.48616722226142883, -0.34260624647140503, -0.008067015558481216, 0.43065735697746277, 0.2141490876674652, 0.20145292580127716, -0.24903763830661774, -0.4520299732685089, 0.740027904510498, -0.45582491159439087, 0.36073970794677734, 0.5021517872810364, -0.011143525131046772, 0.6358915567398071, -0.11866289377212524, -0.1446155458688736, -0.6916542053222656, 0.8480026125907898, 0.34276825189590454, 0.3864991366863251, -0.27098751068115234, 0.01104971393942833, -0.30847612023353577, -0.24240127205848694, -0.6362023949623108, -1.2759771347045898, -0.2525508999824524, 0.814935564994812, -0.03627192974090576, -0.5185859203338623, 0.9258391857147217, -1.267395257949829, -0.8815793991088867, -0.6668513417243958, 0.2209962159395218, 0.10606691986322403, 0.5050491690635681, 0.44652992486953735, 0.7180901765823364, 0.5007623434066772, -0.1082463189959526, -0.552284300327301, 0.7648987174034119, -0.7459015250205994, -0.061985909938812256, 0.40404069423675537, -0.8538017272949219, 0.15434351563453674, -0.37091386318206787, -0.7514662742614746, -0.0942593440413475, 0.2271822690963745, -0.1176825538277626, 0.4039178490638733, -0.19903744757175446, 0.39622700214385986, -0.01838812232017517, 0.36763426661491394, -0.6811727285385132, 0.4131803810596466, 0.24000674486160278, -0.48281192779541016, 0.4657175540924072, 0.9786943197250366, 0.37565070390701294, -1.4095298051834106, 0.14575780928134918, -1.5140033960342407, -1.2854968309402466, 0.6898131966590881, 0.8123413920402527, -1.1031156778335571, 0.23612993955612183, -0.7337303161621094, -0.21338340640068054, -0.2557915151119232, -1.0366368293762207, -0.9773082137107849, 0.26225200295448303, -0.7594544887542725, 0.5674275755882263, -1.0628160238265991, 0.3222675025463104, -0.027141325175762177, 0.015454299747943878, 0.7223178744316101, 0.6297519207000732, 0.07065580040216446, -0.6332762837409973, -0.40598154067993164, 0.30175191164016724, -0.013400204479694366, -0.1525755524635315, -0.3171244263648987, 0.4050084352493286, -0.028741247951984406, -1.2786402702331543, -0.44408202171325684, 0.0814252495765686, 0.15443848073482513, 0.43814074993133545, -0.5226649045944214, 0.9359182119369507, 0.1445578932762146, 0.7588184475898743, 0.5867152214050293, -1.0921167135238647, 0.31939321756362915, -0.3776671588420868, 0.3246181309223175, 0.41631820797920227, -0.00887889415025711, -0.7263317704200745, 1.295341968536377, -0.08899460732936859, 0.7128781080245972, 1.7156736850738525, -0.6387289762496948, -0.15874388813972473, 0.4389239549636841, 0.2161063402891159, 0.4482286870479584, -0.658261239528656, -0.5197750329971313, -0.37222883105278015, 0.24936100840568542, 0.6853222846984863, 0.1384570151567459, -0.07979975640773773, 0.15102815628051758, 1.1881568431854248, 0.20890505611896515, -0.2474147528409958, -0.984214186668396, -0.08531317114830017, -0.17781323194503784, -0.22325417399406433, 0.31801506876945496, 0.5755155682563782, -0.5036711692810059, 0.5869247317314148, -0.7645082473754883, 1.0461950302124023, 0.6504847407341003, -0.20249351859092712, 0.0020838589407503605, -0.22445939481258392, -0.37094515562057495, -0.08948037028312683, -0.11593122780323029, -0.0971602126955986, -0.34310832619667053, -0.0446130707859993, -0.560699462890625, 0.3856556713581085, 0.06115186959505081, -0.21990826725959778, -0.1989501565694809, -0.1891718953847885, 0.02213294804096222, 0.07827059179544449, 0.03921585530042648, -0.5514841675758362, -0.2527831196784973, 0.8617922067642212, -0.13110125064849854, -0.20128662884235382, 1.037947654724121, -0.04265913367271423, 0.4678521454334259, 0.5466118454933167, 1.0674662590026855, 0.07827024161815643, 0.8669664263725281, 0.42746269702911377, -0.5992133021354675, 0.6819144487380981, -0.2565828561782837, 0.4180993139743805, -0.09419139474630356, -0.3063737452030182, 0.48054561018943787, -0.45660993456840515, -0.3482947051525116, -0.21240220963954926, -0.29417604207992554, -0.29830536246299744, -1.0606456995010376, -0.14861918985843658, -0.0547468438744545, 0.04813038557767868, -0.12510336935520172, -0.17466428875923157, 0.2294330596923828, -0.7129305601119995, -0.5552430748939514, 0.5729092955589294, 0.1508452296257019, -1.0147731304168701, 0.5306758284568787, -0.14270150661468506, 0.1814926564693451, 0.4666381776332855, 0.4753538966178894, -0.4851340353488922, -0.1640114188194275, -0.7283827662467957, -0.25565963983535767, -0.7787041068077087, 0.6077859997749329, -0.261788547039032, -0.02850327268242836, -0.2775169014930725, 0.3121562600135803, -0.271651953458786, 0.23871487379074097, -0.11868853867053986, -0.1356348991394043, 0.04896868020296097, 0.6431950926780701, -0.2079540640115738, 0.046006180346012115, 0.8164235353469849, -1.541909098625183, -0.5494663119316101, 0.6098678112030029, 0.32153987884521484, 0.45893147587776184, 0.8873024582862854, -0.2925199270248413, -0.004979402758181095, 0.24975627660751343, 0.2772303819656372, 0.21372418105602264, -0.157757967710495, -0.7996281385421753, -0.032356515526771545, -0.08211791515350342, 0.2764214277267456, -0.11974275857210159, -0.10908538103103638, 0.2584574818611145, -0.25631019473075867, 0.6360591053962708, 0.031041603535413742, -0.5721611380577087, -0.8286163210868835, -0.034722477197647095, 0.6156883835792542, 1.2981756925582886, -0.6226435303688049, 0.7592233419418335, 0.05466751381754875, 0.19871504604816437, 0.41985419392585754, -0.39068862795829773, -1.118241548538208, -0.4035235047340393, 0.043802715837955475, 0.1647946536540985, 0.25339069962501526, 0.6945303082466125, -0.16637933254241943, 0.6562715172767639, -0.7090114951133728, -0.5814447999000549, 0.04399368166923523, -0.46232980489730835, -0.4791311025619507, -0.39607474207878113, 0.9434999823570251, -0.651072084903717, -0.27703240513801575, -0.203446164727211, 0.5483949184417725, 1.0088896751403809, -0.25865939259529114, -0.2172859162092209, -0.7508636713027954, -0.025567831471562386, -0.5114839673042297, -0.3128647208213806, 0.06309446692466736, -0.4710352420806885, -0.34758010506629944, -0.17412987351417542, 0.4773363769054413, -0.42678558826446533, 0.5239182114601135, 0.7056219577789307, 0.40053969621658325, -0.8464168310165405, -0.73720383644104, 0.27641546726226807, -0.20270031690597534, -0.252881795167923, -0.780498206615448, -0.9208337068557739, -1.1944966316223145, -0.6646004319190979, -0.2649098336696625, -0.5816936492919922, -0.8729163408279419, -0.05607964098453522, 0.5642736554145813, 0.5568316578865051, 0.7801082730293274, 0.10713938623666763, -0.5616562366485596, -0.8037165999412537, 0.7066240310668945, 0.8743737936019897, 0.29266679286956787, 1.1542059183120728, 0.23777170479297638, 0.026892662048339844, 0.6302958726882935, -0.19847196340560913, 0.07536302506923676, -0.8499016761779785, 0.38230985403060913, -0.5627231001853943, 0.0626152753829956, 0.8949264883995056, 1.5403878688812256, -0.6243395805358887, -0.5547260046005249, -0.10128147900104523, 0.12849721312522888, -0.3069422245025635, -0.061402931809425354, -0.2914310395717621, -0.7957074046134949, -1.1769731044769287, 0.06351956725120544, 0.6681305766105652, 0.06846815347671509, 0.23317782580852509, -0.2066756635904312, -0.014857541769742966, -0.8030973076820374, 0.6748805046081543, 0.42420321702957153, -0.8305175304412842, -0.14418913424015045, -1.269805908203125, -0.4826354384422302, -0.29452192783355713, 0.37696608901023865, -0.15359511971473694, -0.2019418478012085, 0.11883046478033066, 0.21170149743556976, 0.4499402642250061, 0.32189297676086426, -0.4234473705291748, -0.41362273693084717, -0.14334769546985626, 0.2764754593372345, -0.7711341977119446, -0.2757917642593384, 0.34207722544670105, 0.2053879201412201, 0.4910341203212738, -0.8184255361557007, -0.7768811583518982, 1.2147395610809326, 0.954680323600769, -0.3844239115715027, 0.2798362374305725, -1.0552504062652588, -0.5727007985115051, -0.7121825218200684, -0.9242633581161499, -0.37718522548675537, -0.301349401473999, 0.22187042236328125, 0.23794937133789062, 0.08720408380031586, -0.25507649779319763, 0.41910961270332336, 0.022886604070663452, 0.5998625159263611, -0.5944249033927917, 0.8525524139404297, -0.19529837369918823, -0.9557707905769348, -0.10399741679430008, -0.3055076003074646, -0.34845206141471863, -0.04063227400183678, 0.03803376480937004, 0.3383771479129791, -1.0549300909042358, 0.9997596144676208, 0.4898747503757477, 0.17816725373268127, -0.3527342975139618, -0.4685550332069397, -0.29975828528404236, 1.0553390979766846, 0.563946008682251, -0.2424485981464386, 0.3357105851173401, -0.6263434290885925, -0.015409361571073532, 0.6546840071678162, -0.8105987906455994, 0.09915933012962341, -0.1794392317533493, -0.09397747367620468, 0.43894827365875244, -0.3093910217285156, -0.4455314874649048, -0.3719451427459717, -0.706433892250061, 0.36237478256225586, 0.8723920583724976, 0.34547024965286255, -0.3342101573944092, 0.10078072547912598, 0.5211973786354065, 0.37258610129356384, 0.19500891864299774, 0.7663209438323975, 0.42451441287994385, 0.34050995111465454, -0.062398817390203476, -0.6188636422157288, 0.946000337600708, -0.32670897245407104, -0.033793121576309204, 0.7378658652305603, -0.3581313490867615, 0.5424721837043762, -0.36526280641555786, 0.4598191976547241, -0.8156595230102539, -0.5662054419517517, -1.2252010107040405, -0.2930842936038971, 0.4530240595340729, 0.18438459932804108, 0.7042012214660645, 0.27837175130844116, -0.04303968697786331, -0.7966680526733398, -0.3919539451599121, 0.002221032977104187, -0.8174437284469604, -0.6404602527618408, 0.4818393588066101, 0.08010005205869675, 0.12037386000156403, -0.4302947521209717, -0.08520890027284622, 0.5163129568099976, 0.130588561296463, 0.4698774516582489, -0.2883262038230896, -0.28287166357040405, 0.16163599491119385, -0.420818567276001, -0.2646302580833435, -0.15854531526565552, -0.1143244057893753, 0.6597723364830017, -0.24875786900520325, -0.3467556834220886, -0.25884008407592773, -0.3937869369983673, 0.6891432404518127, -0.6319320201873779, 0.4417627453804016, 0.09756606072187424, 0.11538080126047134, -0.3119676113128662, 0.38487866520881653, 0.059830792248249054, -0.2533300220966339, 0.7183233499526978, -0.5108522176742554, 0.7558802366256714, -0.6503502130508423, 0.5191322565078735, 0.6491400599479675, 0.006115313619375229, 0.5202792286872864, 0.5932022929191589, 0.4061599373817444, -0.36202314496040344, 0.1043744683265686, 0.0634131208062172, 0.6371746063232422, -0.19093655049800873, -0.04131127521395683, 0.010666681453585625, 0.8668831586837769, 0.40003135800361633, 0.15795038640499115, -0.06409469991922379, -0.011622752994298935, 0.9866329431533813, 0.3260645866394043, 0.5567108988761902, 0.5367836356163025, 0.5314672589302063, -0.4104536473751068, 0.5584492087364197, -0.48865431547164917, 0.01132892444729805, -0.18405358493328094, 0.0483100563287735, 0.5007781386375427, -0.20864588022232056, -0.3894909620285034, -0.5400816202163696, 0.4535323977470398, 0.3386614918708801, 0.1336187869310379, 0.7032814025878906, 0.12380758672952652, -0.784544825553894, -0.33869028091430664, 0.952115535736084, 0.21288099884986877, -0.4362486004829407, -0.3039332628250122, 0.3608500361442566, 0.12496694177389145, 0.2990568280220032, 0.6239490509033203, -0.21996989846229553, -0.8219749331474304, -0.3550947308540344, 0.8126183152198792, 0.45749950408935547, -0.03656564652919769, 0.29292207956314087, -0.07314600795507431, 0.14209941029548645, -1.1075401306152344, 0.18038755655288696, 0.7969664335250854, -0.24063915014266968, -0.8047633767127991, -0.5673301219940186, 0.49321669340133667, 0.13373695313930511, 0.2664448916912079, 0.45545172691345215, -0.14854022860527039, -0.2022688090801239, 0.6770399808883667, -0.3456427752971649, 0.47716450691223145, -0.15163227915763855, 0.0057330019772052765, -0.5619528889656067, 0.21276679635047913, 0.018115054816007614, -0.4351975917816162, -0.5722365975379944, 0.09164606779813766, -0.26240187883377075, -1.0702811479568481, -0.3693225681781769, -1.1470122337341309, 0.14067305624485016, -0.7658770084381104, -0.1761651337146759, 0.06443196535110474, 0.6585835218429565, 0.35344377160072327, -0.37176746129989624, -0.17355230450630188, 1.067296028137207, -0.49269434809684753, 0.1338992416858673, 0.2038155347108841, 1.0897144079208374, -0.3451952040195465, 0.008046917617321014, -0.1451716423034668, -0.17546436190605164, -0.4482959806919098, -1.6924405097961426, 0.42474564909935, -0.14168372750282288, -0.4902358651161194, -0.393586665391922, -0.5183358192443848, -0.1902427226305008, 0.04762764647603035, 0.18185210227966309, -0.7253889441490173, -0.5491524338722229, 0.7185472249984741, -0.3220268189907074, 0.0875648632645607, -0.32003048062324524, 0.3651386499404907, 0.8684136271476746, 0.7556067705154419, -0.30974259972572327, 0.1822352111339569, 0.03716493397951126, -0.22185328602790833, -0.5294713973999023, -0.3678458333015442, 0.240099236369133, -0.7082284092903137, -0.27487167716026306, -0.4180818498134613, 0.030818363651633263, 0.08225008100271225, -0.17230528593063354, 0.405423104763031, 0.9079499840736389, 0.00986567884683609, -0.30923113226890564, -0.9320497512817383, -0.49153387546539307, -0.7015489339828491, -0.3087596297264099, -0.5576617121696472, 0.9133711457252502, -0.6942664384841919, 0.5310744643211365, -0.06692957878112793, -1.038590908050537, 4.100334644317627, 0.8438158631324768, 0.9235959053039551, -0.21024349331855774, 0.9873863458633423, 0.7146062254905701, 0.21386760473251343, -0.304068386554718, 0.30316102504730225, -0.5781848430633545, 0.6879402995109558, 0.3536127209663391, 0.754141628742218, 0.17074106633663177, 0.16698692739009857, 0.6457467675209045, -0.37881964445114136, 0.10862898826599121, -0.030446723103523254, -0.701384961605072, -0.6808823347091675, 0.3019167184829712, 0.08129802346229553, -0.1093926727771759, -0.5506486296653748, 0.3386276662349701, 0.19801969826221466, 0.06162708252668381, 0.09209470450878143, -0.42152151465415955, 0.1770888864994049, 0.7634785175323486, 0.13174980878829956, 0.20282797515392303, -0.33814537525177, 0.5972028970718384, -0.10285200923681259, -1.0401620864868164, -0.12240778654813766, 0.8530306816101074, -0.43494394421577454, -0.21397565305233002, -0.195002481341362, 0.4769340753555298, -0.23088987171649933, 0.29098963737487793, -0.469237357378006, 0.11785417050123215, -0.005782098509371281, -0.41697198152542114, 0.6593033671379089, -0.8729000091552734, 0.7447889447212219, -0.9121420383453369, -0.08314342051744461, 0.2825757563114166, 0.49081942439079285, 0.07860018312931061, -0.19237297773361206, -0.6701946258544922, 0.10204624384641647, -0.04747474193572998, -0.37288832664489746, -0.3055190443992615, -0.6584333777427673, 0.6091163754463196, 1.0099966526031494, 0.4219493865966797, -0.5718149542808533, 0.06360625475645065, -0.566274106502533, -0.6851110458374023, -0.2171037495136261, -0.605509340763092, 0.27336370944976807, 0.11418013274669647, -0.4348917007446289, 0.1677711009979248, -0.3759535551071167, 0.3761771023273468, 0.5144508481025696, -0.2185847908258438, -0.14344751834869385, -0.5002943277359009, -0.2235780954360962, -0.05913449823856354, -0.3236725628376007, -0.1701442152261734, -0.5574647188186646, 0.7917976379394531, 0.40696030855178833, 0.2766154408454895, 0.520919919013977, 0.5255631804466248, -0.24873259663581848]\n",
      "<coroutine object BaseRagasEmbeddings.embed_text at 0x30c9cfae0>\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "\n",
    "local_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"WhereIsAI/UAE-Large-V1\",\n",
    "    model_kwargs={\"device\": \"mps\"} # Or \"cuda\" for GPU, \"mps\" for Mac \n",
    ")\n",
    "local_embeddings = LangchainEmbeddingsWrapper(local_embeddings)\n",
    "\n",
    "res = local_embeddings.embed_query(\"Who is this?\")  # Test local embedding model\n",
    "print(res)\n",
    "\n",
    "res = local_embeddings.embed_text(\"Who is this?\")\n",
    "print(res)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251cc400",
   "metadata": {},
   "source": [
    "# Default synthetic data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ac392bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KnowledgeGraph(nodes: 13, relationships: 0)\n"
     ]
    }
   ],
   "source": [
    "from ragas.testset.transforms import (\n",
    "    default_transforms, \n",
    "    apply_transforms, \n",
    "    EmbeddingExtractor, \n",
    "    SummaryExtractor, \n",
    "    TitleExtractor,\n",
    "    HeadlinesExtractor,\n",
    "    HeadlineSplitter,\n",
    "    KeyphrasesExtractor,\n",
    "    HeadlineSplitter,\n",
    "    OverlapScoreBuilder,\n",
    ")\n",
    "from ragas.testset.graph import KnowledgeGraph\n",
    "from ragas.testset.graph import Node, NodeType\n",
    "\n",
    "\n",
    "# initialize your knowledge graph\n",
    "kg = KnowledgeGraph()\n",
    "\n",
    "for chunk in all_processed_chunks:\n",
    "    kg.nodes.append(\n",
    "        Node(\n",
    "            type=NodeType.DOCUMENT,\n",
    "            properties={\"page_content\": chunk.page_content, \"metadata\": chunk.metadata},\n",
    "        )\n",
    "    )\n",
    "print(kg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70d54a97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "146f3736ea514b259607aaba1993aed4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying HeadlinesExtractor:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# headline extractor\n",
    "headline_extractor = HeadlinesExtractor(llm=llm)\n",
    "apply_transforms(kg, headline_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "074e15eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before\n",
      "KnowledgeGraph(nodes: 13, relationships: 0)\n",
      "Applying transform: HeadlinesExtractor(name='HeadlinesExtractor', filter_nodes=<function default_transforms.<locals>.<lambda> at 0x1544a68e0>, llm=LangchainLLMWrapper(langchain_llm=AzureChatOpenAI(...)), merge_if_possible=True, max_token_limit=32000, tokenizer=<Encoding 'o200k_base'>, property_name='headlines', prompt=HeadlinesExtractorPrompt(instruction=Extract the most important max_num headlines from the given text that can be used to split the text into independent sections.Focus on Level 2 and Level 3 headings., examples=[(TextWithExtractionLimit(text='                Introduction\\n                Overview of the topic...\\n\\n                Main Concepts\\n                Explanation of core ideas...\\n\\n                Detailed Analysis\\n                Techniques and methods for analysis...\\n\\n                Subsection: Specialized Techniques\\n                Further details on specialized techniques...\\n\\n                Future Directions\\n                Insights into upcoming trends...\\n\\n                Subsection: Next Steps in Research\\n                Discussion of new areas of study...\\n\\n                Conclusion\\n                Final remarks and summary.\\n                ', max_num=6), Headlines(headlines=['Introduction', 'Main Concepts', 'Detailed Analysis', 'Subsection: Specialized Techniques', 'Future Directions', 'Conclusion']))], language=english), max_num=5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d10679055c7942949cd67e3230e7e963",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying HeadlinesExtractor:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Property 'headlines' already exists in node '73c6ae'. Skipping!\n",
      "Property 'headlines' already exists in node '2e48c4'. Skipping!\n",
      "Property 'headlines' already exists in node '346385'. Skipping!\n",
      "Property 'headlines' already exists in node '14ef3e'. Skipping!\n",
      "Property 'headlines' already exists in node '8aba8e'. Skipping!\n",
      "Property 'headlines' already exists in node '509c2a'. Skipping!\n",
      "Property 'headlines' already exists in node '72f538'. Skipping!\n",
      "Property 'headlines' already exists in node '8c5063'. Skipping!\n",
      "Property 'headlines' already exists in node '40ac1d'. Skipping!\n",
      "Property 'headlines' already exists in node '5f75a7'. Skipping!\n",
      "Property 'headlines' already exists in node 'dcca5e'. Skipping!\n",
      "Property 'headlines' already exists in node '8111fd'. Skipping!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying transform: HeadlineSplitter(name='HeadlineSplitter', filter_nodes=<function default_filter at 0x30cb2aca0>, min_tokens=500, max_tokens=1000)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "193c2e258fba4eeeb4c6cadd425cd20d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying HeadlineSplitter:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying transform: SummaryExtractor(name='SummaryExtractor', filter_nodes=<function default_transforms.<locals>.<lambda> at 0x121443e20>, llm=LangchainLLMWrapper(langchain_llm=AzureChatOpenAI(...)), merge_if_possible=True, max_token_limit=32000, tokenizer=<Encoding 'o200k_base'>, property_name='summary', prompt=SummaryExtractorPrompt(instruction=Summarize the given text in less than 10 sentences., examples=[(StringIO(text='Artificial intelligence\\n\\nArtificial intelligence is transforming various industries by automating tasks that previously required human intelligence. From healthcare to finance, AI is being used to analyze vast amounts of data quickly and accurately. This technology is also driving innovations in areas like self-driving cars and personalized recommendations.'), StringIO(text='AI is revolutionizing industries by automating tasks, analyzing data, and driving innovations like self-driving cars and personalized recommendations.'))], language=english))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4af9bdf7f4240e28acfd5360d883f16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying SummaryExtractor:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying transform: CustomNodeFilter(name='CustomNodeFilter', filter_nodes=<function default_transforms.<locals>.<lambda> at 0x30cb8aac0>, llm=LangchainLLMWrapper(langchain_llm=AzureChatOpenAI(...)), scoring_prompt=QuestionPotentialPrompt(instruction=Given a document summary and node content, score the content of the node in 1 to 5 range., examples=[], language=english), min_score=2, rubrics={'score1_description': 'The page content is irrelevant or does not align with the main themes or topics of the document summary.', 'score2_description': \"The page content partially aligns with the document summary, but it includes unrelated details or lacks critical information related to the document's main themes.\", 'score3_description': 'The page content generally reflects the document summary but may miss key details or lack depth in addressing the main themes.', 'score4_description': 'The page content aligns well with the document summary, covering the main themes and topics with minor gaps or minimal unrelated information.', 'score5_description': \"The page content is highly relevant, accurate, and directly reflects the main themes of the document summary, covering all important details and adding depth to the understanding of the document's topics.\"})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ebef5d08b0e4096818c12402e8438d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying CustomNodeFilter:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying transform: <ragas.testset.transforms.engine.Parallel object at 0x35786a810>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39e67fb8897740c48fd4b2ef47b223bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying transform: <ragas.testset.transforms.engine.Parallel object at 0x3578c1460>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dbff201ee5d405cb904fbd6b0675322",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying [CosineSimilarityBuilder, OverlapScoreBuilder]:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After\n",
      "KnowledgeGraph(nodes: 44, relationships: 124)\n"
     ]
    }
   ],
   "source": [
    "# get all the default transforms as well\n",
    "print(\"Before\")\n",
    "print(kg)\n",
    "trans = default_transforms(documents=docs, llm=llm, embedding_model=local_embeddings)\n",
    "for tran in trans:\n",
    "    print(f\"Applying transform: {tran}\")\n",
    "    apply_transforms(kg, tran)\n",
    "print(\"After\")\n",
    "print(kg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2cd287d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "641fc15e391a40d095573be71703ae19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying EmbeddingExtractor:   0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to apply transformation: node.property('summary') must be a string, found '<class 'NoneType'>'\n",
      "unable to apply transformation: node.property('summary') must be a string, found '<class 'NoneType'>'\n",
      "unable to apply transformation: node.property('summary') must be a string, found '<class 'NoneType'>'\n",
      "unable to apply transformation: node.property('summary') must be a string, found '<class 'NoneType'>'\n",
      "unable to apply transformation: node.property('summary') must be a string, found '<class 'NoneType'>'\n",
      "unable to apply transformation: node.property('summary') must be a string, found '<class 'NoneType'>'\n",
      "unable to apply transformation: node.property('summary') must be a string, found '<class 'NoneType'>'\n",
      "unable to apply transformation: node.property('summary') must be a string, found '<class 'NoneType'>'\n",
      "unable to apply transformation: node.property('summary') must be a string, found '<class 'NoneType'>'\n",
      "unable to apply transformation: node.property('summary') must be a string, found '<class 'NoneType'>'\n",
      "unable to apply transformation: node.property('summary') must be a string, found '<class 'NoneType'>'\n",
      "unable to apply transformation: node.property('summary') must be a string, found '<class 'NoneType'>'\n",
      "unable to apply transformation: node.property('summary') must be a string, found '<class 'NoneType'>'\n",
      "unable to apply transformation: node.property('summary') must be a string, found '<class 'NoneType'>'\n",
      "unable to apply transformation: node.property('summary') must be a string, found '<class 'NoneType'>'\n",
      "unable to apply transformation: node.property('summary') must be a string, found '<class 'NoneType'>'\n",
      "unable to apply transformation: node.property('summary') must be a string, found '<class 'NoneType'>'\n",
      "unable to apply transformation: node.property('summary') must be a string, found '<class 'NoneType'>'\n",
      "unable to apply transformation: node.property('summary') must be a string, found '<class 'NoneType'>'\n",
      "unable to apply transformation: node.property('summary') must be a string, found '<class 'NoneType'>'\n",
      "unable to apply transformation: node.property('summary') must be a string, found '<class 'NoneType'>'\n",
      "unable to apply transformation: node.property('summary') must be a string, found '<class 'NoneType'>'\n",
      "unable to apply transformation: node.property('summary') must be a string, found '<class 'NoneType'>'\n",
      "unable to apply transformation: node.property('summary') must be a string, found '<class 'NoneType'>'\n",
      "unable to apply transformation: node.property('summary') must be a string, found '<class 'NoneType'>'\n",
      "unable to apply transformation: node.property('summary') must be a string, found '<class 'NoneType'>'\n",
      "unable to apply transformation: node.property('summary') must be a string, found '<class 'NoneType'>'\n",
      "unable to apply transformation: node.property('summary') must be a string, found '<class 'NoneType'>'\n",
      "unable to apply transformation: node.property('summary') must be a string, found '<class 'NoneType'>'\n",
      "unable to apply transformation: node.property('summary') must be a string, found '<class 'NoneType'>'\n",
      "unable to apply transformation: node.property('summary') must be a string, found '<class 'NoneType'>'\n",
      "unable to apply transformation: node.property('summary') must be a string, found '<class 'NoneType'>'\n",
      "Property 'summary_embedding' already exists in node '8aba8e'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '40ac1d'. Skipping!\n",
      "Property 'summary_embedding' already exists in node 'dcca5e'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '5f75a7'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '72f538'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '346385'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '2e48c4'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '8111fd'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '8c5063'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '509c2a'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '73c6ae'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '14ef3e'. Skipping!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding extraction complete\n"
     ]
    }
   ],
   "source": [
    "# get summary embeddings required by automatic persona generator\n",
    "summary_embedding_extractor = EmbeddingExtractor(embedding_model=local_embeddings,\n",
    "                                         property_name=\"summary_embedding\",\n",
    "                                         embed_property_name=\"summary\")\n",
    "apply_transforms(kg, summary_embedding_extractor)\n",
    "print(\"Embedding extraction complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7397628e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0bd10330837457baaba536c0908ca4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying EmbeddingExtractor:   0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Property 'embedding' already exists in node '5c5bdc'. Skipping!\n"
     ]
    }
   ],
   "source": [
    "# get regular embeddings\n",
    "regular_embedding_extractor = EmbeddingExtractor(\n",
    "    embedding_model=local_embeddings,\n",
    ")\n",
    "apply_transforms(kg, regular_embedding_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc93f285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a44ca19b4b94d8393898cadec626050",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying HeadlinesExtractor:   0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Property 'headlines' already exists in node '8aba8e'. Skipping!\n",
      "Property 'headlines' already exists in node '8c5063'. Skipping!\n",
      "Property 'headlines' already exists in node '5c5bdc'. Skipping!\n",
      "Property 'headlines' already exists in node '73c6ae'. Skipping!\n",
      "Property 'headlines' already exists in node '346385'. Skipping!\n",
      "Property 'headlines' already exists in node '2e48c4'. Skipping!\n",
      "Property 'headlines' already exists in node '14ef3e'. Skipping!\n",
      "Property 'headlines' already exists in node '509c2a'. Skipping!\n",
      "Property 'headlines' already exists in node '8111fd'. Skipping!\n",
      "Property 'headlines' already exists in node 'dcca5e'. Skipping!\n",
      "Property 'headlines' already exists in node '40ac1d'. Skipping!\n",
      "Property 'headlines' already exists in node '72f538'. Skipping!\n",
      "Property 'headlines' already exists in node '5f75a7'. Skipping!\n",
      "Property 'headlines' already exists in node '5c5bdc'. Skipping!\n"
     ]
    }
   ],
   "source": [
    "# headline extractor\n",
    "headline_extractor = HeadlinesExtractor(llm=llm)\n",
    "apply_transforms(kg, headline_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ab1195b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before\n",
      "KnowledgeGraph(nodes: 44, relationships: 124)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d99694aa9244e52ab5eeee963d4e39c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying HeadlinesExtractor:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Property 'headlines' already exists in node '2e48c4'. Skipping!\n",
      "Property 'headlines' already exists in node '73c6ae'. Skipping!\n",
      "Property 'headlines' already exists in node '72f538'. Skipping!\n",
      "Property 'headlines' already exists in node '8111fd'. Skipping!\n",
      "Property 'headlines' already exists in node '8aba8e'. Skipping!\n",
      "Property 'headlines' already exists in node '8c5063'. Skipping!\n",
      "Property 'headlines' already exists in node '346385'. Skipping!\n",
      "Property 'headlines' already exists in node '509c2a'. Skipping!\n",
      "Property 'headlines' already exists in node '14ef3e'. Skipping!\n",
      "Property 'headlines' already exists in node 'dcca5e'. Skipping!\n",
      "Property 'headlines' already exists in node '40ac1d'. Skipping!\n",
      "Property 'headlines' already exists in node '5f75a7'. Skipping!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bff6bdfee42461b86bb807ccfd7f8e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying HeadlineSplitter:   0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93bfa0cf7aa144b0af9222f696b33068",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying SummaryExtractor:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Property 'summary' already exists in node '2e48c4'. Skipping!\n",
      "Property 'summary' already exists in node '73c6ae'. Skipping!\n",
      "Property 'summary' already exists in node '346385'. Skipping!\n",
      "Property 'summary' already exists in node '8aba8e'. Skipping!\n",
      "Property 'summary' already exists in node '14ef3e'. Skipping!\n",
      "Property 'summary' already exists in node '72f538'. Skipping!\n",
      "Property 'summary' already exists in node '8c5063'. Skipping!\n",
      "Property 'summary' already exists in node '8111fd'. Skipping!\n",
      "Property 'summary' already exists in node 'dcca5e'. Skipping!\n",
      "Property 'summary' already exists in node '40ac1d'. Skipping!\n",
      "Property 'summary' already exists in node '5f75a7'. Skipping!\n",
      "Property 'summary' already exists in node '509c2a'. Skipping!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41cebd78a5ad4ceba168b3978108f103",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying CustomNodeFilter:   0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Node 61fc50e7-2b18-4cf9-9558-c02fd3cd5bc4 does not have a summary. Skipping filtering.\n",
      "Node 10d3e1fd-d012-4786-b02d-64589165eee7 does not have a summary. Skipping filtering.\n",
      "Node 0f900e93-e8db-4f6e-a862-eaa7b3f555c1 does not have a summary. Skipping filtering.\n",
      "Node bb7400c0-e285-46fb-8e11-182f54c0e16b does not have a summary. Skipping filtering.\n",
      "Node 4a13f5a9-58cc-4f6b-9b73-0ccbf53e20d8 does not have a summary. Skipping filtering.\n",
      "Node f718a707-5af2-4563-b623-5a8a9bbf877c does not have a summary. Skipping filtering.\n",
      "Node 8ceb6a22-03a8-4a97-845c-e9cc0f6a76c6 does not have a summary. Skipping filtering.\n",
      "Node 58657099-10ee-4fbb-bd68-34712b4595b0 does not have a summary. Skipping filtering.\n",
      "Node 1b0322cd-8a3a-4587-8a29-72cb9c577631 does not have a summary. Skipping filtering.\n",
      "Node 3ab3ed67-8ae8-4ff6-a9f6-f395073f7e0e does not have a summary. Skipping filtering.\n",
      "Node ad0946d6-f8be-4526-bc7d-b7debb663564 does not have a summary. Skipping filtering.\n",
      "Node 8060a35a-fbf8-4c69-8a72-8975cca6d8bb does not have a summary. Skipping filtering.\n",
      "Node f9ff0a70-9c4b-4329-94d6-7207176a7a11 does not have a summary. Skipping filtering.\n",
      "Node 727e9c7a-d30b-4d14-b4e9-418a5d362d59 does not have a summary. Skipping filtering.\n",
      "Node e5416a91-fc34-4154-9d6c-1d45d8991271 does not have a summary. Skipping filtering.\n",
      "Node af97343d-379c-4c8b-bab7-da0766070a25 does not have a summary. Skipping filtering.\n",
      "Node 9f28dfd4-29d4-4776-9f37-f66787a59feb does not have a summary. Skipping filtering.\n",
      "Node 3b62dccd-4996-414e-9627-177e41b55c65 does not have a summary. Skipping filtering.\n",
      "Node ebb3806a-b962-4404-82d9-d75de24e923b does not have a summary. Skipping filtering.\n",
      "Node e5186f99-dae4-4775-8855-a0c0c98010dc does not have a summary. Skipping filtering.\n",
      "Node 060d6b46-5288-4335-8a59-8a8ec94d5ef5 does not have a summary. Skipping filtering.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edd18000b30f4db89bf12afb841a5ea6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:   0%|          | 0/216 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Property 'summary_embedding' already exists in node '8c5063'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '40ac1d'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '2e48c4'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '346385'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '8aba8e'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '73c6ae'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '72f538'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '5f75a7'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '509c2a'. Skipping!\n",
      "Property 'summary_embedding' already exists in node 'dcca5e'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '8111fd'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '14ef3e'. Skipping!\n",
      "Property 'themes' already exists in node 'd1feee'. Skipping!\n",
      "Property 'themes' already exists in node 'cd5862'. Skipping!\n",
      "Property 'themes' already exists in node '5b92f5'. Skipping!\n",
      "Property 'themes' already exists in node '550798'. Skipping!\n",
      "Property 'themes' already exists in node '0c8ca1'. Skipping!\n",
      "Property 'themes' already exists in node '6527f0'. Skipping!\n",
      "Property 'themes' already exists in node '543754'. Skipping!\n",
      "Property 'themes' already exists in node 'fd4f9e'. Skipping!\n",
      "Property 'themes' already exists in node '0b3781'. Skipping!\n",
      "Property 'themes' already exists in node 'b8bbf0'. Skipping!\n",
      "Property 'themes' already exists in node '4415d3'. Skipping!\n",
      "Property 'themes' already exists in node '174d06'. Skipping!\n",
      "Property 'themes' already exists in node '1fc584'. Skipping!\n",
      "Property 'themes' already exists in node '2beb31'. Skipping!\n",
      "Property 'themes' already exists in node '57b85d'. Skipping!\n",
      "Property 'themes' already exists in node 'de137c'. Skipping!\n",
      "Property 'themes' already exists in node 'e17158'. Skipping!\n",
      "Property 'themes' already exists in node '0003db'. Skipping!\n",
      "Property 'themes' already exists in node '8d5af5'. Skipping!\n",
      "Property 'themes' already exists in node '37e625'. Skipping!\n",
      "Property 'themes' already exists in node '10775b'. Skipping!\n",
      "Property 'themes' already exists in node '21e1e6'. Skipping!\n",
      "Property 'themes' already exists in node '6a3445'. Skipping!\n",
      "Property 'themes' already exists in node '4a4ab2'. Skipping!\n",
      "Property 'themes' already exists in node '25d4b4'. Skipping!\n",
      "Property 'themes' already exists in node '341949'. Skipping!\n",
      "Property 'themes' already exists in node 'a0b502'. Skipping!\n",
      "Property 'themes' already exists in node '7e00de'. Skipping!\n",
      "Property 'themes' already exists in node '65d136'. Skipping!\n",
      "Property 'themes' already exists in node 'd1feee'. Skipping!\n",
      "Property 'themes' already exists in node 'cd5862'. Skipping!\n",
      "Property 'themes' already exists in node '5b92f5'. Skipping!\n",
      "Property 'themes' already exists in node '57b85d'. Skipping!\n",
      "Property 'themes' already exists in node '0c8ca1'. Skipping!\n",
      "Property 'themes' already exists in node '2beb31'. Skipping!\n",
      "Property 'themes' already exists in node '0b3781'. Skipping!\n",
      "Property 'themes' already exists in node 'e17158'. Skipping!\n",
      "Property 'themes' already exists in node 'fd4f9e'. Skipping!\n",
      "Property 'themes' already exists in node '1fc584'. Skipping!\n",
      "Property 'themes' already exists in node '10775b'. Skipping!\n",
      "Property 'themes' already exists in node '8d5af5'. Skipping!\n",
      "Property 'themes' already exists in node '0003db'. Skipping!\n",
      "Property 'themes' already exists in node 'c1946c'. Skipping!\n",
      "Property 'themes' already exists in node '341949'. Skipping!\n",
      "Property 'themes' already exists in node '4a4ab2'. Skipping!\n",
      "Property 'themes' already exists in node '6a3445'. Skipping!\n",
      "Property 'themes' already exists in node '7e00de'. Skipping!\n",
      "Property 'themes' already exists in node '65d136'. Skipping!\n",
      "Property 'entities' already exists in node '5b92f5'. Skipping!\n",
      "Property 'entities' already exists in node '550798'. Skipping!\n",
      "Property 'entities' already exists in node 'd1feee'. Skipping!\n",
      "Property 'themes' already exists in node '21e1e6'. Skipping!\n",
      "Property 'entities' already exists in node 'b8bbf0'. Skipping!\n",
      "Property 'entities' already exists in node 'de137c'. Skipping!\n",
      "Property 'entities' already exists in node '0c8ca1'. Skipping!\n",
      "Property 'entities' already exists in node '57b85d'. Skipping!\n",
      "Property 'entities' already exists in node '0b3781'. Skipping!\n",
      "Property 'entities' already exists in node '543754'. Skipping!\n",
      "Property 'entities' already exists in node '4415d3'. Skipping!\n",
      "Property 'entities' already exists in node '2beb31'. Skipping!\n",
      "Property 'entities' already exists in node 'e17158'. Skipping!\n",
      "Property 'entities' already exists in node '1fc584'. Skipping!\n",
      "Property 'entities' already exists in node '6527f0'. Skipping!\n",
      "Property 'entities' already exists in node '174d06'. Skipping!\n",
      "Property 'entities' already exists in node '0003db'. Skipping!\n",
      "Property 'entities' already exists in node 'fd4f9e'. Skipping!\n",
      "Property 'entities' already exists in node '10775b'. Skipping!\n",
      "Property 'entities' already exists in node '8d5af5'. Skipping!\n",
      "Property 'entities' already exists in node '37e625'. Skipping!\n",
      "Property 'entities' already exists in node 'cd5862'. Skipping!\n",
      "Property 'entities' already exists in node '25d4b4'. Skipping!\n",
      "Property 'entities' already exists in node '4a4ab2'. Skipping!\n",
      "Property 'entities' already exists in node 'a0b502'. Skipping!\n",
      "Property 'entities' already exists in node '6a3445'. Skipping!\n",
      "Property 'entities' already exists in node '341949'. Skipping!\n",
      "Property 'entities' already exists in node 'c1946c'. Skipping!\n",
      "Property 'entities' already exists in node '7e00de'. Skipping!\n",
      "Property 'entities' already exists in node '21e1e6'. Skipping!\n",
      "Property 'entities' already exists in node '65d136'. Skipping!\n",
      "Property 'entities' already exists in node '5b92f5'. Skipping!\n",
      "Property 'entities' already exists in node 'cd5862'. Skipping!\n",
      "Property 'entities' already exists in node 'd1feee'. Skipping!\n",
      "Property 'entities' already exists in node '57b85d'. Skipping!\n",
      "Property 'entities' already exists in node '0b3781'. Skipping!\n",
      "Property 'entities' already exists in node 'e17158'. Skipping!\n",
      "Property 'entities' already exists in node '0c8ca1'. Skipping!\n",
      "Property 'entities' already exists in node '2beb31'. Skipping!\n",
      "Property 'themes' already exists in node 'c1946c'. Skipping!\n",
      "Property 'entities' already exists in node '0003db'. Skipping!\n",
      "Property 'entities' already exists in node '1fc584'. Skipping!\n",
      "Property 'entities' already exists in node 'fd4f9e'. Skipping!\n",
      "Property 'entities' already exists in node '8d5af5'. Skipping!\n",
      "Property 'entities' already exists in node '10775b'. Skipping!\n",
      "Property 'entities' already exists in node '4a4ab2'. Skipping!\n",
      "Property 'entities' already exists in node '6a3445'. Skipping!\n",
      "Property 'entities' already exists in node '341949'. Skipping!\n",
      "Property 'entities' already exists in node 'c1946c'. Skipping!\n",
      "Property 'entities' already exists in node '7e00de'. Skipping!\n",
      "Property 'entities' already exists in node '21e1e6'. Skipping!\n",
      "Property 'entities' already exists in node '65d136'. Skipping!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7fa9d457f0447678d27e76ad0c55545",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying [CosineSimilarityBuilder, OverlapScoreBuilder]:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After\n",
      "KnowledgeGraph(nodes: 118, relationships: 675)\n"
     ]
    }
   ],
   "source": [
    "# get all the default transforms as well\n",
    "print(\"Before\")\n",
    "print(kg)\n",
    "trans = default_transforms(documents=docs, llm=llm, embedding_model=local_embeddings)\n",
    "apply_transforms(kg, trans)\n",
    "print(\"After\")\n",
    "print(kg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "afd1e47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Knowledge Graph Debug Info ===\n",
      "Total nodes: 118\n",
      "Total relationships: 675\n",
      "Relationship types found: {'child', 'entities_overlap', 'cosine_similarity', 'next'}\n"
     ]
    }
   ],
   "source": [
    "# Add this debugging code to see what relationships exist\n",
    "print(\"=== Knowledge Graph Debug Info ===\")\n",
    "print(f\"Total nodes: {len(kg.nodes)}\")\n",
    "print(f\"Total relationships: {len(kg.relationships)}\")\n",
    "\n",
    "# Check relationship types\n",
    "rel_types = set()\n",
    "for rel in kg.relationships:\n",
    "    rel_types.add(rel.type)\n",
    "    \n",
    "print(f\"Relationship types found: {rel_types}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41592f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KnowledgeGraph(nodes: 118, relationships: 675)\n"
     ]
    }
   ],
   "source": [
    "print(kg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ee375d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ad062ad0c8f4e9ea6b30e25b41e4888",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating personas:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12dd4c0aa4474af1a816fd394d941017",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Scenarios:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0801081b6fd24727a6052973e9af2c99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Samples:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ragas.testset import TestsetGenerator\n",
    "from ragas.testset.synthesizers import default_query_distribution\n",
    "\n",
    "generator = TestsetGenerator(llm=llm, embedding_model=local_embeddings, knowledge_graph=kg)\n",
    "query_distribution = default_query_distribution(llm)\n",
    "testset = generator.generate(testset_size=10, query_distribution=query_distribution)\n",
    "testset_pd = testset.to_pandas()\n",
    "testset_pd.to_json('barclays_synthetic_multihop.json', orient='records', indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c852e6d1",
   "metadata": {},
   "source": [
    "### Custom persona "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a7271ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.graph import KnowledgeGraph\n",
    "from ragas.testset.graph import Node, NodeType\n",
    "\n",
    "\n",
    "kg = KnowledgeGraph()\n",
    "for doc in docs:\n",
    "    kg.nodes.append(\n",
    "        Node(\n",
    "            type=NodeType.DOCUMENT,\n",
    "            properties={\n",
    "                \"page_content\": doc.page_content,\n",
    "                \"document_metadata\": doc.metadata,\n",
    "            },\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0417345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf250d2a204c4a9680e726d3568c6034",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying HeadlinesExtractor:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "054ba9b0777040a88164bad0d7050474",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying HeadlineSplitter:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f1b6138272a439899420481d5dc6ac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying KeyphrasesExtractor:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54cecf3eb91f455d89398db247f3d084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying OverlapScoreBuilder:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ragas.testset.transforms import Parallel, apply_transforms\n",
    "from ragas.testset.transforms import (\n",
    "    HeadlinesExtractor,\n",
    "    HeadlineSplitter,\n",
    "    KeyphrasesExtractor,\n",
    "    OverlapScoreBuilder,\n",
    ")\n",
    "\n",
    "\n",
    "headline_extractor = HeadlinesExtractor(llm=llm)\n",
    "headline_splitter = HeadlineSplitter(min_tokens=300, max_tokens=1000)\n",
    "keyphrase_extractor = KeyphrasesExtractor(\n",
    "    llm=llm, property_name=\"keyphrases\", max_num=10\n",
    ")\n",
    "relation_builder = OverlapScoreBuilder(\n",
    "    property_name=\"keyphrases\",\n",
    "    new_property_name=\"overlap_score\",\n",
    "    threshold=0.01,\n",
    "    distance_threshold=0.9,\n",
    ")\n",
    "\n",
    "transforms = [\n",
    "    headline_extractor,\n",
    "    headline_splitter,\n",
    "    keyphrase_extractor,\n",
    "    relation_builder,\n",
    "]\n",
    "\n",
    "apply_transforms(kg, transforms=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b0ddddc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's generate using custom personas\n",
    "from ragas.testset.persona import Persona\n",
    "\n",
    "person1 = Persona(\n",
    "    name=\"Banking Executive\",\n",
    "    role_description=\"Explore AI and data strategies to modernize financial services and unlock new revenue streams. Asks only one question at a time.\",\n",
    ")\n",
    "persona2 = Persona(\n",
    "    name=\"FinTech Founder\",\n",
    "    role_description=\"Leverage AI and data innovation to build competitive, regulation-aware financial products. Asks only one question at a time.\",\n",
    ")\n",
    "persona_list = [person1, persona2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "05e818b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiHopScenario(\n",
       "nodes=2\n",
       "combinations=['artificial intelligence', 'Artificial Intelligence technologies']\n",
       "style=QueryStyle.PERFECT_GRAMMAR\n",
       "length=QueryLength.LONG\n",
       "persona=name='FinTech Founder' role_description='Leverage AI and data innovation to build competitive, regulation-aware financial products. Asks only one question at a time.')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "import typing as t\n",
    "from ragas.testset.synthesizers.multi_hop.base import (\n",
    "    MultiHopQuerySynthesizer,\n",
    "    MultiHopScenario,\n",
    ")\n",
    "from ragas.testset.synthesizers.prompts import (\n",
    "    ThemesPersonasInput,\n",
    "    ThemesPersonasMatchingPrompt,\n",
    ")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MyMultiHopQuery(MultiHopQuerySynthesizer):\n",
    "\n",
    "    theme_persona_matching_prompt = ThemesPersonasMatchingPrompt()\n",
    "\n",
    "    async def _generate_scenarios(\n",
    "        self,\n",
    "        n: int,\n",
    "        knowledge_graph,\n",
    "        persona_list,\n",
    "        callbacks,\n",
    "    ) -> t.List[MultiHopScenario]:\n",
    "\n",
    "        # query and get (node_a, rel, node_b) to create multi-hop queries\n",
    "        results = kg.find_two_nodes_single_rel(\n",
    "            relationship_condition=lambda rel: (\n",
    "                True if rel.type == \"keyphrases_overlap\" else False\n",
    "            )\n",
    "        )\n",
    "\n",
    "        num_sample_per_triplet = max(1, n // len(results))\n",
    "\n",
    "        scenarios = []\n",
    "        for triplet in results:\n",
    "            if len(scenarios) < n:\n",
    "                node_a, node_b = triplet[0], triplet[-1]\n",
    "                overlapped_keywords = triplet[1].properties[\"overlapped_items\"]\n",
    "                if overlapped_keywords:\n",
    "\n",
    "                    # match the keyword with a persona for query creation\n",
    "                    themes = list(dict(overlapped_keywords).keys())\n",
    "                    prompt_input = ThemesPersonasInput(\n",
    "                        themes=themes, personas=persona_list\n",
    "                    )\n",
    "                    persona_concepts = (\n",
    "                        await self.theme_persona_matching_prompt.generate(\n",
    "                            data=prompt_input, llm=self.llm, callbacks=callbacks\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                    overlapped_keywords = [list(item) for item in overlapped_keywords]\n",
    "\n",
    "                    # prepare and sample possible combinations\n",
    "                    base_scenarios = self.prepare_combinations(\n",
    "                        [node_a, node_b],\n",
    "                        overlapped_keywords,\n",
    "                        personas=persona_list,\n",
    "                        persona_item_mapping=persona_concepts.mapping,\n",
    "                        property_name=\"keyphrases\",\n",
    "                    )\n",
    "\n",
    "                    # get number of required samples from this triplet\n",
    "                    base_scenarios = self.sample_diverse_combinations(\n",
    "                        base_scenarios, num_sample_per_triplet\n",
    "                    )\n",
    "\n",
    "                    scenarios.extend(base_scenarios)\n",
    "\n",
    "        return scenarios\n",
    "\n",
    "query = MyMultiHopQuery(llm=llm)\n",
    "scenarios = await query.generate_scenarios(\n",
    "    n=200, knowledge_graph=kg, persona_list=persona_list\n",
    ")\n",
    "\n",
    "scenarios[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "edaa981c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How does the concept of trust in AI and data sharing influence the relationship between consumers and financial institutions?\n",
      "Trust is a fundamental principle in banking, influencing how consumers perceive the use of their data by financial institutions. As individuals engage with various online services, they often relinquish control over their personal data, relying on trust that it will be handled respectfully and appropriately. This trust is crucial, especially when considering the sensitive nature of the data involved, such as personal preferences and financial information. The emergence of transparent data-sharing protocols could empower consumers to manage their data more effectively, potentially increasing their trust in financial institutions. By allowing consumers to set their own data-sharing policies and review how their data is used, banks can foster a more trusting relationship, which is essential for the successful implementation of AI technologies in financial services.\n",
      "[\"<1-hop>\\n\\nHow data became valuable\\\\n- 16 Survival tips for startups: Know your regulator\\\\n\\\\n## 18 Data commercialisation\\\\n\\\\n\\\\n| Page | Title |\\\\n| ----- | ----- |\\\\n| 20 | The data revolution |\\\\n| 26 | The Netflix era: Data rentals |\\\\n\\\\n\\\\n28 AI use cases in finance\\\\n\\\\n\\\\n| Page | Topic |\\\\n| ----- | ----- |\\\\n| 30 | ML takes aim at FinCrime risks |\\\\n| 32 | Company spotlight |\\\\n\\\\n- 34 Data and AI in institutional investing: More revolution than evolution?\\\\n- 37 Improved credit decisions for the unbanked and Gen-Z\\\\n- 40 The democratisation of investing\\\\n\\\\n44 Focus on CX – AI and the customer experience\\\\n\\\\n\\\\n52 Ethics and bias in AI\\\\n\\\\n\\\\n54 Ethics in AI\\\\n\\\\n\\\\n58 Increasing trust when deploying AI systems\\\\n\\\\n60 Rise and programmes updates\\\\n\\\\n\\\\n60 From our Rise sites\\\\n\\\\n\\\\n| Page | Topic |\\\\n| ----- | ----- |\\\\n| 60 | From our Rise sites |\\\\n| 64 | Rise Academies |\\\\n| 65 | Join Rise Connect |\\\\n| 66 | Rise global network |\\\\n\\\\n\\\\n3 / rise.barclays\\\\n\\\\n\\\\nPage 2\\\\n---\\\\n\\\\n## A new relationship with our data\\\\n\\\\n\\\\nAs individuals conduct their daily online work and personal lives, they spray data, often unwittingly, into a vast array of corporate data silos around the globe. These data streams fan out, magnifying the dispersion: every clicked link, form submission or app swipe can leave traces in logs and databases in tens of companies beyond the provider of the website or app itself.\\\\n\\\\nWhile privacy regulations have in recent years set rules for corporate holders of data, it's notable that, right at the point of interaction, the individual loses all direct control. They must rely on trust alone, trust that those datasets will be treated with respect and used only in appropriate ways, when they contain intimate details of their personal lives, which foods they like, what products they've bought, their personal contact information, their physical location, and so on.\\\\n\\\\nDoes it have to be this way or could a more transparent protocol of data sharing be conceived? Could consumers instead manage the storage and sharing of their personal data with the companies who wish to use it? Imagine them choosing their own masking policies, setting time limits on data availability and even setting a price to sell each type of data record. Opening a personal analytics app, the consumer could review which API calls had been made to use their data during the day, by whom and for precisely which purpose. This presents new commercial models, beyond simply handing over all data in exchange for the free usage of an email platform or social messaging tool.\\\\n\\\\nA combination of existing technologies could underpin this controlled data sharing vision on a global scale. The technologists among you may have encountered: GraphQL APIs on top of data stores that support dynamic encryption and dynamic masking; distributed ledgers and smart contracts providing decentralised identity and specifically tracked data transactions; cheap cloud computing providing globally distributed key value stores.\\\\n\\\\nSupporting this approach to data would be Artificial Intelligence (Al) technologies that allow machines to sift through data much faster than humans, to learn from it and then to suggest rich customer experiences as well as to detect and prevent fraud.\\\\n\\\\nWhile the incentive may not be obvious for companies to give up their traditional approach of storing their own vast internal silos of raw data, those silos could themselves be as much a liability as an asset. The potential fines and reputational loss for data breaches, the operational difficulty of keeping data up to date, the expense of ensuring the use and transmission of data is audited and tracked carefully are all very significant risks, and\\\\n\\\\nimplementations are hard to get right. There is already precedent for companies offloading such data storage risks to specialised providers, such as in card processing where ecommerce companies have used external data tokenisation services for years, but these cases are usually narrowly limited in scope.\\\\n\\\\nVisit rise.barclays to:\\\\n- - Learn more about Rise\\\\n- - Participate in our community of innovators\\\\n\\\\nRaw data is rocket fuel, highly powerful but also highly dangerous. Today, we are spraying it around and hoping to avoid accidents. Wrapping data more smartly, giving its wide uses specific aims and enforcing these with provable algorithmic guarantees, offers significant benefits to consumers and companies alike.\\\\n\\\\nThis report offers perspectives on the innovations and opportunities from expert technologists, founders and businesses. I hope you enjoy their insights.\\\\n- - Get industry news, further insights and other content\\\\n\\\\nBen George Distinguished Engineer for Data, Barclays Chief Technology Office\\\\n\\\\n\\\\nin ben-george-5a11a7b8\\\\n\\\\n4 / Rise Insights\\\\n\\\\n\\\\n5 / rise.barclays\\\\n\\\\n\\\\nPage 3\\\\n---\\\\n\\\\n## The evolution of data\\\\n\\\\n\\\\nData is the lifeblood of financial services. By using Al and complying with regulations, banks of the future can infuse their businesses with new opportunities.\\\\n\\\\n6 / Rise Insights\\\\n\\\\n\\\\n7 / rise.barclays\\\\n\\\\n\\\\nPage 4\\\\n---\\\\n\\\\n## How data became valuable\\\\n\\\\n\\\\nWhat started the data revolution, and how did data become the focus of innovation? I take a brief look at the history of, and a possible future for, data commercialisation.\\\\n\\\\n## User control and consent\\\\n\\\\n\\\\nMuch is rightfully made of Jeff Bezos' API Mandate, in which he laid out plans in 2002 for all of Amazon's internal services to become accessible through APIs. This began as an efficiency drive to ensure departments could access the data they needed to operate effectively. But it delivered the technical foundation for Amazon Web Services, which now generates the vast majority of Amazon's profits.\\\\n\\\\n## Where fintechs lead...\\\\n\\\\n\\\\nThe banking equivalent of the API Mandate came not from a visionary CEO – there remain few CEOs in banking with a technology background. Instead it came around the same time as the 2008 financial crisis, from the European Union and the UK Government. Through the Open Banking model they decreed that banks must make their data available to third parties through APIs when their customers demanded it.¹ This unleashed a wave of innovation in financial services and demonstrated the latent commercial opportunities in the sector. So much so that, by 2021, Accenture estimated there to be a $416 billion revenue opportunity.²\\\\n\\\\nBacked by lots of venture capital and without the complexity of legacy technology stacks and large bureaucracies, a wide range of fintechs were able to quickly\", '<2-hop>\\n\\n\"\\\\n-\\\\n\\\\n\\\\nRise Insights report \\\\#HomeofFinTech\\\\n\\\\n# Making data count with AI\\\\n\\\\n\\\\nPage 1\\\\n---\\\\n\\\\n## Contents\\\\n\\\\n\\\\n4 Foreword\\\\n\\\\n- 4 A new relationship with our data\\\\n\\\\n6 The evolution of data\\\\n\\\\n- 8 How data became valuable\\\\n- 16 Survival tips for startups: Know your regulator\\\\n\\\\n## 18 Data commercialisation\\\\n\\\\n\\\\n| Page | Title |\\\\n| ----- | ----- |\\\\n| 20 | The data revolution |\\\\n| 26 | The Netflix era: Data rentals |\\\\n\\\\n\\\\n28 AI use cases in finance\\\\n\\\\n\\\\n| Page | Topic |\\\\n| ----- | ----- |\\\\n| 30 | ML takes aim at FinCrime risks |\\\\n| 32 | Company spotlight |\\\\n\\\\n- 34 Data and AI in institutional investing: More revolution than evolution?\\\\n- 37 Improved credit decisions for the unbanked and Gen-Z\\\\n- 40 The democratisation of investing\\\\n\\\\n44 Focus on CX – AI and the customer experience\\\\n\\\\n\\\\n52 Ethics and bias in AI\\\\n\\\\n\\\\n54 Ethics in AI\\\\n\\\\n\\\\n58 Increasing trust when deploying AI systems\\\\n\\\\n60 Rise and programmes updates\\\\n\\\\n\\\\n60 From our Rise sites\\\\n\\\\n\\\\n| Page | Topic |\\\\n| ----- | ----- |\\\\n| 60 | From our Rise sites |\\\\n| 64 | Rise Academies |\\\\n| 65 | Join Rise Connect |\\\\n| 66 | Rise global network |\\\\n\\\\n\\\\n3 / rise.barclays\\\\n\\\\n\\\\nPage 2\\\\n---\\\\n\\\\n## A new relationship with our data\\\\n\\\\n\\\\nAs individuals conduct their daily online work and personal lives, they spray data, often unwittingly, into a vast array of corporate data silos around the globe. These data streams fan out, magnifying the dispersion: every clicked link, form submission or app swipe can leave traces in logs and databases in tens of companies beyond the provider of the website or app itself.\\\\n\\\\nWhile privacy regulations have in recent years set rules for corporate holders of data, it\\'s notable that, right at the point of interaction, the individual loses all direct control. They must rely on trust alone, trust that those datasets will be treated with respect and used only in appropriate ways, when they contain intimate details of their personal lives, which foods they like, what products they\\'ve bought, their personal contact information, their physical location, and so on.\\\\n\\\\nDoes it have to be this way or could a more transparent protocol of data sharing be conceived? Could consumers instead manage the storage and sharing of their personal data with the companies who wish to use it? Imagine them choosing their own masking policies, setting time limits on data availability and even setting a price to sell each type of data record. Opening a personal analytics app, the consumer could review which API calls had been made to use their data during the day, by whom and for precisely which purpose. This presents new commercial models, beyond simply handing over all data in exchange for the free usage of an email platform or social messaging tool.\\\\n\\\\nA combination of existing technologies could underpin this controlled data sharing vision on a global scale. The technologists among you may have encountered: GraphQL APIs on top of data stores that support dynamic encryption and dynamic masking; distributed ledgers and smart contracts providing decentralised identity and specifically tracked data transactions; cheap cloud computing providing globally distributed key value stores.\\\\n\\\\nSupporting this approach to data would be Artificial Intelligence (Al) technologies that allow machines to sift through data much faster than humans, to learn from it and then to suggest rich customer experiences as well as to detect and prevent fraud.\\\\n\\\\nWhile the incentive may not be obvious for companies to give up their traditional approach of storing their own vast internal silos of raw data, those silos could themselves be as much a liability as an asset. The potential fines and reputational loss for data breaches, the operational difficulty of keeping data up to date, the expense of ensuring the use and transmission of data is audited and tracked carefully are all very significant risks, and\\\\n\\\\nimplementations are hard to get right. There is already precedent for companies offloading such data storage risks to specialised providers, such as in card processing where ecommerce companies have used external data tokenisation services for years, but these cases are usually narrowly limited in scope.\\\\n\\\\nVisit rise.barclays to:\\\\n- - Learn more about Rise\\\\n- - Participate in our community of innovators\\\\n\\\\nRaw data is rocket fuel, highly powerful but also highly dangerous. Today, we are spraying it around and hoping to avoid accidents. Wrapping data more smartly, giving its wide uses specific aims and enforcing these with provable algorithmic guarantees, offers significant benefits to consumers and companies alike.\\\\n\\\\nThis report offers perspectives on the innovations and opportunities from expert technologists, founders and businesses. I hope you enjoy their insights.\\\\n- - Get industry news, further insights and other content\\\\n\\\\nBen George Distinguished Engineer for Data, Barclays Chief Technology Office\\\\n\\\\n\\\\nin ben-george-5a11a7b8\\\\n\\\\n4 / Rise Insights\\\\n\\\\n\\\\n5 / rise.barclays\\\\n\\\\n\\\\nPage 3\\\\n---\\\\n\\\\n## The evolution of data\\\\n\\\\n\\\\nData is the lifeblood of financial services. By using Al and complying with regulations, banks of the future can infuse their businesses with new opportunities.\\\\n\\\\n6 / Rise Insights\\\\n\\\\n\\\\n7 / rise.barclays\\\\n\\\\n\\\\nPage 4\\\\n---\\\\n\\\\n## How data became valuable\\\\n\\\\n\\\\nWhat started the data revolution, and how did data become the focus of innovation? I take a brief look at the history of, and a possible future for, data commercialisation.\\\\n\\\\n## User control and consent\\\\n\\\\n\\\\nMuch is rightfully made of Jeff Bezos\\' API Mandate, in which he laid out plans in 2002 for all of Amazon\\'s internal services to become accessible through APIs. This began as an efficiency drive to ensure departments could access the data they needed to operate effectively. But it delivered the technical foundation for Amazon Web Services, which now generates the vast majority of Amazon\\'s profits.\\\\n\\\\n## Where fintechs lead...\\\\n\\\\n\\\\nThe banking equivalent of the API Mandate came not from a visionary CEO – there remain few CEOs in banking with a technology background. Instead it came around the same time as the 2008 financial crisis, from the European Union and the UK Government. Through the Open Banking model they decreed that banks must make their data available to third parties through APIs when their customers demanded it.¹ This unleashed a wave of innovation in financial services and demonstrated the latent commercial opportunities in the sector. So much so that, by 2021, Accenture estimated there to be a $416 billion revenue opportunity.²\\\\n\\\\nBacked by lots of venture capital and without the complexity of legacy technology stacks and large bureaucracies, a wide range of fintechs were able to quickly prototype and iterate a range of products and services that leveraged large volumes of data that had been under- utilised. For example, Ramp, valued at $3.9 billion, is threatening to overtake Amex in the corporate card market thanks to an Al engine on their expense-management platform that claims to save an average of 3.3% through cost cutting.3\\\\n\\\\nMany unicorns – startups valued at over $1 billion – were created in the process and delivered the levels of innovation that had been aspired to by the Open Banking policy makers. Fintechs had access to records from a range of banks and from their own products covering many use cases. In the right hands, the data proved to be highly valuable.\\\\n\\\\n## ...banks may follow\\\\n\\\\n\\\\nThe level of innovation that started over 10 years ago demonstrated to banks the exceptional value of the data they hold. Six of the top 12 unicorns in the world are fintechs⁴ and deliver modern services powered by data.\\\\n\\\\n8 / Rise Insights\\\\n\\\\n\\\\n1. Bank of England 2. Accenture 3. Forbes 4. CB Insights\\\\n\\\\n\\\\nThe largest have access to the financial and transactional records of millions of their own customers, and many of those transact with customers from other banks and financial institutions providing an even wider data footprint. There is a parallel for this in the payments infrastructure of traditional banking alongside the additional data points gathered from the relatively new digital banking platforms - including devices and location - that increases the breadth, accuracy and timeliness of the data footprint.\\\\n\\\\nConsequently, there remains an opportunity for financial institutions to create more compelling products and services for their customers. There\\'s also the opportunity to generate new revenue streams and even entire lines of business through commercialising their data as a product itself.\\\\n\\\\nHarbr is a data commerce platform that helps companies across multiple sectors, including financial services, create new revenue streams from their data by delivering high-margin data products and a refined customer experience. We work with large organisations across multiple industries, including financial services and banking, to help them build and run successful data businesses. Without exception, these companies have had to invest in new capabilities across technology, data, commerce and regulation to be successful.\\\\n\\\\n## Trust is key\\\\n\\\\n\\\\nTrust is arguably the most fundamental principle in banking, and may explain to some degree why banks have been reluctant to exploit their data assets. The use of data to impact consumer behaviour is common in the tech industry and we\\'re exposed to it whenever we use services like Google, Meta and Amazon. Yet the idea of a bank doing the same somehow feels wrong. Financial transactions are no more personal than pictures of our loved ones or our web-browsing history, but that wasn\\'t part of the deal when we opened our bank accounts. Fair or not, different expectations have been set for banks than those set for technology companies.\\\\n\\\\nWhile technology companies are subject to increasing scrutiny by courts and policymakers, and the fintech wave may have altered the view of what banks should be, they must still maintain high levels of trust. If successful, they may find that customers grant them equal or greater approval to use their data than that typically demanded by technology companies to use their services. Additionally, there are ways to commercialise data without exploiting or revealing personally identifiable information (PII) or affecting consumer behaviour, which is a refreshing alternative to big tech and a trend we see across our customer base.\\\\n\\\\n9 / rise.barclays\\\\n\\\\n\\\\nPage 5\\\\n---\\\\n\\\\n## From inputs to insights\\\\n\\\\n\\\\n## The path to success\\\\n\\\\n\\\\nMany existing data businesses thrive on selling input data that is engineered and analysed by their customers - often in conjunction with inputs from other sources - to get the insight they desire. However, this increases cost of ownership and time to value for the customer. The future of data businesses is to provide the insight the customer actually wants rather than a raw input. We see this being achieved through a range of options including secure collaboration between the supplier and consumer, the use of models or data science services, and the creation of segment or macro-level views that deliver an answer.\\\\n\\\\nMost organisations manage their understanding of customers, suppliers and partners through segmentation, the grouping of entities with related characteristics such as income, spending habits, location and age. Segmentation simplifies and accelerates a resource-intensive analytical task. Similarly, the provision of a macro view of a particular market, sector or jurisdiction can provide an immediate answer that can help inform business decisions. Both of these approaches also have the dual advantages of not directly impacting individual privacy and driving improvements that can benefit everyone.\\\\n\\\\nA vibrant data commerce market providing meaningful insights that avoid poor decisions and bad outcomes shouldn\\'t just be seen as a commercial opportunity. It should also be seen as a social good. Fraud, financial crime, poor investment decisions, excessive market volatility and supply chain disruption negatively affect economies and societies. They are also areas where banks and other financial institutions are uniquely placed to provide insights that can reduce or avoid these problems without impacting individual privacy.\\\\n\\\\nI experienced first-hand how challenging it is to deliver change in a large bank. Decades of cost-cutting, legacy technology estates, complex regulation and cyber security challenges all take their toll and create a highly-complex and political environment. Add to this the culture of banking with a tightly- defined risk appetite and an expectation of predictable returns from a given investment and you have an environment that is profoundly different to a startup. Given that the commercial exploitation of data would be a new endeavour, and not necessarily have any parallel to the business of banking, what would a large bank or financial services company need to do to succeed? What would that look like, and how might it impact the world around us?\\\\n\\\\nMy experience reveals that successful organisations focus on use cases for their data that benefit their customers and society while also complying with regulations and respecting consumer privacy. They typically obsess about finding product-market fit for their data, which means collaborating with potential data consumers to understand the necessary specification of their product for a given use case and the overall value proposition. This is a process every startup goes through and building a data business in a large organisation is no exception. There are often \\'failures\\' along the way where hypotheses are proven to be wrong, but by iterating rapidly and getting as much feedback as possible, the failures are an invaluable opportunity to learn. The key is to create an environment where they\\'re anticipated and learning from them is expected.\\\\n\\\\n10 / Rise Insights\\\\n\\\\n\\\\nThis process yields a range of outcomes from the creation of low-value, commodity products such as views of financial behaviour coupled with segmented baselines to provide context to high-value, unique insights that address highly specific business needs. These include outcomes you\\'d obviously equate with financial data such as financial health and long-term financial projections, but they can also include less obvious opportunities such as:\\\\n- • Helping with the post-Covid regeneration of the high street by intelligently planning store locations according to local demographics\\\\n- - Enabling better economic policy making through a more accurate and timely understanding of macro-spending habits across defined segments and markets\\\\n- - Managing trusted identity credentials to enable safer and more convenient access to products and services\\\\n- - Reducing criminality by addressing money laundering, fraud and other financial crimes.\\\\n\\\\n## The impact of data commerce\\\\n\\\\n\\\\nresilience through diversification of their core business model and building more compelling relationships with their customers. We would also expect to see a change in attitudes and budgets, as data shifts from being an overhead to a core, and quantified, part of P&L.\\\\n\\\\nOne area that remains unclear is how this will affect the relationships between large, established financial services companies and fintechs. There continue to be a range of ways the two engage, from forced interactions through API-based data sharing, through sponsorship, direct investment, partnerships and acquisition. As data becomes a driver of revenue, the technology, skills, culture and data contained within fintechs may take on an even greater significance.\\\\n\\\\nWhile Open Banking APIs have given fintechs a headstart, established financial institutions can still seize the opportunity to build better products and services driven by data. But more exciting is the opportunity to create new and compelling business models where the actual data itself, and high-value derived insights, are the product. Financial institutions, their customers and society at large all stand to benefit greatly.\\\\n\\\\nAt Harbr we\\'re already seeing banks and other financial institutions building new lines of business focused solely on commercialising their data and doing so in a way that respects privacy and provides social benefits. The underlying data, the quality of thinking and the level of investment dictate the size and nature of these businesses and will no doubt impact their future success. When compared to their competitors, these organisations should expect to have better financial results and greater\\\\n\\\\n11 / rise.barclays\\\\n\\\\n\\\\nPage 6\\\\n---\\\\n\\\\n## What is AI?\\\\n\\\\n\\\\nComputer science (CS) is a scientific and practical approach to computation and its applications. A computer scientist specialises in the theory of computation and the design of computational systems. Artificial Intelligence (Al) is a subset of CS. In its simplest form, Al deals with operations like \\\\\"if the price of A is higher than the price of B, then buy B.\\\\\" More formally, Al sets forth explicit rules that are followed exactly as stated, like a recipe. Result: you control the process and the outcome.\\\\n\\\\nA subset of AI is Machine Learning (ML), which deals with the development of computer programs (systems) that can access data and use it to automatically learn and improve their performance from the data they encounter. Unlike AI systems, ML systems learn from data and make decisions while trying to improve their own performance.\\\\n\\\\n## The data environment is key\\\\n\\\\n\\\\nML typically learns and deals with problems represented by static or dynamic environments. First, take static (or semi-static) environments. For a facial recognition system, you can apply an ML system like a neural network (ideally suited to\\\\n\\\\npattern recognition and classification problems), because your face doesn\\'t change much and they do not need to be trained often.\\\\n\\\\nBut what happens when the problem constantly changes? Think finding investment opportunities in a financial market.\\\\n\\\\nTo explain this, you need to understand why an ML system designed for a static environment wouldn\\'t work. Financial markets constantly fluctuate and, in automated decision-making, the system makes decisions by analysing all the data that humans cannot process, and eliminating human biases which tend to affect the investment management process. This is a dynamic environment.\\\\n\\\\n12 / Rise Insights\\\\n\\\\n\\\\n## Is AI a modern phenomenon?\\\\n\\\\n\\\\n## AI in finance\\\\n\\\\n\\\\nNo—it\\'s been with us for millennia. The origins of AI date back to ancient history. For example, robotic, humanoid servants and automatic doors were the AI imaginings of ancient Greeks.¹ Fast-forward thousands of years to 1940 and the first digital computer created by Alan Turing. He\\'s considered the father of theoretical Computer Science and AI.\\\\n\\\\nThe birth of AI as a field of research is widely considered to have started in 1956 at a workshop in Hanover, New Hampshire, where participants discussed how learning could be simulated by a machine.² By 1966 more funding and increased interest helped establish the Department of Machine Intelligence and Perception at the University of Edinburgh.³ The MIT AI Lab was established in 1970.⁴\\\\n\\\\nThe current state of AI is fuelled both from academia and the private sector. AI applications are embedded across the technical infrastructure of many industries, including manufacturing, autonomous driving, autonomous investing and medical diagnosis. Advances have been made with Natural Language Processing, image recognition and search engines. And don\\'t forget games: AlphaZero (chess), AlphaGo (Go) and Watson (Jeopardy) are all non-human champions.\\\\n\\\\nIn the early 1990s, an attempt was made to popularise neural networks in finance.⁵ They were used to predict prices in equities, commodities and FX, as well as to create macroeconomic and risk models. Back then, most applications used structured data, such as that from daily stocks and FX rates.\\\\n\\\\nBecause early results of Al models were encouraging, some of these early neural networks were deployed in live trading. But over time, it was discovered that these systems had limitations as the environments they were in were so dynamic.\\\\n\\\\nIt seems that humans must learn first, then AI.\\\\n\\\\n| | | |\\\\n| ----- | ----- | ----- |\\\\n| 1. Reuters | 2. Livescience | 4. Wikipedia |\\\\n| 3. The University of Edinburgh School of Informatics | 5. ML Quant | 13 / rise.barclays |\\\\n\\\\n\\\\nPage 7\\\\n---\\\\n\\\\n## While AI is critical for banks\\' future...\\\\n\\\\n\\\\nThere are several ways AI can accelerate financial services and make it function more efficiently. However, the sector is lagging behind other industries in implementing the technology, such as healthcare and medicine.\\\\n\\\\nBanks and other financial services organisations can greatly enhance their operations in investment management, wealth management and the crypto market by applying Al. For example, while ESG is on the minds of most investors these days, there are volumes of sustainability data that could be used during the portfolio construction stage. Asking humans to manage this process manually is prohibitively time consuming, but Al could be used to sort through mountains of data in a fraction of the time.\\\\n\\\\nThe crypto market is another obvious use case: There are over 8,000 crypto currencies, exchanges, platforms and ecosystems offering certainly innovative and possibly attractive returns, but there is no straightforward method of contending with all the related data.⁶ This is where blockchain and AI intersect. For example, while blockchain stores all the trade data in the crypto market, AI can be used to not only analyse the data, but to create products and trading opportunities.⁷\\\\n\\\\n## ...fintechs are driving adoption\\\\n\\\\n\\\\nAccording to one study run just two years ago, financial services will see a mass adoption of AI starting in a year.⁸ But while organisations have the obvious benefits of AI in mind, the fact remains that they\\'ll need to integrate it with their legacy systems or replace them completely. Enter fintechs.\\\\n\\\\nLarge companies do not necessarily have the infrastructure needed to deal with vast amounts of data, but smaller fintechs do. They\\'re inserting themselves into the value chain aggressively with big funding rounds to support their growth.⁹ Voice assistant, or chatbots, used for customer service are an example. By implementing AI, some fintechs have empowered large banks to greatly enhance their customers\\' experiences and improve efficiency in their phone channels— without the need for a human representative. One study shows that, in 2023, 826 million hours will be saved by banks by using voice assistants. ¹⁰\\\\n\\\\nAl has come a long way since Ancient Greece. It\\'s now evolving at a rapid pace in a short amount of time and its applications are multiplying. But while Al can help financial organisations leverage opportunities, they first need to understand what it is, how they want to use it and the real costs associated with it.\\\\n\\\\nDr Sonia Schulenburg CEO & Director, Level E Research and Investment Committee Member & Director, Level E Capital\\\\n\\\\n\\\\n- levele.ai - levelecapital.ai\\\\n\\\\n| 14 / Rise Insights | 6. CryptoPR 7. Medium | 8. Forbes 9. UK Tech News | 10. FinTech Magazine |\\\\n| ----- | ----- | ----- | ----- |\\\\n\\\\n\\\\n\\\\\"Large companies do not necessarily have the infrastructure needed to deal with vast amounts of data, but smaller fintechs do.\\\\\"\\\\n\\\\nDr Sonia Schulenburg CEO & Director, Level E Research and Investment Committee Member & Director, Level E Capital\\\\n\\\\n- RS:/ 011 - RS:/ 011\\\\n\\\\n\\\\n►RS:/0211TR /ON ►RS:/0211TR /ON\\\\n\\\\n\\\\n-\\\\n\\\\n\\\\n15 / rise.barclays\\\\n\\\\n\\\\nPage 8\\\\n---\\\\n\\\\n## Survival tips for startups: Know your regulator\\\\n\\\\n\\\\nFounders building data-driven startups and their advisors (especially their lawyers) have probably learned a hard truth about building Artificial Intelligence (AI) and Machine Learning (ML) technologies—it is much harder than it looks. New entrants are required to navigate a dense fog of old laws, new regulations and unclear guidance from policymakers about the use of AI and ML.\\\\n\\\\nAdditionally, as this technology ecosystem matures, we\\'re observing significant innovation in the use of data that exists on-chain. (On-chain data is stored within a blockchain— think the Ethereum ledger—and compares with off-chain data, which comes from another non-DLT source.) For example, companies like Google Cloud are already exploring the synergies between on-chain data and off-chain data by adding blockchain data to BigQuery, and allowing companies to apply traditional analytics tooling and AI/ML models to the mixture.¹ This is proving useful in deriving patterns and trends not previously identified by simply observing the blockchain alone.\\\\n\\\\nWhether companies are building applications and products that involve decentralised or traditional finance, how they use and apply AI/ ML models on datasets may impact their long-term viability down the road.\\\\n- - Historical, data-driven technical analysis, such as trend analysis/pattern recognition, and more involved quantitative analysis leveraging complex mathematics, such as stochastic calculus\\\\n- - Tasks and workflows where AI/ML replaces or - helps individuals analysing large datasets for - regulated financial institutions\\\\n- • Data-driven decision making that extracts insight from massive and discrete data assets.\\\\n\\\\nSome of these traditional applications are also increasingly being deployed in the blockchain and distributed ledger ecosystems—unleashing true innovation. Both private cryptocurrency exchanges as well as regulators are now utilising traditional AI/ML applications on datasets that include both traditional data and blockchain data.\\\\n\\\\n## Use cases\\\\n\\\\n\\\\nFor example, enrichment of blockchain data and off-chain information about users can help examine transactions in pseudonymous cryptocurrency wallets, either in criminal investigations² or as part of KYC/AML programmes for financial institutions like cryptocurrency exchanges.\\\\n\\\\nAs the AI/ML technology stack has evolved into more tangible offerings, enterprises across the globe are identifying more and more use cases to make sense of their data assets. Within the financial services sector, applications include:\\\\n\\\\n1. Google Cloud 2. Financial Crimes Enforcement Network\\\\n\\\\n\\\\n16 / Rise Insights\\\\n\\\\n\\\\nNon-financial services companies are experimenting with AI/ML to understand consumer engagement with new forms of digital media, such as non-fungible tokens. In many instances, these are purchased with traditional browser applications providing rich on- and off-chain data that can be analysed by AI/ML and tools like Google Analytics to derive insights on user engagement, trends and ROI.\\\\n\\\\n## The (legal) catch\\\\n\\\\n\\\\nThese new methodologies and increasingly valuable applications of AI/ML technology stacks are not without limitations or risks. While many regulations governing these use cases, such as the GDPR³ and the CCPA, ⁴ historically concerned data privacy, there are now additional AI-specific regulations being implemented or explored throughout the world.\\\\n\\\\nFor example, the European Commission\\'s proposal for the creation of the Artificial Intelligence Act (AIA) in 2021 should be carefully considered by companies looking to use AI/ML applications. On top of these data-specific regulatory schemes, companies may be forced to navigate vexing questions involving AI technologies applied to blockchain-based datasets—GDPR\\'s \\\\\"right to erasure\\\\\" of personal data is technically incompatible with data stored on blockchains such as Ethereum or Bitcoin. Additionally, companies will need to consider other potentially lesser-known regulations like the Bank Secrecy Act (BSA), applicable Virtual Asset Service Provider regulations, use-specific data privacy regs and other legal regimes for the KYC and AML use cases mentioned earlier. Understanding not only which laws, but also which regulators enforce those laws is important to navigating your compliance priorities - each agency functions differently, engages with market participants differently and can sometimes have competing priorities.\\\\n\\\\n3. General Data Protection Regulation\\\\n\\\\n\\\\n4. California Consumer Privacy Act\\\\n\\\\n\\\\nFor example, determining whether the AI/ML technology alone is sufficient to meet Treasury\\'s compliance requirements under the BSA in assessing the identity of customers opening accounts utilizing pseudonymous virtual wallets is still largely uncharted territory. Compliance programs may also have greater obligations to apply AI/ML to publicly-available on-chain data in connection with KYC obligations—in essence not just reviewing your customers documentary identification, but also their digital wallet\\'s transaction history to sufficiently screen out bad actors or tainted funds. There is likely an additional layer of complexity, and another regulator, who may have an opinion on whether the funds or digital assets reviewed as part of that program should be treated more like money and less like some other financial instrument. Companies would do well to anticipate these challenges by increasing awareness within their compliance teams about the nuances of blockchain technology, the applicable regulations and the AI/ML applications to both meet their product needs and compliance programme obligations.\\\\n\\\\nIncreasing opportunities to deploy AI/ML applications has also resulted in increased legal and regulatory scrutiny of those applications. The combination of additional new datasets derived from blockchain technologies is promising, but it also introduces potential regulatory hurdles. The long-term viability of startups working in this area may well depend on how quickly and effectively they can coordinate their technological and legal strategies. This could determine their ability to deploy their applications at scale in a globally-compliant manner.\\\\n\\\\nRichard Widmann Head of Strategy, Digital Assets, Google Cloud\\\\n\\\\n\\\\nin rich-widmann-a816a54b\\\\n\\\\n\\\\n17 / rise.barclays\\\\n\\\\n\\\\nPage 9\\\\n---\\\\n\\\\n## Data commercialisation\\\\n\\\\n\\\\nData isn\\'t just data – it\\'s comprised of many key components that have inherent value. How it\\'s used and safeguarded is critical.\\\\n\\\\n18 / Rise Insights\\\\n\\\\n\\\\n19 / rise.barclays\\\\n\\\\n\\\\nPage 10\\\\n---\\\\n\\\\n## The data revolution\\\\n\\\\n\\\\nSince the start of the century, we\\'ve become increasingly obsessed with, and reliant on, data. This singular focus has meant that almost unconsciously we\\'ve forever altered what it means to be human and triggered the creation of infrastructure capable of supporting a dual existence – physical and digital.\\\\n\\\\nThere are few people on this planet that don\\'t have a digital twin, in the form of a digital bank account, an Amazon account, social media profiles, etc. For many, these two worlds have inextricably merged.\\\\n\\\\nThe acceleration of this as a direct result of the global pandemic is well documented.¹ Overnight, most organisations in all sectors and even whole cities were confronted with the immediate need to adapt. As a result, they started to generate and consume more data in real time.\\\\n\\\\nThe proliferation and increased consumption of data has also highlighted the challenges we have with digital trust. Globally, there are initiatives underway (see panel) to address this and to build better bridges between regulated, trusted economic actors, such as governments, financial services and telecommunication companies, and the emergent personal data ecosystem that now includes people and things.\\\\n\\\\n## Global data trust initiatives\\\\n\\\\n\\\\n| | |\\\\n| ----- | ----- |\\\\n| UK | The Open Banking² regime empowers customers to make better financial choices by providing access to and control over their banking data. |\\\\n| EU | The Data Governance Act³ will deliver a legal framework for the sharing of personal, enterprise and public sector data, enabled by accredited data intermediaries. |\\\\n| Australia | The Consumer Data Right⁴ aims to deliver greater consumer data empowerment to drive competition and choice. |\\\\n| California | The California Consumer Privacy Act (CCPA) focuses on how data is used commercially and gives individuals more visibility of and control over personal information collected by businesses.⁵ |\\\\n\\\\n\\\\n20 / Rise Insights\\\\n\\\\n\\\\n1. McKinsey 2. Open Banking 3. EU Support Centre for Data Sharing\\\\n\\\\n\\\\n4. Australian Treasury 5. California Consumer Privacy Act\\\\n\\\\n\\\\n## User control and consent\\\\n\\\\n\\\\nThese global initiatives create valuable opportunities to deliver benefits for consumers who can now access, control and exchange data.\\\\n\\\\nThis scenario is a key foundation for Web3 and the metaverse. While the jury is out on some of the more ambitious predictions about the next iteration of the internet and related services, it\\'s clear that they\\'re not just about data. We all want to decide on how our data is used and to be offered personalised services. But we need to trust data networks in the way we trust payment networks. There must be certainty, transparency, compliance and accountability, and global, interoperable standards-based infrastructure to make this work at scale.\\\\n\\\\nThis move to enable both the physical and digital is driving three important data shifts:\\\\n\\\\nnetworks of value. For example, in Queensland Australia, the local government is exploring citizen-centric data sharing to create workplace credentials.⁶ Based on data minimisation, this builds strong trust and delivers significant efficiencies and process improvement.\\\\n\\\\n## Mutual incentives\\\\n\\\\n\\\\nThere\\'s an exponential shift towards the tokenisation of everything, from data to money and even time. The option to issue fungible and non-fungible value will change everything, from the way we invest in art to how we\\'re rewarded for loyalty. This new approach to creating and sharing value is at the heart of tune.fm, for example - a new marketplace for music combining streaming with payments and social connections.\\\\n\\\\n## Citizen-centric ecosystems\\\\n\\\\n\\\\nThere\\'s an evolution away from \\\\\"winner takes it all\\\\\" (a few large technology firms and their platforms) towards collaborative networks that address the significant challenges of our planet and modern digital lives. In the decade ahead, we\\'ll see this driven by decentralisation, and anti-trust and privacy regulation. This shift is at the heart of law suits in the EU and the US.\\\\n\\\\n## Peer-to-peer networks\\\\n\\\\n\\\\nThe move away from \\\\\"winner takes it all\\\\\" also means people, organisations and things each require the means to authenticate, authorise and provide use-case specific access. The drive towards decentralisation with the adoption of blockchain and DeFi is already generating new\\\\n\\\\n\\\\\"The ability to uniquely identify everything will enable everything to be uniquely valued.\\\\\"\\\\n\\\\nKatryna Dow Founder and CEO, Meeco\\\\n\\\\n6. Queensland Government\\\\n\\\\n\\\\n21 / rise.barclays\\\\n\\\\n\\\\nPage 11\\\\n---\\\\n\\\\n## Do no harm: The ethical data dilemma\\\\n\\\\n\\\\nCentral to the debate is how do we do all this in a way that helps individuals harness the value of their data.\\\\n\\\\nTo enable shared value, it\\'s essential that anything that can be tokenised will be. The tokens themselves will enable us to uniquely identify whether something is an asset, object, person or thing. If we get this right, and have clarity around provenance and ownership, there\\'s an opportunity to make data transactions more equitable. But to avoid the unconscious mistakes of the past, when the balance of power landed with a few large technology firms because the internet was not designed with a secure identity layer, society now needs to make some conscious design decisions.\\\\n\\\\nWe must also face the challenge of a world where deep fakes are possible. Just because we see or hear something digital doesn\\'t mean it\\'s true. The manipulation of digital content increases the need to design-in digital trust, provenance and verification. We don\\'t just want data. We want data that is verified and can be traced back to a trusted source.\\\\n\\\\nThe success of big data and social networks depended on the centralised ownership (and perhaps exploitation) of data. By way of contrast, Web3 promises participation, in place of exploitation, directly rewarding individuals for the value they create. By participating in this new approach to data, both individuals and organisations should be able to make better and faster decisions that are augmented by individual context and intent. This shared approach should also increase the value exponentially for both parties.\\\\n\\\\nService providers (that is, banks and other financial institutions) that treat their customers like partners, helping to solve problems, respecting privacy and explicitly asking for consent will ultimately win in this new economy. These are the hallmarks of trust and the foundation for requesting access to data to enable more personalised outcomes.\\\\n\\\\n\\\\\"Data that is accurate, timely, contextual and has intent makes it exponentially valuable.\\\\\"\\\\n\\\\nKatryna Dow Founder and CEO, Meeco\\\\n\\\\n22 / Rise Insights\\\\n\\\\n\\\\n## With trust comes value\\\\n\\\\n\\\\nThose that protect their customers\\' data in the same way they protect their customers\\' money stand to generate net new value through tailored products and services based on granular and context-specific information. This approach maximises the opportunities of Open Banking - the foundation of modern data-sharing capabilities in financial services - while minimising data collection and fraud. For example, an \\\\\"over_18\\\\\" credential could underpin the purchase of age-restricted goods or an \\\\\"under_14\\\\\" credential could help protect online spaces for minors.\\\\n\\\\nBanks could provide customers with a \\\\\"has_ bank_account\\\\\" credential that indicates they\\'ve undergone due diligence processes. In these ways, customers can limit the amount of personal information they provide to a third party website when enrolling in new services, reducing the proliferation of personal information.\\\\n\\\\nOrganisations wishing to reach new customers could, for example, rely on strong buying signals from pre-qualified customers who are able to easily demonstrate that they meet an age or credit threshold.\\\\n\\\\nThis paradigm shift has been coined the Intention Economy⁷ in contrast to the Attention Economy, in which advertisers compete for consumer eyeballs. The Intention Economy (suggested over a decade ago) puts individuals at the centre of transactions, the buyer notifies the market of the intent to buy and sellers compete for the buyer\\'s purchase. The buyer is able to selectively disclose relevant data to complete a purchase. This is the real data revolution. The combination of new data regulation, increased consumer awareness of privacy and advanced technology has made it achievable.\\\\n\\\\n7. Linux Journal\\\\n\\\\n\\\\n23 / rise.barclays\\\\n\\\\n\\\\nPage 12\\\\n---\\\\n\\\\n## Where to start\\\\n\\\\n\\\\nThe introduction of the EU\\'s General Data Protection Regulation⁸ (GDPR), Open Banking, PSD2⁹ and the soon-to-be introduced Data Governance Act¹⁰ (DGA) all focus attention on compliance. However, when these regulatory requirements are implemented in consumer- centric ways that protect customers - such as Privacy and Security by Design - compliance quickly becomes an opportunity to differentiate.\\\\n\\\\n## Here are some practical steps to consider:\\\\n\\\\n- 1. Help customers access, protect and gain value from their data. Digital trust is the cornerstone of data protection and data access. For financial service providers, maintaining that trust by helping customers in this way is the natural evolution of helping them access, protect and gain value from their money.\\\\n- 2. Allow customers to exercise their digital rights. This plays an important role in protecting their identity, verifying their data and easily adopting new products and services. It can start with something as simple as helping people understand their digital rights and develop into deploying data custodian services, which store and protect individuals\\' data to reduce complexity in their lives.\\\\n\\\\nDigital wallets that enable people to manage payments and identity in a single application (like a \\'super app\\') have huge customer appeal. Financial service providers have an opportunity to deliver a similar utility but with a markedly different approach to data use. A more citizen-centric approach based on more equitable and ethical considerations to data is entirely possible given today\\'s technology.\\\\n- 3. Enable customers to track how and where their data is used, and selectively disclose and revoke access. This also provides ways for banks to safely access and process data when customers request or approve. This underscores data compliance and reduces the cost and burden of data collection and processing.\\\\n\\\\n## The metaverse and the new data frontier\\\\n\\\\n\\\\nIn the first nine months of 2020, as gaming surged under COVID-19 lockdowns, Roblox earned $1.2 billion from selling virtual currency. 11 Roblox users are typically eight to 18 years old and mostly under 13. While purchases are most likely under adult supervision, this is a generation growing up with digital currency, tokens and rewards. It\\'s a virtual data world that is wiring an entire generation for user experiences they will come to expect in the physical world.\\\\n\\\\nThis virtual-digital generation will soon be expected to navigate opening a bank account, making payments, selecting offers and confirming choices in the physical world.\\\\n\\\\n24 / Rise Insights\\\\n\\\\n\\\\n8. GDPR.EU\\\\n\\\\n9. European Commission 10. Data Governance Act 11. Financial Times\\\\n\\\\nWhy is that important? Because currently, many of the data protections in place for children may begin to erode as their digital twins become monetised through social networks and ad platforms. 12 New measures recently introduced in the UK, for example, look to address this challenge. 13 This is a generation whose digital footprint is also a virtual asset. As for any asset, safeguarding it, first, and compounding its value, secondarily, are key.\\\\n\\\\n\\\\\"Digital identity is at the heart of almost everything we are trying to do now and will continue to do in a post-pandemic world.\\\\\"\\\\n\\\\nKatryna Dow Founder and CEO, Meeco\\\\n\\\\nWe\\'re well on the way to having an entire generation that is comfortable with the idea of things being tokenised in the virtual world. The importance of educating this generation in the management of wallets and digital keys cannot be underestimated. They need the tools to prove their identity, sign up safely and protect themselves against fraud, all delivered with a superior, digital-native experience.\\\\n\\\\nWe\\'re starting to see digital life moving between ledgers and networks within trusted data ecosystems. The core design principle is to ensure that digital twins are protected and private. This provides significant opportunities for financial service providers to offer data verification, data provenance and custodial services.\\\\n\\\\nThere will need to be incentives for everyone and importantly, playing to the strengths of banks, a treasury function to enable the exchange of value at the moment that value is agreed, with fast settlement and transparency. As part of this, governance – part of the financial service industry’s DNA – could help to monitor fairness and ensure greater equity for customers.\\\\n\\\\nIn this amazing new world, what data future will you build?\\\\n\\\\nKatryna Dow Founder and CEO, Meeco\\\\n\\\\n\\\\n| | |\\\\n| ----- | ----- |\\\\n| Pointer icon | meeco.com |\\\\n| LinkedIn icon | katrynadow |\\\\n| Twitter icon | @katrynadow |\\\\n\\\\n\\\\n12. Guardian. Children\\'s digital privacy code 13. The Children\\'s Code\\\\n\\\\n\\\\n25 / rise.barclays\\\\n\\\\n\\\\nPage 13\\\\n---\\\\n\\\\n## The Netflix era: Data rentals\\\\n\\\\n\\\\nMost of us know or have used Netflix. But do you remember the company\\'s original business model? Netflix started by renting or selling DVDs to customers via post.\\\\n\\\\nToday, this type of content—or data—is largely rented or purchased online for a flat fee—there is no physical ownership of a DVD. With data residing on the vendor\\'s servers or cloud storage, there is no need to own a DVD. A similar concept can be found in financial services (FS).\\\\n\\\\nBuying and selling data within large FS companies isn\\'t a new concept, but it often isn\\'t quick and easy like with Netflix. Internally, bank data typically originates from many customer segments, asset classes and regulatory regimes. Procurement isn\\'t simple, either—data providers, exchanges and banks argue over usage, uptime availability, contributed data and rights management on an annual basis when payments are due. Consultants and audit firms are often engaged to ensure regulatory compliance is met. Result: banks spend more time administrating data than they do discovering strategic opportunities with technology such as artificial intelligence (AI).\\\\n\\\\n## Focus on the data\\\\n\\\\n\\\\nAccording to one survey of 10 industries in 2020¹, banking made more data-driven decisions than any other by a healthy margin. With all this focus on data, it\\'s a burden upon banks to ensure that data works for them and not the other way around. Enter the Chief Data Officer (CDO), which more companies are hiring to focus on data, processes and technology.\\\\n\\\\nFirst, a CDO can break down silos to make data easier to find so datasets don\\'t have to be recreated or duplicated. 2 According to one report, people spend 60% to 80% of their time just trying to find data. 2 One way to alleviate bottlenecks is to consolidate data with APIs, which create streamlined datasets. Creating a comprehensive data catalogue to empower keyword searches is another way.\\\\n\\\\nNext, a CDO can promote internal data democratisation and literacy. ² Too often, data isn\\'t freely shared among departments, which can create data silos and even \\'knowledge silos\\'. CDOs can drive education across departments to help alleviate the problem.\\\\n\\\\nFinally, a CDO can support new products and services that successfully commercialise data. According to Della Shea, Chief Privacy Officer at Symcor, her job is to create operating models that leverage data as an asset. But you also need to communicate the strategy: \\\\\"Executive management is more likely to invest... \\\\[if the CDO shows]...a full understanding of the \\'what\\' and \\\\[executives] understand the \\'why.\\'\\\\\"²\\\\n\\\\n## Who\\'s paying attention to data right now?\\\\n\\\\n\\\\nThe Chicago Mercantile Exchange is the largest securities firm in the world by market cap. In response to customers\\' data demands, they built DataMine—a data webstore built with Ticksmith. Today, they have over 75,000 data products that customers can access themselves.3\\\\n\\\\n26 / Rise Insights\\\\n\\\\n\\\\n1. Statista 2. MIT Sloan School of Management 3. Ticksmith\\\\n\\\\n\\\\nStep outside FS and consider Tesla. Do they only make electric vehicles or are they a streaming company capturing petabytes of data on driving logistics, behaviours and user preferences? This data gets fed into AI systems for driverless cars as well as improved instructions and specs for the hundreds of suppliers and assemblers in their value chain. This multichannel capture and participation in data/analytics by a wide audience may be what\\'s driving valuations and stock prices.\\\\n\\\\n## But there are challenges\\\\n\\\\n\\\\nWhile AI and ML are players in the journey toward data commercialisation, companies\\' data philosophies must be transformed from descriptive to predictive to prescriptive. Descriptive data helps answer simple questions like what and when; predictive analysis takes a stab at predicting future outcomes with historical data; and prescriptive analysis takes predicting the future up another notch with ML. This transformation will help organisations become more responsive to new opportunities as well as more efficient with capital and talent.\\\\n\\\\nIn order to fine-tune data-driven decisions, banks may need to replace legacy systems that don\\'t work well with purely data-driven technologies⁴, such as AI and machine learning (ML). This means employing tools that require significant computing power. And with technology comes technologists—banks also need to attract and retain the talent that understand these new technologies.\\\\n\\\\nNext, think of data quality. If you find a bug, can you easily fix it? If you own the data or have had it piped into your organisation, you can fix it in situ. But if the data has been leveraged externally from an API, you must wait for the vendor to fix the problem. While data is an valuable resource, its value declines if the curation process behind it doesn\\'t ensure quality control, provenance and lineage.\\\\n\\\\nData isn\\'t just A, B, C or 2+2=4. Other factors must be accounted for, such as social considerations. According to Gartner, in 2020 media mentions of ESG grew 303% YoY and 85% of investors considered ESG factors in their decisions.⁵\\\\n\\\\n## Data is the future, the future is data\\\\n\\\\n\\\\nData is a time-stamped commodity that has its own shelf life. By enhancing the methods used to retrieve it, banks may be able to mirror Netflix\\'s model of offering quick-and-easy data streams. But in order to get to that point, banks should first answer important questions about data: what, why, and how. The what is the data itself and where it lives within your firm, the why explains the need to invest in order to maximise opportunities, and the how is the processes and technology required to harvest it at its peak benefit.\\\\n\\\\nMark Rodrigues Executive Chairman at Ticksmith and Venture Partner at Illuminate Financial\\\\n\\\\nin markrodriguestr\\\\n\\\\n4. Protocol 5. Gartner\\\\n\\\\n\\\\n27 / rise.barclays\\\\n\\\\n\\\\nPage 14\\\\n---\\\\n\\\\n## AI use cases in finance\\\\n\\\\n\\\\n28 / Rise Insights\\\\n\\\\n\\\\nAl technology empowers banks with a range of benefits, from providing more accurate client services to implementing predictive models that prevent data breaches.\\\\n\\\\n29 / rise.barclays\\\\n\\\\n\\\\nPage 15\\\\n---\\\\n\\\\n## ML takes aim at FinCrime risks\\\\n\\\\n\\\\nFinancial crime has a direct connection to corruption, terrorism, human trafficking, drugs trafficking and illegal arms dealing. The financing of these activities has an adverse impact on society and communities around the world.\\\\n\\\\nSome of the activities are low cost but high impact – the 2015 Paris terrorist attacks cost less than €10,000 but caused over €2 billion worth of damage.¹ It is estimated the annual cost of corruption and bribery alone is $3.6 trillion (ICA), ² and banks spend over $200 billion a year on financial crime compliance.³ Financial services organisations are struggling to contain compliance costs including fines due to breaches, but the threat of money laundering continues. There is an urgent need for change.\\\\n\\\\nThis is where Machine Learning (ML) comes in. Using ML, banks can now build behavioural- based segmentation models that look not just at demographic attributes, but at payment patterns and other human behaviours. ML enables financial services organisations to incorporate multiple features representing these behaviours into models and they can find which combination best defines targeted customer segments. Ultimately, these predictive models show the strongest combination of features/behaviours that define each group, which in turn allows more accurate risk analysis.\\\\n\\\\nHuman beings display set behaviours, and criminals have particular ones that make them outliers. ML models seek to identify these. For example, ML models can be deployed to determine a pattern of what known criminals do with excess cash deposits compared to those with no criminal records. But models need to get deeper into the weeds to be more accurate.\\\\n\\\\n## Greater segmentation makes more accurate predictive models...\\\\n\\\\n\\\\nTraditional segmentation is typically based on shared demographic characteristics, such as gender, age or financial product use. While this is a good start, it ignores behavioural characteristics that are critical in understanding the broader risk profile of a specific group. In order to create a more effective model, it must be as specific as possible.\\\\n\\\\nTo explain why this is important, here\\'s an example: Two people of the same gender have similar salaries. One makes regular withdrawals and the other makes fewer, larger withdrawals. One person has a busy social life who makes frequent withdrawals whereas the other is a parent who needs to make fewer transactions but of larger value. Yes, these two people are demographically the same based on income and have similar products (e.g., a savings account), but they should be assigned different levels of risk based on frequency of transactions. Why? Because, in your predictive model for this segment (two high earners of the same gender), identifying transactional behaviour becomes the key classifier/feature in refining the model: large cash withdrawals could indicate mule activity, or money laundering.\\\\n\\\\nHere\\'s another example: Two people earn $100,000 and $1 million per year. Both have a premium current account, but obviously with\\\\n\\\\n30 / Rise Insights\\\\n\\\\n\\\\n1. Quartz 2. World Economic Forum 3. Yahoo\\\\n\\\\n\\\\ndifferent thresholds. If the first person transferred $10,000 in one day, it could be considered a risk event because it represents 10% of their earnings. If the latter individual transferred the same amount, it would amount to only 1% of their earnings and, given a threshold based on percentage, would be deemed less risky.\\\\n\\\\n## ...but predictive models aren\\'t without problems\\\\n\\\\n\\\\nBanks must be able to explain their conclusions. The capabilities of ML are powerful, but there is a limit to how far you can pursue matters in a legal sense. Banks are not criminal enforcement bodies, but need to be able to explain the results of the models to the relevant authorities, who in turn take action. Banks must be able to produce a rationale behind the proprietary scores generated by their predictive models and must also demonstrate their findings aren\\'t biased.\\\\n\\\\nTaken a step further, over-complicated models, such as neural networks, may be very accurate, but because of their black-box nature, it can be difficult for banks to unpack all the features that lead to a specific combination of behaviours indicating potential criminality. Of course, this is critical when communicating results with law enforcement or in court.\\\\n\\\\nFor some monitoring systems, productivity levels are in single figures because too many inaccurate alerts are created, which means that analysts spend about 90% of their time reviewing alerts that are of little or no value instead of reviewing \\'productive\\' alerts. 4 This creates a significant cost for banks because analyst time is not used in an optimal way. This is why alerts must continue to be refined.\\\\n\\\\n## Continual evolution based on risk scoring\\\\n\\\\n\\\\nRegulators have advised that FinCrime alerts should be investigated using a risk-based approach that is more efficient and identifies the biggest risks. 5 This requires understanding which models are most effective so they can be emulated for new ones while also retiring underperforming models. Risk, as well as businesses, change over time, so the efficacy of any model must be periodically re-examined.\\\\n\\\\nIncreasing model efficacy is made easier by applying ML technology. But the human element can\\'t be removed from this equation. Through their models, risk officers must learn from past mistakes and successes. New models can always be built and existing ones can be tweaked or decommissioned. And every model—whether successful or not—can be used as a learning opportunity.\\\\n\\\\nOver-simplified models, such as regression or decision trees, can easily output false-positive alerts because rules may be too broad or too narrow for the customer segment. False- positives generated for client or transaction segments can be counterproductive because they may not accurately identify risk or where further investigation is required.\\\\n\\\\nSamuel Safo-Kantanka VP Infrastructure, Surveillance Analytics, Barclays\\\\n\\\\n🐦 @skantanka\\\\n\\\\n4. IBM 5. Financial Conduct Authority 31 / rise.barclays\\\\n\\\\n\\\\nPage 16\\\\n---\\\\n\\\\n## Company spotlight:\\\\n\\\\n\\\\nQuantexa\\'s data analytics software connects data in a meaningful way to drive trusted decisions.\\\\n\\\\n## The company\\\\n\\\\n\\\\nData-intensive organisations face the same challenge – closing the gap between the vast amount of data they accumulate and the millions of mission-critical decisions that need to be made every year. Pioneering Contextual Decision Intelligence (CDI), Quantexa\\'s software enables organisations to harness the power of their internal and external datasets by identifying hidden risks and uncovering previously undiscovered opportunities\\\\n\\\\nacross financial crime, Know Your Customer (KYC), fraud, credit risk, customer insight and data management.\\\\n\\\\nFounded in 2016, the London-based company is backed by a number of investors, including Warburg Pincus, Dawn Capital, British Patient Capital, Albion VC and other leading firms in venture capital.\\\\n\\\\n-\\\\n\\\\n\\\\n## The proposition\\\\n\\\\n\\\\nThe compliance departments of large financial institutions are experiencing increased pressure as the digital world adds sophistication and complexity to financial crime activity. It\\'s costly and inefficient to manually wade through mountains of data in order to effectively prevent financial crime.\\\\n\\\\nQuantexa Syneo provides organisations with a powerful financial crime or fraud toolkit that combines automation with human expertise. This frees up operational staff to make faster, better decisions by uncovering hidden connections between data points. The platform can generate up to 75% fewer false positives compared to less sophisticated technologies.\\\\n\\\\n32 / Rise Insights\\\\n\\\\n\\\\n## quantexa\\\\n\\\\n\\\\n## Key features\\\\n\\\\n\\\\n## More accuracy\\\\n\\\\n\\\\nEntity resolution overcomes missing or poor quality data with 99% accuracy\\\\n\\\\n## Greater efficiency\\\\n\\\\n\\\\nWith an intelligence-led approach, investigations are 80% faster\\\\n\\\\n## Future-proofing\\\\n\\\\n\\\\nGranular security levels provide dynamic control with all activity audited\\\\n\\\\n## 👤 Working with Barclays\\\\n\\\\n\\\\n## The executive team\\\\n\\\\n\\\\nSince 2017, Barclays and Quantexa have been working together to implement Quantexa\\'s award-winning entity resolution and network analytics technology. The Barclays Financial Crime and Compliance teams use Quantexa to help improve financial crime processes across multiple use cases such as AML investigations, mules fraud, terrorist financing and human trafficking.\\\\n\\\\nThe technology is currently implemented to identify markets money laundering risk, and the next project will be for correspondent banking money laundering identification.\\\\n\\\\nquantexa.com\\\\n\\\\nVishal Marria CEO and Co-founder in vishalmarria\\\\n\\\\n\\\\nAlexon Bell Chief Product Officer and Co-founder in alexon-bell-096474\\\\n\\\\n\\\\nLaura Hutton Chief Customer Officer and Co-founder\\\\n\\\\nlahutton\\\\n\\\\nJamie Hutton Chief Technology Officer and Co-founder\\\\n\\\\nin huttonjamie\\\\n\\\\n33 / rise.barclays\\\\n\\\\n\\\\nPage 17\\\\n---\\\\n\\\\n## Data and AI in institutional investing: More revolution than evolution?\\\\n\\\\n\\\\nWhile the investment industry has always relied on statistical methods (think linear regression) that would now be described as Artificial Intelligence (AI) or Machine Learning (ML), in recent years we\\'ve seen a strong increase in the use of data and Al by institutional investors in an attempt to reduce bias and make more informed investment decisions.\\\\n\\\\nThis was made possible by progress in AI and the growth in datasets available for institutional investors, which has led many companies (including non-financial services ones) to monetise their data for investors.\\\\n\\\\n## AI is here to stay\\\\n\\\\n\\\\nDevelopments in ML are now allowing institutional investors to examine large volumes of data and discover relationships and insights that would not be obvious to human analysts.\\\\n\\\\nFor instance, Natural Language Processing (NLP)¹, which enables a computer to read and interpret textual data, can extract sentiment from a CEO\\'s speech, identify the subject of a press release or determine company names in an analyst report. These techniques allow investors to discover value in new and existing data sources, some with very long and complex histories such as regulatory filings. They also reduce the need for an army of analysts to review each document, which can be hundreds of pages, and extract key elements. Whether investment decisions are\\\\n\\\\nmade by a computer or a human, the ability to distil large amounts of dissimilar – sometimes unstructured (text, images or voice) – information into actionable insights has been the key driver for the adoption of ML within the investment industry.\\\\n\\\\nThe explosion of computing power, either in the cloud or on premise, has also made it possible to process and analyse billions of financial market events in seconds. For example, adoption of GPUs (originally created for computer graphics) can cut the processing time of certain AI algorithms by a third or more². Similarly, with the price of computer storage cut by five since 2010³, investors have been able to gather an ever-increasing amount of data.\\\\n\\\\nWhile new investment approaches involving AI are more powerful than human-only methods, one downside is their additional complexity. More AI generally means more quants or the upskilling of non-tech institutional investors.\\\\n\\\\nSome approaches, such as deep neural- networks, may even be considered black boxes, in which the underlying drivers of the\\\\n\\\\n34 / Rise Insights\\\\n\\\\n\\\\n1. Similarweb 2. LexisNexis 3. jcmit.net\\\\n\\\\n\\\\nalgorithm decisions are not easily understood by humans. If applied to direct investments, they can lead to substantial financial losses very quickly. This highlights how human involvement remains crucial at each step of the investment process, from model creation to risk management.\\\\n\\\\n## It\\'s a data deluge\\\\n\\\\n\\\\nThe past decade has seen a staggering increase in the depth and breadth of data sources available to investors. The ‘alternative data’ market, which includes datasets not covered by traditional price and financial data, has been growing by an estimated 40% a year⁴.\\\\n\\\\nExtracting data from social media or public websites, a technique known as web-scraping, has increasingly become part of the investment process in an attempt to capture sentiment towards companies. The GameStop episode of 2020⁵, during which members of the message board Reddit managed to affect stock prices, shed light on the influence that public data can have on investments. As an example of how such data can generate even more derived datasets, some data vendors subsequently began\\\\n\\\\nproviding the count of Reddit\\'s \\'rocket\\' emoji in messages as a prediction of stock potential.\\\\n\\\\nAs the world changes, so does the data used for investing. The rise of ESG over the past few years has led investors to consider sustainability data in their processes, and they now expect a high degree of transparency and disclosure from their portfolio companies. Similarly, with COVID-19 putting pressure on global trade, the use of satellite imagery or datasets representing supply chain relationships between companies is an already strong and growing trend.\\\\n\\\\n\\\\\"The alternative data market has been growing by an estimated 40% a year.\\\\\"\\\\n\\\\nNicolas Guilloux\\\\n\\\\nSystematic Data, Maven Securities\\\\n\\\\n4. Yahoo Finance 5. International Banker\\\\n\\\\n\\\\n35 / rise.barclays\\\\n\\\\n\\\\nPage 18\\\\n---\\\\n\\\\n## Monetising data from surprising sources\\\\n\\\\n\\\\nAs institutional investing shifts to a more data-driven approach, some traditional players in the industry have started establishing businesses that monetise their data as a standalone Data-As-A-Service offering rather than attracting clients to their traditional products such as brokerage. For instance, some major US investment banks now sell subscriptions to their internal datasets through their institutional platforms.\\\\n\\\\nSome retail financial institutions, particularly in the US, have also been selling anonymised credit card data as a way for investors to estimate the future performance of companies or sectors. Although this is may be an opportunity for some, it poses a number of regulatory and reputational challenges for companies wishing to sell their customers\\' data.\\\\n\\\\nIn recent years, we have also seen the emergence of companies from other industries starting to generate extra revenue by selling new, alternative datasets to a diversified customer base while avoiding selling their treasured data to direct competitors. For example Similarweb, a leading digital analytics firm has been offering access to its web-traffic data, allowing institutional investors to understand in near real-time the sales potential of their stock portfolio. Even companies far\\\\n\\\\nremoved from financial services, such as publishing, have opened their datasets to institutional investors. For example LexisNexis, a well-known company in legal and news publishing has made its news data available for purchase to investors to help them create and train their ML models.\\\\n\\\\nAs these and other new datasets become available, and as more companies release their data, they\\'re becoming more integral to the mainstream investment process. Coupled with the rapid pace of innovation in this area, the impact of Al and data across the investment industry is likely to be profound.\\\\n\\\\nNicolas Guilloux Systematic Data, Maven Securities mavensecurities.com nicolasguilloux\\\\n\\\\n\\\\n36 / Rise Insights\\\\n\\\\n\\\\n## Improved credit decisions for the unbanked and Gen-Z\\\\n\\\\n\\\\nCredit decisions can impact the direction of many citizens\\' lives. One of the most common ways in which credit scores are used is when a borrower seeks a loan.\\\\n\\\\nA lender focuses on two important metrics prior to underwriting: the credit score of the borrower and their affordability. Simply put, an individual\\'s credit score quantifies their perceived willingness to repay a loan. Traditionally this value has been measured by previous loans and credit that a borrower has or has not repaid on time. Affordability, on the other hand, considers whether a borrower can afford the monthly repayment they will have to make if the loan is approved.\\\\n\\\\n\\\\\"Younger generations are not taking credit in the same way older ones are.\\\\\"\\\\n\\\\n## Ethan Fraenkel\\\\n\\\\n\\\\nCo-founder and CEO, ProGrad\\\\n\\\\nThe emergence of Artificial Intelligence (Al) has massively impacted the credit underwriting industry. Al offers the opportunity to consider complex, unstructured data from a variety of sources to make better decisions on someone\\'s credit score and their affordability. An example is the use of Al in Open Banking to categorise transactional data.\\\\n\\\\nIn its simplest form, Open Banking allows a person or a company to provide access to all their bank account transactions, for the previous two years, to a consented third party. Considering that typically people make over 850 transactions a year, ¹ Open Banking provides a unique use case for Al to ingest and make inferences on large quantities of data, in a short period of time.\\\\n\\\\nAl can examine almost every single transaction made by millions of people and categorise the transactions as spending on food or beverages, as income received, etc. This categorisation can be used to help consumers with budgeting, saving or even to enhance their credit score when they\\'re looking for a loan.\\\\n\\\\nOpen Banking has been important to improving financial inclusion. It\\'s estimated that in the UK alone there are almost six million \\'credit invisible\\' people (individuals with thin or no credit files). The emergence of Open Banking has given these previously ineligible individuals greater access to credit. The reason for this is simple: the \\'credit invisible\\' may not have a credit file but most of them will have made transactions through debit cards. This transaction history makes it possible to assess their creditworthiness for the first time in an accurate manner, reducing the information risk associated with thin-credit or no-credit file borrowers.\\\\n\\\\n1. CB Insights\\\\n\\\\n\\\\n37 / rise.barclays\\\\n\\\\n\\\\nPage 19\\\\n---\\\\n\\\\nToday, Open Banking is used in many other ways and has a variety of use cases. The two most important ones are the ability to manage personal finances through the consolidation of different bank accounts, and the ability to simplify access to credit. For this last use case, Open Banking combined with Al is now often used to provide access to short-term credit in a matter of seconds. The most famous application of this is Point of Sale credit (or \\'Buy Now Pay Later\\'). People can now make purchases of things through deferred payment plans, arranged at the time of purchase (unlike credit cards, which are pre-approved with a spending limit).\\\\n\\\\nThe use of AI to measure affordability for credit provisioning has also been a recent and fundamental development. Younger generations are not taking credit in the same way older ones are. Over 50% of people under 30 do not have a line of credit with traditional banks.² And banks often find it hard to price lending products, such as overdrafts, for Generation-Z.\\\\n\\\\nProGrad, a UK-based startup, has developed a technology that assesses current affordability and predicts future affordability through the use of unique and proprietary data. It offers the potential to give young people access to sustainable credit when they need it most. ProGrad uses new datasets to predict the income of students and graduates over their\\\\n\\\\n38 / Rise Insights\\\\n\\\\n\\\\n2. Bankrate\\\\n\\\\n\\\\nnext five years. This means that lenders can now create new lending products for young people by considering their current and future affordability in order to understand how much money a person will be able to spend at different points in time.\\\\n\\\\nThis capacity for dynamic prediction enables lenders to provide young people with credit that they can afford (rather than giving them more money they may struggle to repay). It also gives lenders a truer picture of customers\\' lifetime creditworthiness.\\\\n\\\\nThe adoption of Open Banking for credit scoring and Al to measure future affordability could be a solution to help increase lending opportunities to young people. Banks, for example, would be able to develop new products that are appealing to Generation-Z and also price overdrafts more optimally.\\\\n\\\\nEthan Fraenkel Co-founder and CEO, ProGrad\\\\n\\\\n\\\\nPage 20\\\\n---\\\\n\\\\n## The democratisation of investing\\\\n\\\\n\\\\nInvesting has become more democratic and investors more diverse. This trend is one of the biggest in financial services over the past two years, and it\\'s closely linked to the extraordinary increase in customer engagement, which has grown ten times¹ in the same period.\\\\n\\\\nNotably, as we went into lockdown, the industry witnessed higher volumes of investors engaging across digital channels, exposing the limitations of the traditional face-to-face model. Of the $78 trillion of total wealth invested in equities, the market share among digital platforms is now 20% and growing. Furthermore, the retail investor share of total US equities trading volume last year reached a record 23%.\\\\n\\\\nClearly, people are investing more actively than ever before and abandoning the old reliance on advisors to facilitate all of these services.\\\\n\\\\nMeanwhile, $30 trillion in assets is gradually moving from boomers to their heirs – the greatest wealth transfer in history. With this confluence of rapid technological innovation and major societal trends, we\\'re witnessing a shift in values and expectations in a new type of customer. Millennials, which in 2020 comprised over one third of the global workforce, ² currently represent most of this emerging, diverse audience with fundamentally different needs to previous generations.\\\\n\\\\n## How AI benefits investors\\\\n\\\\n\\\\nArtificial Intelligence (Al) improves access to investing for a more diverse audience of previously excluded investors – from women and younger people to Black, Asian and minority ethnic (BAME). The implementation of behavioural science, combined with robust Al analytics capabilities, allows fintechs and larger financial services firms to avoid demographic bias and identify genuine customer interests based on user activity rather than ethnicity, gender or education. As a result, for example, retail investors can now be helped in making informed decisions with curated content (such as tailored financial education), guided away from risky investments or supported in improving their long-term investing behaviour and outcomes.\\\\n\\\\nNow more than ever, technology allows people to easily learn and invest in markets. Robinhood has made investing straightforward, BlackRock offers an exchange-traded fund (ETF) for nearly everything, fractionalisation removes barriers to entry, and cryptocurrencies present something entirely new and unimaginable just five years ago.\\\\n\\\\n40 / Rise Insights\\\\n\\\\n\\\\n1. Oliver Wyman 2. ManPower Group\\\\n\\\\n\\\\nAl has also offered incredible advances in service delivery. Financial services companies can provide bespoke, individualised experiences and expand their offering of services at scale. Where previously service costs were high, such as when seeking expert advice or acquiring information from a premium news service, they are now lower and the services available to everyone. The unrivalled cost-effectiveness of Al brings benefits to both firms and consumers, and maximises the potential of the retail market.\\\\n\\\\nAI offers an unparalleled opportunity to successfully oversee customers\\' financial wellbeing, empowering their investment choices through personalised guidance, while allowing them to explore new options that truly reflect their values.\\\\n\\\\nFintechs and financial services firms can use AI to cater for every type of customer and utilise real-time data to build meaningful associations on a continuous exchanging cycle. The ability to acquire deeper insights in real-time about customers helps secure greater loyalty and offers unique journeys to a more diverse pool of customers.\\\\n\\\\nPage 21\\\\n---\\\\n\\\\n## The risks of AI in investments\\\\n\\\\n\\\\n## A bright future\\\\n\\\\n\\\\nAlthough new AI technologies offer enormous benefits to consumers and organisations, the latter should be cautious, and adopt a collective culture of accountability and transparency, and a mindset of dependability and best practice. Specifically, there are four areas that require regular review:\\\\n- • Transparency and privacy: data collected from customers must be expressly endorsed by them, with its intended purposes made clear\\\\n- - Segment bias: human oversight must guard against any unintended bias for vulnerable customers or minorities\\\\n\\\\nApplying AI at scale will be a tipping point in the investment sector. By understanding why consumers make the choices they do, financial service firms can expect greater engagement, higher customer lifetime creditworthiness and lower acquisition costs.\\\\n\\\\nThe democratisation of investment is still in its early stages, but AI can offer what every consumer expects nowadays – unique experiences – and what every organisation wants – opportunities for high quality engagement and services. The essential part of the journey with this technology is understanding the implications and preparing accordingly.\\\\n- - Model explainability: the more ethical alternative of sharing simple, high-level processes used by the Al algorithms, rather than unknown black-box reasoning, should be addressed early to avoid data inaccuracy\\\\n- - Ethical standards: besides regulatory compliance, standards should be used that protect investor wellbeing.\\\\n\\\\n## Daniel Stack - -\\\\n\\\\n\\\\nHead of Strategic Partnerships & Growth, AlphaStream\\\\n\\\\ndanieljstack\\\\n\\\\nPlans and risk assessments should also be monitored regularly to identify ambiguities in new (and existing) regulations, and solutions designed that address potentially adverse issues.\\\\n\\\\n43 / rise.barclays\\\\n\\\\n\\\\nPage 22\\\\n---\\\\n\\\\nFocus on CX\\\\n\\\\n\\\\n## Voice assistants\\\\n\\\\n\\\\n## and the customer experience\\\\n\\\\n\\\\nEven though many companies offer customer support in apps or via chatbots, over 60% of our customer service interactions happen over the phone. As a result, many companies have started to invest in voice automation to replace traditional “press 1 for credit cards” Interactive Voice Responses (IVRs) with sophisticated, AI-powered voice assistants.\\\\n\\\\nEnterprises have been exploring voice automation for years, but until very recently, voice solutions have been unreliable, failing to understand callers and effectively contain queries. How do enterprises take advantage of voice Al while staying true to their commitment to providing accessible customer service for everyone?\\\\n\\\\n## The problem\\\\n\\\\n\\\\nAl-powered voice assistants alleviate the pressure on contact centres by fully handling common, repetitive and urgent queries, freeing up call centre agents to focus on complex queries that require empathy and judgement.\\\\n\\\\nBuilding voice assistants has proved a unique challenge for enterprises that have experimented with conversational Al. In digital chat, people tend to type keywords and short sentences, which means there\\'s no room for error when it comes to misinterpreting \\'important\\' details.\\\\n\\\\nProviding support over the phone is much more complex, for several reasons:\\\\n- - Accounting for accents and dialects\\\\n- - Processing calls with background noise, e.g. when people are working from home\\\\n- - Managing people\\'s comfort levels with automated voice assistants\\\\n- - Understanding personal quirks, e.g. wordiness.\\\\n\\\\nSpeech recognition tools tend to work best for the majority of customers only if they have a \\'standard\\' accent. In the UK, that would be those with a Home Counties English accent. A commitment to accessible and inclusive customer service means providing access to every customer, regardless of who they are, how old they are, where they come from and how they speak.\\\\n\\\\nNikola Mrkšić CEO and Co-founder, PolyAI in nikola-mrkšić-80b64528\\\\n\\\\n\\\\n44 / Rise Insights\\\\n\\\\n\\\\n## Case study: BP\\\\n\\\\n\\\\n## PolyAI\\\\n\\\\n\\\\nMultinational oil and gas giant, BP, needed help maintaining contact centres to support thousands of petrol stations whilst sustaining a high standard of customer experience. Call handlers have to handle many complex queries from broken gas pumps to customer credit card issues, expressed in different words as well as different accents. When BP reached out to us, we demonstrated how we could help achieve their goals:\\\\n- 1. We used a proprietary Spoken Language Understanding (SLU) engine that processes audio recordings of sentences rather than detecting very specific keywords like chatbots do. This allows users to speak freely and naturally, using words and phrases of their own choice.\\\\n- 2. We tuned our voice assistant to account for how different people say numbers and letters, including order numbers, phone numbers and postcodes, which are critical pieces of information that need to be captured with 100% accuracy.\\\\n- 3. We enabled our voice assistants to understand non-native names. My name is Nikola Mrkšić, and as you can imagine, it is frequently misunderstood by speech recognition tools. I\\'ve been Nikola Sandwich, Nikola Worksheets and Nikola Ice Cream. We\\'ve put extensive work into creating a phonetic name matching solution that understands names like Nikola Mrkšić and Tsung-Hsien Wen (my fellow co-founder and CTO of PolyAI) as well as it understands John Smith or Sarah Jones.\\\\n\\\\nThe PolyAI voice assistant was able to accurately recognise 97% of customer intents from day one. After tweaking the model, we raised that score to 98.8% in just three weeks, and customers gave us an average satisfaction score of 92%.\\\\n\\\\n## About PolyAI\\\\n\\\\n\\\\nPolyAI builds enterprise voice assistants that carry on natural conversations with customers to solve their problems. Our voice assistants understand customers, regardless of what they say or how they say it.\\\\n\\\\npoly.ai\\\\n\\\\n45 / rise.barclays\\\\n\\\\n\\\\nPage 23\\\\n---\\\\n\\\\nFocus on CX\\\\n\\\\n\\\\n## NLP and the customer experience\\\\n\\\\n\\\\nNatural Language Processing (NLP) is radically enhancing the way companies deliver customer experience (CX). NLP is a branch of Al that helps computers understand, interpret and manipulate human language.\\\\n\\\\nFrom customer service chatbots to conducting sentiment analysis on social media platforms, and even contributing to the development of immersive experiences in autonomous vehicles, NLP continues to shape the customer experience landscape.\\\\n\\\\n## Market and applications\\\\n\\\\n\\\\nThe global market for NLP solutions and services is set to skyrocket to $127.26 billion in 2028 from $20.98 billion in 2021, a CAGR of 29.4% during this timeframe.¹ The worldwide market revenue is due to rise rapidly from $17.5 million in 2021 to $43.2 million in 2025.²\\\\n\\\\nThe applications of NLP in financial services are vast and varied, and include:\\\\n- - Prediction of volatile stock market behaviour to analyse financial documents\\\\n- - Scrutiny of unstructured data to resolve regulatory and compliance issues, and conduct risk assessments to protect against financial malpractices\\\\n- - Sentiment analysis to understand firms\\' reputations in markets\\\\n- - Auditing to improve the accuracy and scalability that aren\\'t possible manually.\\\\n\\\\nThe ability to commercialise data is one of the key reasons behind NLP adoption. Businesses realise that leveraging NLP to deliver improved CX carries significant monetary implications. The financial services sector is one of the biggest adopters of data commercialisation with NLP, and has been at the forefront in using the technology to refine CX and drive positive business outcomes. Healthcare and manufacturing are among other sectors with similar ambitions.\\\\n\\\\nThe current landscape provides an ideal opportunity for next-gen enterprises to shape customer experiences and commercialise data simultaneously. They say the future is here, and as far as applications like NLP are concerned, that certainly holds true today.\\\\n\\\\nNeerav Parekh Founder & CEO, vPhrase\\\\n\\\\n\\\\n46 / Rise Insights\\\\n\\\\n\\\\n1. Fortune Business Insights 2. Statista\\\\n\\\\n\\\\n## Case study: Phrazor\\\\n\\\\n\\\\nvPhrase Analytics implemented their double-patented business intelligence (BI) tool, Phrazor, as a scalable solution to a leading financial services firm operating in 2500+ locations and employing 6000+ professionals. Phrazor added language-based insights to thousands of monthly portfolio analysis reports prepared by the firm for its diversified clientele, which includes both retail and high net-worth clients.\\\\n\\\\nPhrazor auto-generated those insights as personalised recommendations within the language of the reports based on individual customers\\' investment portfolios and goals. This helped them understand their portfolios much better than before, increasing the engagement and hence the number of transactions. With Phrazor, the firm was also able to:\\\\n- - Override the communication barrier by generating portfolio statements in four languages (English and three regional languages)\\\\n- - Reallocate the time of their talented employees from laborious, manual tasks to more important work\\\\n- - Quantify success in various ways, generating over 400,000 personalised client statements, saving an estimated four hours per client previously spent in manual portfolio analysis and report writing.\\\\n\\\\n## About vPhrase Analytics\\\\n\\\\n\\\\nvPhrase Analytics, through its Al-powered BI product Phrazor, lets enterprises create highly personalised language-based reports at scale, at machine speed. In addition, it allows non-technical users to conduct ad-hoc exploration and analyses without the need for technical expertise. Phrazor\\'s auto-generated, language-based insights are also officially available as plugins on Tableau and Power BI.\\\\n\\\\nvphrase.com\\\\n\\\\n47 / rise.barclays\\\\n\\\\n\\\\nPage 24\\\\n---\\\\n\\\\nFocus on CX\\\\n\\\\n\\\\n## AI biometrics and the customer experience\\\\n\\\\n\\\\nWhen employed intelligently, Artificial Intelligence (AI) can bring a never-before-seen change in the way businesses communicate with buyers. Further, human-like tech features (biometrics) are fundamentally fuelling this shift.\\\\n\\\\nTailored and timely access to product and service information is both helping consumers make more informed choices and inspiring organisations to have a better perspective of what their clients want. No brownie points for guessing that people today prefer the omnichannel experience that is consistent and personalised - all at the touch of a button.\\\\n\\\\n## The evolution of data security\\\\n\\\\n\\\\nAccording to a new study, nearly 80% of bank executives agree they need to dramatically re-engineer banking technologies, making way for a more value-based, human-centred experience. Al is the go-to tool for overcoming customer experience (CX) challenges in digital transformation.¹ One reason why is paperwork and passwords are often the problems synonymous with financial institutions. So, why not bring some speed and simplicity into the system with the help of Al?\\\\n\\\\nquietly changing CX in banking. With emotion Al now revolutionising the banking industry, consumer experience has become more personal, effective and even futuristic.\\\\n\\\\nUsing AI to identify and automatically respond to data breaches is critical for protecting data. AI biometrics authentication safeguards against a data breach that is essential for organisations of any scale.\\\\n\\\\nAccording to a report by IBM, ² over 20% of data breaches are due to compromised credentials. And it can take about 287 days to identify a data breach and respond to it. Al-based biometric security\\'s usage has become a necessity to remain competitive in any industry. The report also states that as of 2021, 25% of the organisations have a complete deployment of Al-based security, while 40% of the businesses are partially deployed.\\\\n\\\\nCommunication is no longer limited to spoken words or written texts. Learning facial expressions and understanding body language are being considered more reliable components. Biometrics are the brains behind this rejig. Facial and voice recognition are\\\\n\\\\nRanjan Kumar Founder & CEO, Entropik Tech\\\\n\\\\n\\\\n48 / Rise Insights\\\\n\\\\n\\\\n1. Vernacular.ai 2. IBM\\\\n\\\\n\\\\n## Case study: Affect UX\\\\n\\\\n\\\\n## entropik TECH\\\\n\\\\n\\\\nAffect UX from Entropik Tech is a cloud-based user testing SaaS platform powered by Emotion AI tech. Purpose-built for UX researchers and designers, Affect UX allows you to test websites, apps, and prototypes. With high-precision eye tracking and facial coding, Affect UX gives you a peek into the minds of your users.\\\\n\\\\nWe\\'ve implemented Affect UX in financial services for payment gateways and other large, customer-facing portals.\\\\n\\\\nFor a number of reasons, our clients use Affect UX to implement or understand:\\\\n- • Highly accurate eye tracking: Know with razor-sharp accuracy what users are looking at and for how long they are looking at it\\\\n- - In-built screen recording: Record complete computer vision sessions in real-time for participant observation, eye tracking, and user journey mapping\\\\n- • Dashboards and data exports: Visualise engagement, attention and emotion metrics through eye tracking heatmaps, gaze plots and area-of-interest (AOI) charts\\\\n- - Untapped emotion insights: Understand the emotional triggers and subconscious preferences of your users to deliver experiences they will love and resonate with.\\\\n\\\\n## About Entropik Tech\\\\n\\\\n\\\\nEntropik Tech is the world\\'s leading Emotion AI platform that reads human emotions and helps brands redefine their offerings and experiences. In our mission to humanise experiences, we have built AI technologies that understand human emotion through facial expressions, eye movement, voice tonality and brainwaves - in a fast and scalable manner.\\\\n\\\\nentropiktech.com\\\\n\\\\n49 / rise.barclays\\\\n\\\\n\\\\nPage 25\\\\n---\\\\n\\\\nFocus on CX\\\\n\\\\n\\\\n## Data analytics and the customer experience\\\\n\\\\n\\\\nAl allows us to uncover insights about people at lightning speed, enabling us to deliver meaningful guidance and recommendations in the moments they need it.\\\\n\\\\nIn combination with a robust data strategy, Al can truly help to reshape our understanding of the optimal customer journey and experience. As far as banks are concerned, it helps them shift from mass campaigns to real-time, behaviour-based interactions with customers.\\\\n\\\\n## AI isn\\'t one-size-fits-all\\\\n\\\\n\\\\nIt\\'s important not to put the cart before the horse: Before adopting an Al solution of any kind, you must first identify the business problem you\\'re trying to solve to minimise downstream risks. From there, you can determine which type of Al is required. There are more than 50 different ways to use Al in large enterprises. But that\\'s only the beginning. Further challenges lie in data portability and securely moving data from one system to another.\\\\n\\\\nLook at Spotify. They use algorithms to build a profile and suggest music based on your preferences. They also offer new experiences based on your past choices. Now apply those concepts to a bank: Imagine receiving real-time, relevant recommendations from your bank with offers and rewards that you automatically qualify for. These could be local offers and cashback on items that you already spend money on, sent to you at the exact moment you need them.\\\\n\\\\nSo how can banks achieve that? By connecting the dots with their data. They have access to transactional data, but can also link their proprietary data with many other data sources, such as device data, CRM data, and location data. In doing so, a bank can generate a more holistic view of their customers and create unique marketplaces that link them to relevant third-party providers or even offer wealth-management advice.\\\\n\\\\nBut while many banks may encounter difficulty adopting Al-based models, it\\'s clear that other industries have used Al to reinvent individual customer journeys and experiences. The result is improved loyalty, retention and trust. By following those examples, banks can meet the demand for personalised customer expectations.\\\\n\\\\nHossein Rahnama CEO, Flybits\\\\n\\\\n\\\\n50 / Rise Insights\\\\n\\\\n\\\\n## Case study:\\\\n\\\\n\\\\nA tier-1 North American bank\\\\n\\\\n## Flybits\\\\n\\\\n\\\\nThe bank recognised that their customers\\' expectations were rapidly evolving as tech advancements ushered in intelligent digital solutions that allowed companies to anticipate and meet needs more dynamically. In partnering with Flybits, the bank designed a mobile app with following features:\\\\n\\\\nA digital concierge that proactively provides personalised guidance, services and recommendations to over 6.2 million customers based on their transaction data, location and interests.\\\\n\\\\nValue generated as a result of the deployment:\\\\n- - Less than one minute to deploy a new experience in-market\\\\n- - 30-40% increase in spend on merchant offers\\\\n- - 200% lift in fulfillment rates compared to traditional channels\\\\n- - 15-30% engagement rates on personalised experiences with 50-70% fulfillment\\\\n\\\\n## About Flybits\\\\n\\\\n\\\\nThe deployment of a dozen highly-personalised experiences, providing customers with real-time access to what they need:\\\\n- - Biometrics: enabling facial recognition and fingerprint authentication\\\\n- - OTP Secure: offering enhanced security through two- step verification\\\\n\\\\nFlybits is the leading customer experience platform for the financial services sector, delivering personalisation at scale. With the most advanced capabilities in the market, its enterprise-level solution brings relevant content, products, offers and information to a bank\\'s digital channels based on what each individual customer needs in the moments that matter.\\\\n- - Mobile Remote Deposit Capture: providing a simplified way to deposit checks digitally\\\\n\\\\nflybits.com\\\\n- • eStatements: encouraging customers to enroll in eStatements, eliminating the need for paper statements\\\\n- - Bill pay: offering the ability for customers to pay bills directly through their mobile app\\\\n- - Digital Wallet: enabling the addition of bank-issued credit and debit cards to digital wallets\\\\n- - Contact information: prompting customers to update their information when needed\\\\n\\\\n51 / rise.barclays\\\\n\\\\n\\\\nPage 26\\\\n---\\\\n\\\\n## Ethics and bias in AI\\\\n\\\\n\\\\nWith evolving AI technology, banks have the responsibility to create a more inclusive playing field so that everyone benefits.\\\\n\\\\n52 / Rise Insights\\\\n\\\\n\\\\n53 / rise.barclays\\\\n\\\\n\\\\nPage 27\\\\n---\\\\n\\\\n## Ethics in AI\\\\n\\\\n\\\\nDavid Bholat, Barclays UK Chief Data Scientist, and Michael Payne, Barclays UK Chief Analytical Officer, talk to Grace Batchelor, Rise London Platform Manager, about the imperative in understanding how ethics can make Al a force for good.\\\\n\\\\nGrace Batchelor: Is there anything that particularly excites you about AI and its ethical use in society? And do you see this as becoming a reality in the short, medium or long term?\\\\n\\\\nDavid Bholat: The exciting part for me is that Al prompts us to discuss ethics, full stop. Many of the ethical debates around Al, such as the boundary between humans and non-humans, or what is \\'fair\\', predate Al. These issues have long been discussed in every culture\\'s philosophy, theology, and fiction, and we\\'d be wise to return to those resources. But debates around some of these issues have been dormant in the public sphere for decades. One of Al\\'s positive externalities has been to raise these ethical questions anew, with the prospect that they might now be answered more precisely.\\\\n\\\\nMichael Payne: I am naturally excited by the fact that Al has become mainstream as its power and potential have reached the consciousness of the media and policymakers. What excites me more, though, is the growing recognition that, like the nuclear field (which spawned breakthrough developments in energy and medicine as well as weapons), Al has the power to transform the lives of consumers and society as a whole for the better. But if it\\'s left to its own devices, Al also has the potential to be misused by the few, and ethics is a key tool to avoid this scenario.\\\\n\\\\nGB: Some of the potential impacts on society (say, in credit underwriting) are scary. For example, the ethical and legal ramifications of deep learning when IBM Watson is capable of diagnosing cancer in patients. What if this is wrong and how would this impact a credit decision?\\\\n\\\\nDB: Deep learning describes a set of machine learning models where the relationship between various pieces of information, on the one hand, and an outcome of interest, on the other, is learned by finding the optimal weights to give to those different pieces of information. The weighted information then passes through a series of layers where it is transformed, using mathematical functions to find non-linear patterns. If that sounds unclear, it is because deep learning models often are\\\\! And we naturally fear what we don\\'t fully understand. That\\'s especially true in the contexts you describe. Cancer diagnoses are scary situations. Credit allocation is high- stakes. We understandably fear the wrong decision being made. But I think we should distinguish between the scariness intrinsic to those situations, and the application of Al in them. Credit decisions and cancer diagnoses need to be made. We should be pragmatic, and use the best tools at our disposal. In this context, it\\'s worth noting there are several studies showing that deep learning techniques outperform clinical experts in tumour detection. Equally, some studies show deep learning models outperform simpler statistical models in underwriting.\\\\n\\\\n54 / Rise Insights\\\\n\\\\n\\\\nMP: I think there is also a broader point that is implicit in that question: Al is an arms race and the winners won\\'t be those who just invent new advanced analytical techniques or create innovative uses for those techniques—such as those for diagnosing diseases. Rather it will be those who curate the most extensive datasets (in terms of breadth and depth) that fuel those techniques and new opportunities. For any democracy, where individual freedom is the bedrock of its society, you need to carry the voters with you. Mass acceptance of Al is the essential prerequisite to achieve mass adoption of it. In order to do that, members of the public need to feel developments are in their interest. So I think countries like the US or the UK and the businesses that operate within them face two futures—either you\\'re falling behind or running ahead.\\\\n\\\\nIf we get it wrong, we risk a backlash from policymakers and consumers both directly and indirectly. In terms of the former, by shunning businesses that use Al, or the latter, if consumers refused to share their data and businesses were unable to use it without explicit and express consent for each and every use. If this happens, we risk stalling the momentum of Al developments or, worse, setting it back, at least for those of us operating in democratic regimes. On the other hand, if we get it right, more people will share their data, adopt edge use cases and tilt the norm to early adoption. In doing so, innovation will be accelerated as bigger datasets unlock the opportunities to create even better, more innovative use cases. The difference between these two futures is ethics and that is one of the reasons we, at Barclays, are so passionate about both Al and the ethical framework that underpins our work in this area.\\\\n\\\\nGB: How are banks and other financial services organisations feeling about the prospect of Al, and what are they doing right now to plan for its widespread adoption? What\\'s happening to determine and eliminate bias in Al?\\\\n\\\\nDB: It\\'s a sign of our own anthropocentric bias that we often hold machines to higher standards than we ourselves can evidence. As social science studies have shown, the decisions people make are rife with cognitive, cultural and idiosyncratic biases. One of the promises of Al is that it can help us see beyond our own blinders. Perhaps this is why investment in Al by the financial sector has been resilient through the pandemic. In fact, I think the pandemic has accelerated interest in adopting Al in the financial sector. Digitalisation, or the shift from in-person to online banking necessitated by social distancing measures, increased the opportunity to apply Al. For example, Barclays\\' robo-advisory service, launched during the pandemic, has automated the creation of personalised investment plans. Additionally, labour shortages wrought by the pandemic have engendered the need to automate previously manual processes with Al. Last year, for example, one data scientist in the team I lead reduced the time to output of a calculation by a factor of 10, from 5 hours to 30 minutes each run.\\\\n\\\\nMP: Bias in AI is a key issue we all need to address, but in our view, for any adoption programme to be successful, it needs to be bound by five tenets:\\\\n- - Transparency: A willingness to openly articulate (in an explainable and understandable way) whatever you are doing with your customers\\' data. This acts as a safety valve on activities.\\\\n\\\\n55 / rise.barclays\\\\n\\\\n\\\\nPage 28\\\\n---\\\\n- - A willingness to cede control to customers: This includes giving customers easy ways to control how their data is used and gives them clear rights akin to a Bill of Rights.\\\\n- • Line of sight to who is benefiting: This is about ensuring that there is a clear view of how and which consumers will benefit. This is important for reflecting fairness, inclusiveness and doing no harm.\\\\n- - Providing clear accountability: This is to ensure there is clear algorithmic accountability. This means having checks in place that ensure Al systems are performing reliably and safely (including removing bias), with clear routes to take action both internally and for those outside the business.\\\\n- - A C-level focus: The aim is to foster Board discussions on ethics, especially around edge use cases. The purpose is not about coming to a consensus about the \\'right answers,\\' but about having regular debates in a way that the whole business can see how ethics is at the core of decision making.\\\\n\\\\nGB: What responsibilities will banks have and how are these being established? Are conversations taking place with stakeholders in and outside organisations?\\\\n\\\\nDB: There are numerous frameworks aiming to ensure ethical AI. The Monetary Authority of Singapore has been a real thought leader here, articulating a few years ago its FEAT principles (Fairness, Ethics, Transparency, and Accountability). While those principles were meant to apply specifically to the use of AI in Singapore\\'s financial sector, the general themes of fairness, transparency and accountability are mirrored in codes of best practice across various jurisdictions, including the UK. With respect to fairness, ensuring models do not unjustly discriminate against\\\\n\\\\npeople is fundamental. A basic touchstone is that model errors are not greater for one subgroup than another. Regarding transparency, the objective is to make Al-driven decisions understandable to those affected by them. And regarding accountability, the objective is to prevent fully autonomous Al, where humans are \\'in the loop\\' and ultimately accountable. But leaving the details of different frameworks aside, the key point to stress is that the Al we\\'re developing is embedded within the overarching purpose of deploying finance that responsibly supports people and businesses, for the common good and the long term.\\\\n\\\\nGB: The black box question. How quickly can a predictive model or Al become so complex that humans will be unable to determine the reasons for an incorrect decision made by a machine? And what technology are companies using to address this issue?\\\\n\\\\nDB: We can assess the complexity of a decision-making process by how hard it is to go back and replicate the steps that resulted in it, and how intuitive the reasoning is behind them. It\\'s again instructive to compare artificial and human intelligences along these dimensions. In terms of replication, Al is created with code stored in version control systems, which makes it easy to audit and reproduce. By contrast, our faulty memories often make humans unreliable witnesses to past decisions. Furthermore, the biases I noted earlier mean we are often strangers even to ourselves in knowing the deeper reasons for our choices. However, if we take at face value the reasons people give for their decisions in natural language, these are easier to understand than, say, the decision points in deep learning models coded in a programming language. True, there are so-called \\'explainable Al\\' techniques. For example, Shapley values can help explain the relative importance of variables in complex models.\\\\n\\\\n56 / Rise Insights\\\\n\\\\n\\\\nWe might then use a regression to determine which Shapley values are statistically significant in a decision. But this correlation still wouldn\\'t prove causation, or provide an intuitive reason for a deep learning model\\'s decision.\\\\n\\\\n## GB: Moving on to data privacy, which privacy concerns do you see as most important?\\\\n\\\\n\\\\nDB: I think it\\'s difficult to rank which issues under the umbrella of data privacy are most important. They all are\\\\! But what I think is often overlooked are the ways ML can alleviate privacy concerns rather than amplify them. For example, a type of deep learning, called Generative Adversarial Networks (GANs), can create synthetic versions of datasets that retain the properties of the original data while shedding sensitive aspects. In this way, GANs can facilitate safer data sharing. Similarly, federated learning offers a paradigm for building better models across organisations without data being transferred between them. Essentially, what happens in federated learning is that scientific teams at different organisations develop the initial parameters for an algorithm. The algorithm is then distributed to each organisation to train on their own data. Those local models are then combined to create an aggregated one, which then gets shared equally among all the organisations. Thus, everyone benefits from having a model trained on more data than any one of them would have had alone—without the risks entailed in sharing the actual data between them.\\\\n\\\\nMP: David is spot on. They are all important and the key prerequisite is transparency. Transparency on what you are doing, transparency on how this benefits society and the individual and transparency (and ease) with which one can opt out. It goes back to the five tenets I talked about earlier.\\\\n\\\\nGB: Looking to the future, how will the impacts of Al and data commercialisation shape the bank of the future?\\\\n\\\\nMP: I think AI will reset the way banks support their customers. For example, it will give back customers\\' data, but in way that transforms it into tools that can help them make smarter decisions that make their lives easier and better. Looking further, this could be small changes in their spending that unlocks opportunities to buy their first home or how, by changing a provider of a service such as utilities, they can free up cash to put aside to save for a rainy day. Returning to what I said at the beginning: AI has the power to transform the lives of consumers and society as a whole for the better. Who wouldn\\'t be excited about this?\\\\n\\\\nDavid Bholat Chief Data Scientist, Barclays UK\\\\n\\\\n- 🐦 @DavidBholat\\\\n\\\\nMichael Payne MD, Data & Analytics, Barclays UK\\\\n\\\\n\\\\nGrace Batchelor Rise London Fintech Platform Manager\\\\n\\\\nin grace-batchelor-a5796591\\\\n\\\\n57 / rise.barclays\\\\n\\\\n\\\\nPage 29\\\\n---\\\\n\\\\n## Increasing trust when deploying AI systems\\\\n\\\\n\\\\nBusiness reliance on algorithms is becoming ubiquitous, and companies are increasingly concerned about their algorithms causing major financial or reputational damage.\\\\n\\\\nHigh-profile cases include Knight Capital\\'s bankruptcy¹ by a glitch in its algorithmic trading system, and Amazon\\'s Al recruiting tool being scrapped² after showing bias against women. In response, governments are legislating and imposing bans, regulators are fining companies and the judiciaries are discussing the idea of making algorithms artificial \\\\\"persons\\\\\" in law.\\\\n\\\\nSoon, there will be billions of algorithms making everyday decisions with minimal human intervention, from autonomous\\\\n\\\\nvehicles and finance to medical treatment, employment and legal considerations. We are entering the \\'Big Algo\\' age (an analogy to the Big Data period) where aspects such as safety, ethics and legality will become crucial considerations when developing, deploying and using intelligent algorithms.³\\\\n\\\\nSo how do we ensure Big Algo is an opportunity and not a threat? Different communities have come up with approaches to risk-assess and manage the growing threat that Al systems can pose:\\\\n\\\\n| Community | Focus | Solutions |\\\\n| ----- | ----- | ----- |\\\\n| AI ethics | Aspects of fairness, accountability and transparency | Establishing an organisational-level AI ethics board (e.g. IBM) |\\\\n| AI governance | Legal, privacy and management risks | Working with regulators and legislators on a regulatory framework (e.g. AI Public-Private Forum or Global Partnership on AI) |\\\\n| AI safety | Short-term focus on robustness and security, and long-term emphasis on preventing existential risks4 | Setting up best practices and standards for applications (e.g. IEEE Ethically Aligned Design or ISO/IEC directives) |\\\\n\\\\n\\\\n58 / Rise Insights\\\\n\\\\n\\\\n1. Reuters 2012 2. Reuters 2018 3. Social Science Research Network 4. Wikipedia\\\\n\\\\n\\\\n## AI risk in financial services\\\\n\\\\n\\\\nIn general, the key technical risks of AI applications cover:\\\\n- - Performance and robustness: systems should be safe and secure, not vulnerable to tampering or compromising of the data they are trained on\\\\n- - Bias and discrimination: systems should avoid unfair treatment of individuals or groups\\\\n- - Interpretability and explainability: systems should provide decisions or suggestions that can be understood by their users, developers and regulators\\\\n- - Algorithm privacy: systems should be trained using data minimisation principles and adopt privacy-enhancing techniques to mitigate personal or critical data leakage.\\\\n\\\\nOf these, the most high-profile concerns are bias and explainability. The public are acutely aware that algorithmic systems may reflect historical bias and may even make them worse. And they expect the systems\\' recommendations or outputs to be easily explainable, particularly where they concern people.\\\\n- • Recruitment: HR departments in banks and other financial services companies will be under pressure to report on the use of Al in talent management.\\\\n\\\\nWhat are the risks of getting it wrong? The impact of failure depends on the size of the organisation. Prominent examples include Amazon\\'s Al recruiting tool mentioned earlier or South Wales Police\\'s facial recognition technology being ruled unlawful. Managing the risks of Al is like managing those of cybersecurity. Without good management of both, businesses might even become unviable.\\\\n\\\\nLarge institutions in particular can expect most scrutiny from regulators and society. Negative impacts could include restricting the use of certain Al applications, being fined for misuses and lack of governance, and changing procurement processes to include risk management of Al. Company boards and shareholders may well seek to mitigate these risks by promoting what were previously data science issues to the C-suite level, in particular CTOs, CIOs, and CDOs. And social media coverage can leave negative impressions even if the Al is not to blame.\\\\n\\\\nFinancial services regulators have identified the following as of most concern:\\\\n- - Credit scoring: specifically bias, discrimination and explainability aspects\\\\n- - Trading systems: specifically, robustness and \\'herding\\' (e.g. algorithms moving at a similar direction in their intentions of buying/selling products)\\\\n- • Insurance: specifically bias discrimination and explainability aspects though scrutiny in this area is in its early days\\\\n\\\\nEmre Kazim Co-founder and COO, Holistic AI\\\\n\\\\n\\\\nholisticai.com\\\\n\\\\nAdriano Soares Koshiyama Co-founder and CEO, Holistic AI\\\\n\\\\nholisticai.com\\\\n\\\\n5. FCA 6. Alan Turing Institute 7. BBC\\\\n\\\\n\\\\n59 / rise.barclays\\\\n\\\\n\\\\nPage 30\\\\n---\\\\n\\\\n## From our Rise sites\\\\n\\\\n\\\\nrise london Created by BARCLAYS\\\\n\\\\n\\\\nrise india Created by BARCLAYS\\\\n\\\\n\\\\n## Rise London\\\\n\\\\n\\\\nWhat a year 2021 was.\\\\n\\\\nThe UK fintech sector raised an incredible £8.6 billion – nearly three times the amount raised in 2020,¹ and maintained its status as the lead in Europe for fintech funding. We\\'re delighted this success was felt within Rise London with a number of raises by members throughout 2021, the largest being an incredible $118 million Series B by spend management startup Spendesk.\\\\n\\\\nBut we also need to draw attention to the work that\\'s still required in the sector. Janine Hirt, Innovate Finance CEO, highlights that \\\\\"it\\'s time to also properly address the funding gap for underrepresented founders, if we are to create a truly sustainable and forward-looking sector\\\\\". 2 In the UK, female fintech founders only received 9% of all capital invested in 2021, down from 13% in 2019.3 And, although scaleup funding is hitting new highs in the UK, early-stage startup funding is yet to return to pre-pandemic levels.4\\\\n\\\\nWith this very much in mind, Barclays announced the launch of the Female Innovators Lab in the UK and Europe last October.⁵ The Lab combines Anthemis\\' investment expertise and the Rise, created by Barclays, fintech network to identify early-stage female founders and match them with the capital and support to scale their companies.\\\\n\\\\nAnother trend we\\'ve seen, exacerbated by the pandemic, is the rush to digitisation in financial services. The UK has one of the highest fintech adoption rates in the world at 71%, compared with the global average of 64%.⁶ With this\\\\n\\\\nincreased popularity comes increased expectations and demands from consumers, and Bill Gates\\' 1994 quote \\'Banking is necessary, banks are not\\' becomes more pertinent. We see this as an opportunity for the fintech sector to innovate financial services, and change the way consumers engage with their finances.\\\\n\\\\nAl and data commercialisation has an important role to play in this innovation - whether it\\'s advancements in CX, the democratisation of wealth investment services or the added security provided through the advances in the fight against FinCrime. These technologies have the potential to touch all areas of our industry. With efficiency at its core, data and Al can empower banks to focus on what really matters when delivering financial services of the future.\\\\n\\\\nAt Rise, we\\'re excited to play our part in this digital revolution, and we can\\'t wait to continue the growth of a diverse and vibrant fintech ecosystem in the UK.\\\\n\\\\nAfter all, as Stuart Harrison, Director of Fintech West, puts it: \\\\\"Fintech is no longer a sub-sector of financial services, it is financial services.\\\\\"8\\\\n\\\\n## Grace Batchelor\\\\n\\\\n\\\\nRise London Fintech Platform Manager\\\\n- in grace-batchelor-a5796591\\\\n\\\\n| | | |\\\\n| ----- | ----- | ----- |\\\\n| 1. Computer Weekly | 5. Barclays | |\\\\n| 2. AltFi | 6. The Global City | |\\\\n| 3. Markets Media | 7. Tech Wire Asia | |\\\\n| 4. Business Leader | 8. Business Leader | 61 / rise.barclays |\\\\n\\\\n\\\\nPage 31\\\\n---\\\\n\\\\n## Rise India\\\\n\\\\n\\\\nAccording to data by Tracxn, Indian fintechs raised a record $9 billion in 2021,¹ making India the world\\'s third-largest fintech ecosystem.\\\\n\\\\nWe saw the emergence of new trends across sectors – including payments, digital lending, Point of Sales (PoS) financing (also known as Buy Now Pay Later), neobanking, stockbroking and crypto. The country now boasts 18 fintech unicorns out of 187 globally.²\\\\n\\\\nRise India alumni marked their presence in these trends. PoS financing startup ZestMoney raised $50 million in funding, Flexmoney raised $4.5 million, Crediwatch was selected to be part of the AlFinTech100 (an annual list of 100 of the world\\'s most innovative Al solution providers for financial services), MonetaGo\\'s Secure Financing solution received the award for best solution in trade finance during Hong Kong Fintech Week, Winvesta enabled a Unified Payments Interface for global investments, and Indian multinational ridesharing giant Ola acquired GeoSpoc to build the next generation of location technology.\\\\n\\\\nIn Q4 2021, Rise India continued with our key engagements both internally and externally. Our unique VP Internship Programme turned out to be a great success with more than 30 Barclays employees working for 20 fintechs over a six-month period to understand and embrace how fintechs work and leverage emerging technologies to solve business problems. To enhance financial awareness, we organised multiple boot camps with LXME (India\\'s first financial platform for women), and saw a turnout of around 200 people from the Rise ecosystem and their spouses.\\\\n\\\\nIn the last decade, the Indian Government\\'s Digital India campaign has accelerated the country\\'s transition towards a data economy. With large sections of the population embracing digital payments, e-commerce and digital banking services, there is now a generation with vast amounts of personal data. Fintechs have been successfully leveraging this digital footprint to explore how data can be used to reach unserved segments and open up new revenue streams.\\\\n\\\\nRise Fintech Company of the Year, CreditEnable, is helping SMEs to access formal and affordable funding through their proprietary data algorithms. And companies like Rise India alumnus Entropik, a leading Emotion Al company that was named leader in Gartner\\'s Emotion Al report, are helping corporates to improve customer journeys, customer acquisition and conversion rates by deploying this technology in clients\\' digital channels. Account Aggregator, another initiative by the Government, is likely to shape the future of data sharing in India by bringing together financial institutions, technology service providers and certifiers. As a variety of businesses are looking to deliver great value to their consumers, advances in data collection, processing and distribution are going to significantly boost data commercialisation efforts in 2022.\\\\n\\\\nLincy Therattil Head of Rise India\\\\n\\\\n\\\\nin lincy-therattil-9010157 @LTherattil\\\\n\\\\n62 / Rise Insights\\\\n\\\\n\\\\n1. The Times of India\\\\n\\\\n\\\\n2. India Briefing\\\\n\\\\n\\\\n## Rise New York\\\\n\\\\n\\\\nCheers to a new year from Rise New York\\\\!\\\\n\\\\nLast year went out with a bang as we hosted a series of events on the back of the Rise Insights: Decrypting Crypto report. The talk of the city has been Web3 and the future of money. Fintech continued to garner the most interest and investment from top-performing VC firms¹ and Rise remains well positioned to support that growth this year.\\\\n\\\\nAccording to Forbes, enterprises adopting AI focused strategic goals jumped to 59% in 2020, and financial firms are driving that trend with 71% of them adopting big data applications into their business model.⁶ Big data, analytics and AI continue to encourage earlier fintech adoption in industries from music to sports to gaming to healthcare.\\\\n\\\\nIf Web3 is the next frontier of the internet, Al is a key ingredient making it possible. In 2021, corporate retailer Amazon aggressively invested in their fintech infrastructure to support growth through the pandemic. Using existing customer data to make customer journey more seamless at payment also increased Amazon\\'s average transaction size, signaling the importance of embedded digital wallet considerations for both retailers and their networks². For startups, leading VC firm Andreessen Horowitz made enterprise tech the largest industry by invested dollars in 2021 with their focus on IT, cloud, cybersecurity, developer and other software solution companies.3\\\\n\\\\nIn the US, we await a regulatory posture on broad acceptable uses of commercialised consumer data, to support requirements for increased minimum security and privacy management measures.⁴ Data sharing mandates have been one proposed solution, but re-identification concerns have been raised by both corporate and governmental entities during early negotiations.⁵\\\\n\\\\nFintech applications also help elevate consumers who are traditionally underserved in markets like housing, by using their data to paint a fuller picture of who they are. As applications of big data in fintech continue to be a focus, cyber security, privacy protections and technological bias are areas in 2022 that will continue to push the adoption of these new technologies in an equitable way.\\\\n\\\\nAlready this year, Rise launched Rise Connect, our digital platform. This opportunity to scale our expert network in support of fintech founders, with a global digital presence, is endlessly exciting\\\\! Rise is proud to be the world\\'s only fintech-exclusive, hybrid-first, global founder community. Come join us\\\\!\\\\n\\\\nBrian Luciani Fintech Platform Lead, Rise New York\\\\n\\\\n\\\\nin brianluciani\\\\n\\\\n\\\\n| Column 1 | Column 2 |\\\\n| ----- | ----- |\\\\n| 1. PitchBook | 4. MarketWatch |\\\\n| 2. CB Insights | 5. EFF |\\\\n| 3. CB Insights | 6. Built In |\\\\n\\\\n\\\\n63 / rise.barclays\\\\n\\\\n\\\\nPage 32\\\\n---\\\\n\\\\n## Rise Academies\\\\n\\\\n\\\\nRise has announced a new three-year partnership with world-leading venture-builder, Rainmaking. With their support, we\\'re launching a new suite of digital programmes to support fintech startups across the globe.\\\\n\\\\nOur programmes. Your opportunities\\\\n\\\\nRise Start-Up Academy\\\\n\\\\nBuild your fintech in 20 weeks Our programme to empower founders who are rethinking the traditional approach to finance\\\\n- Rise Growth Academy For businesses ready to scale Programme opening later this year\\\\n\\\\n64 / Rise Insights\\\\n\\\\n\\\\nOur support. Your development\\\\n\\\\nLearn from anywhere\\\\n\\\\n24/7 access to your curriculum with weekly modules and bite size videos on our dedicated learning platform\\\\n\\\\nLearn by doing\\\\n\\\\nApply your learnings through weekly activities and live workshops with support from a dedicated community manager\\\\n\\\\nLearn as a community\\\\n\\\\nBe part of a leading fintech community where you can share knowledge and grow together\\\\n\\\\nFind out more at rise.barclays\\\\n\\\\n## Join Rise Connect\\\\n\\\\n- - Rise Connect is a global, virtual network of fintech leaders shaping the future of financial services, with support from the Barclays network and key industry partners\\\\n- - • Access exclusive content, peer-to-peer networking, events and opportunities to help you succeed\\\\n\\\\nLearn more about the benefits of membership at rise.barclays\\\\n\\\\nrise Created by BARCLAYS\\\\n\\\\n65 / rise.barclays\\\\n\\\\n\\\\nPage 33\\\\n---\\\\n\\\\n## Rise global network\\\\n\\\\n\\\\nBlockchain and Crypto\\\\n\\\\n\\\\nEcosystem Partner\\\\n\\\\n\\\\nThis infographic shows companies resident at our Rise locations. The information is accurate at the time of publication.\\\\n\\\\n\\\\n## rise Created by BARCLAYS\\\\n\\\\n\\\\n66 / Rise Insights\\\\n\\\\n\\\\n67 / rise.barclays\\\\n\\\\n\\\\nPage 34\\\\n---\\\\n\\\\n## rise Created by BARCLAYS\\\\n\\\\n\\\\n## About Rise, created by Barclays\\\\n\\\\n\\\\nRise, created by Barclays, is a global community of the world\\'s top innovators and entrepreneurs working together to create the future of financial services. By connecting technology, talent and trends, the mission of Rise is to accelerate innovation and growth in the financial services industry.\\\\n\\\\nTo join our community, or keep in touch with the latest from Rise, visit or follow us on:\\\\n\\\\nrise.barclays\\\\n\\\\n🐦 @ThinkRiseGlobal\\\\n\\\\nRise Fintech Podcast\\\\n\\\\nin Rise, created by Barclays\\\\n\\\\n\\\\#HomeofFinTech\\\\n\\\\nAll information contained herein shall only be used by the recipient for his/her own personal reference. This document is intended for general information purposes only. Barclays makes no representations, warranties or guarantees, whether express or implied, that the content contained herein is accurate, complete, or up to date, and accepts no liability for its use. The views expressed herein are those of the authors alone and do not necessarily reflect those of Barclays, its affiliates, officers, or employees. No part of this communication is intended to constitute business, investment, financial, bidding, property, or legal advice and should not be relied upon by any party as a substitute for professional advice. Readers are strongly encouraged to make their own inquires and to seek independent professional advice before making any buying, selling or business decision. Any third-party entities described in this document are not endorsed or sponsored by Barclays or its affiliates. The information in this document has not been independently verified by Barclays, and Barclays does not assume any liability associated with its use. © Barclays 2022\\\\n\\\\n\\\\nPage 35\\\\n\"']\n"
     ]
    }
   ],
   "source": [
    "result = await query.generate_sample(scenario=scenarios[1])\n",
    "print(result.user_input)\n",
    "print(result.reference)\n",
    "print(result.reference_contexts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b72b1e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenario: How has the Open Banking model influenced the commercialisation of data in financial services, and what opportunities does it present for startups?\n",
      "--------------------------------------------------------------------------------\n",
      "Scenario: How does the concept of trust in AI and data sharing influence the relationship between consumers and financial institutions?\n",
      "--------------------------------------------------------------------------------\n",
      "Scenario: How can increasing trust in AI and data sharing enhance consumer relationships in financial services?\n",
      "--------------------------------------------------------------------------------\n",
      "Scenario: What role does the Open Banking model play in enhancing user control and consent over personal data in financial services?\n",
      "--------------------------------------------------------------------------------\n",
      "Scenario: How do artificial intelligence technologies enhance the customer experience in financial services, and what role does data commercialisation play in this transformation?\n",
      "--------------------------------------------------------------------------------\n",
      "Scenario: How does the Open Banking policy facilitate the use of AI in providing credit to younger generations, and what innovations have emerged from this integration?\n",
      "--------------------------------------------------------------------------------\n",
      "Scenario: How has the Open Banking policy influenced the development of new financial products for younger generations, particularly in terms of credit access?\n",
      "--------------------------------------------------------------------------------\n",
      "Scenario: How does the Open Banking policy facilitate the development of innovative financial products for younger generations?\n",
      "--------------------------------------------------------------------------------\n",
      "Scenario: How has the Open Banking policy influenced the development of fintech products that utilize AI for credit provisioning?\n",
      "--------------------------------------------------------------------------------\n",
      "Scenario: How has the Open Banking policy facilitated the development of innovative financial products and services, particularly in relation to AI's role in assessing creditworthiness for younger generations?\n",
      "--------------------------------------------------------------------------------\n",
      "Scenario: In what ways can controlled data sharing enhance user control and consent in the context of the data revolution and the evolution of financial services?\n",
      "--------------------------------------------------------------------------------\n",
      "Scenario: How did the Open Banking model lead to innovation in financial services and what was the estimated revenue opportunity by 2021?\n",
      "--------------------------------------------------------------------------------\n",
      "Scenario: How did the Open Banking model contribute to the data revolution and what revenue opportunities did it create?\n",
      "--------------------------------------------------------------------------------\n",
      "Scenario: What are some AI use cases in finance that have emerged from the data revolution and how do they contribute to improved credit decisions for the unbanked and Gen-Z?\n",
      "--------------------------------------------------------------------------------\n",
      "Scenario: What are the implications of controlled data sharing for consumers and companies, and how does this relate to the evolution of data in financial services?\n",
      "--------------------------------------------------------------------------------\n",
      "Scenario: How does AI enhance credit scoring and provisioning for younger generations in the context of Open Banking?\n",
      "--------------------------------------------------------------------------------\n",
      "Scenario: How does the integration of AI in credit scoring and provisioning enhance access to credit for younger generations, particularly in the context of Open Banking?\n",
      "--------------------------------------------------------------------------------\n",
      "Scenario: How does AI impact credit scoring and provisioning for younger generations in the context of Open Banking?\n",
      "--------------------------------------------------------------------------------\n",
      "Scenario: How does AI impact credit scoring and provisioning for younger generations in banking?\n",
      "--------------------------------------------------------------------------------\n",
      "Scenario: How does AI improve credit scoring and provisioning for younger generations in the context of Open Banking?\n",
      "--------------------------------------------------------------------------------\n",
      "Scenario: How does Open Banking improve credit decisions for the unbanked and what role do Open Banking APIs play in this process?\n",
      "--------------------------------------------------------------------------------\n",
      "Scenario: How do Open Banking APIs enhance financial inclusion and what role does AI play in this process?\n",
      "--------------------------------------------------------------------------------\n",
      "Scenario: How has Open Banking, particularly through the use of Open Banking APIs, improved credit decisions for previously credit invisible individuals, and what implications does this have for the financial services industry?\n",
      "--------------------------------------------------------------------------------\n",
      "Scenario: How do Open Banking APIs enhance financial inclusion for credit invisible individuals, and what role does AI play in this process?\n",
      "--------------------------------------------------------------------------------\n",
      "Scenario: How does Open Banking improve credit decisions for the unbanked and what role do Open Banking APIs play in this process?\n",
      "--------------------------------------------------------------------------------\n",
      "Scenario: How did the Open Banking model and policy contribute to the innovation in fintech, particularly in leveraging under-utilised data?\n",
      "--------------------------------------------------------------------------------\n",
      "Scenario: How did the Open Banking model and policy contribute to the innovation in financial services and the creation of unicorn fintechs?\n",
      "--------------------------------------------------------------------------------\n",
      "Scenario: How did the Open Banking model and policy contribute to the innovation in financial services and the creation of unicorn fintechs?\n",
      "--------------------------------------------------------------------------------\n",
      "Scenario: How did the Open Banking model, initiated by the European Union and UK Government, lead to the creation of new revenue streams for fintechs and banks, and what role did the Open Banking policy play in this transformation?\n",
      "--------------------------------------------------------------------------------\n",
      "Scenario: How did the Open Banking model, established by the EU and UK Government, lead to innovation in financial services and what role did the Open Banking policy play in this transformation?\n",
      "--------------------------------------------------------------------------------\n",
      "Scenario: How do financial institutions leverage AI and Open Banking to enhance financial inclusion for credit invisible individuals?\n",
      "--------------------------------------------------------------------------------\n",
      "Scenario: How has the Open Banking policy contributed to the creation of innovative financial products and improved credit decisions for previously ineligible individuals?\n",
      "--------------------------------------------------------------------------------\n",
      "Scenario: How does Open Banking improve credit decisions for the unbanked and what role does AI play in this process?\n",
      "--------------------------------------------------------------------------------\n",
      "Scenario: How are financial institutions leveraging AI and Open Banking to enhance financial inclusion for credit invisible individuals?\n",
      "--------------------------------------------------------------------------------\n",
      "Scenario: How does Open Banking improve financial inclusion for credit invisible individuals and what opportunities does it create for financial institutions?\n",
      "--------------------------------------------------------------------------------\n",
      "Scenario: How does the Open Banking policy facilitate the creation of unicorns in the fintech sector, and what role does data commercialisation play in this process?\n",
      "--------------------------------------------------------------------------------\n",
      "Scenario: How has the Open Banking policy influenced the creation of unicorns in the fintech sector and what role does data commercialisation play in this process?\n",
      "--------------------------------------------------------------------------------\n",
      "Scenario: How does trust in AI relate to the trust in banking, especially in the context of data commercialisation and the role of fintechs?\n",
      "--------------------------------------------------------------------------------\n",
      "Scenario: How has the Open Banking policy influenced the creation of unicorn fintechs and the commercialisation of data in the financial services sector?\n",
      "--------------------------------------------------------------------------------\n",
      "Scenario: How can banks increase trust in AI while also ensuring trust in banking, particularly in the context of data commercialisation and the evolving relationship with customer data?\n",
      "--------------------------------------------------------------------------------\n",
      "Scenario: How does Machine Learning (ML) enhance predictive models in financial services, particularly in combating financial crime, and what challenges do banks face in implementing these models?\n",
      "--------------------------------------------------------------------------------\n",
      "Scenario: How does Machine Learning (ML) enhance predictive models in financial services, particularly in combating financial crime, and what challenges do banks face in implementing these models?\n",
      "--------------------------------------------------------------------------------\n",
      "Scenario: How does Machine Learning (ML) enhance predictive models in financial services, particularly in addressing financial crime risks?\n",
      "--------------------------------------------------------------------------------\n",
      "Scenario: How does the implementation of Machine Learning (ML) in financial services enhance predictive analysis and improve data-driven decision-making, especially in combating financial crime?\n",
      "--------------------------------------------------------------------------------\n",
      "Scenario: How does Machine Learning (ML) enhance predictive models in finance, particularly in identifying financial crime risks, and what challenges do banks face in implementing these models?\n",
      "--------------------------------------------------------------------------------\n",
      "Scenario: How does the Open Banking model enhance user control and consent in data sharing, and what implications does this have for financial services?\n",
      "--------------------------------------------------------------------------------\n",
      "Scenario: How does the Open Banking model enhance user control and consent in data sharing?\n",
      "--------------------------------------------------------------------------------\n",
      "Scenario: How does the Open Banking model enhance user control and consent in financial services?\n",
      "--------------------------------------------------------------------------------\n",
      "Scenario: How has the Open Banking model influenced the commercialisation of data in financial services, and what opportunities does it present for fintechs?\n",
      "--------------------------------------------------------------------------------\n",
      "Scenario: How has the Open Banking model influenced the commercialisation of data in financial services, and what opportunities does it present for fintechs?\n",
      "--------------------------------------------------------------------------------\n",
      "Scenario: How did the Open Banking model and policy contribute to the innovation in financial services and the creation of unicorn fintechs?\n",
      "--------------------------------------------------------------------------------\n",
      "Scenario: How did the Open Banking model, initiated by the European Union and UK Government, lead to the creation of new revenue streams for banks and fintechs, and what role does the Open Banking policy play in this transformation?\n",
      "--------------------------------------------------------------------------------\n",
      "Scenario: How did the Open Banking model and policy contribute to the innovation in financial services and the creation of new revenue streams?\n",
      "--------------------------------------------------------------------------------\n",
      "Scenario: How did the Open Banking model and policy contribute to the innovation in financial services and the creation of unicorns in the fintech sector?\n",
      "--------------------------------------------------------------------------------\n",
      "Scenario: How did the Open Banking model and policy contribute to the innovation in financial services and the creation of unicorn fintechs?\n",
      "--------------------------------------------------------------------------------\n",
      "Scenario: How has the Open Banking model contributed to improved credit decisions for the unbanked and Gen-Z, and what role does AI play in this process?\n",
      "--------------------------------------------------------------------------------\n",
      "Scenario: How does the Open Banking model enhance credit decisions for individuals with thin or no credit files?\n",
      "--------------------------------------------------------------------------------\n",
      "Scenario: How does the Open Banking model enhance financial inclusion and what role does AI play in this process?\n",
      "--------------------------------------------------------------------------------\n",
      "Scenario: How does the Open Banking model enhance financial inclusion for credit invisible individuals by utilizing AI technologies?\n",
      "--------------------------------------------------------------------------------\n",
      "Scenario: How has the Open Banking model contributed to improved credit decisions for the unbanked and Gen-Z, and what role does AI play in this process?\n",
      "--------------------------------------------------------------------------------\n",
      "Scenario: How can banks leverage AI to meet personalised customer expectations and provide personalised recommendations based on customer data?\n",
      "--------------------------------------------------------------------------------\n",
      "Scenario: How can banks leverage AI to meet personalised customer expectations and provide personalised recommendations, as demonstrated by the case studies of Flybits and vPhrase?\n",
      "--------------------------------------------------------------------------------\n",
      "Scenario: How can banks leverage AI to meet personalised customer expectations and provide personalised recommendations based on customer data?\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCancelledError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m scenario \u001b[38;5;129;01min\u001b[39;00m scenarios:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m query.generate_sample(scenario=scenario)\n\u001b[32m      3\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mScenario: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult.user_input\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m80\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sources/syftr/.venv/lib/python3.12/site-packages/ragas/testset/synthesizers/base.py:121\u001b[39m, in \u001b[36mBaseSynthesizer.generate_sample\u001b[39m\u001b[34m(self, scenario, callbacks)\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;66;03m# new group for Sample Generation\u001b[39;00m\n\u001b[32m    116\u001b[39m sample_generation_rm, sample_generation_grp = new_group(\n\u001b[32m    117\u001b[39m     name=\u001b[38;5;28mself\u001b[39m.name,\n\u001b[32m    118\u001b[39m     inputs={\u001b[33m\"\u001b[39m\u001b[33mscenario\u001b[39m\u001b[33m\"\u001b[39m: scenario},\n\u001b[32m    119\u001b[39m     callbacks=callbacks,\n\u001b[32m    120\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m sample = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._generate_sample(scenario, sample_generation_grp)\n\u001b[32m    122\u001b[39m sample_generation_rm.on_chain_end(outputs={\u001b[33m\"\u001b[39m\u001b[33msample\u001b[39m\u001b[33m\"\u001b[39m: sample})\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m sample\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sources/syftr/.venv/lib/python3.12/site-packages/ragas/testset/synthesizers/multi_hop/base.py:173\u001b[39m, in \u001b[36mMultiHopQuerySynthesizer._generate_sample\u001b[39m\u001b[34m(self, scenario, callbacks)\u001b[39m\n\u001b[32m    165\u001b[39m reference_context = \u001b[38;5;28mself\u001b[39m.make_contexts(scenario)\n\u001b[32m    166\u001b[39m prompt_input = QueryConditions(\n\u001b[32m    167\u001b[39m     persona=scenario.persona,\n\u001b[32m    168\u001b[39m     themes=scenario.combinations,\n\u001b[32m   (...)\u001b[39m\u001b[32m    171\u001b[39m     query_style=scenario.style.name,\n\u001b[32m    172\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m173\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.generate_query_reference_prompt.generate(\n\u001b[32m    174\u001b[39m     data=prompt_input, llm=\u001b[38;5;28mself\u001b[39m.llm, callbacks=callbacks\n\u001b[32m    175\u001b[39m )\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m SingleTurnSample(\n\u001b[32m    177\u001b[39m     user_input=response.query,\n\u001b[32m    178\u001b[39m     reference=response.answer,\n\u001b[32m    179\u001b[39m     reference_contexts=reference_context,\n\u001b[32m    180\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sources/syftr/.venv/lib/python3.12/site-packages/ragas/prompt/pydantic_prompt.py:129\u001b[39m, in \u001b[36mPydanticPrompt.generate\u001b[39m\u001b[34m(self, llm, data, temperature, stop, callbacks, retries_left)\u001b[39m\n\u001b[32m    126\u001b[39m callbacks = callbacks \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[32m    128\u001b[39m \u001b[38;5;66;03m# this is just a special case of generate_multiple\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m output_single = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.generate_multiple(\n\u001b[32m    130\u001b[39m     llm=llm,\n\u001b[32m    131\u001b[39m     data=data,\n\u001b[32m    132\u001b[39m     n=\u001b[32m1\u001b[39m,\n\u001b[32m    133\u001b[39m     temperature=temperature,\n\u001b[32m    134\u001b[39m     stop=stop,\n\u001b[32m    135\u001b[39m     callbacks=callbacks,\n\u001b[32m    136\u001b[39m     retries_left=retries_left,\n\u001b[32m    137\u001b[39m )\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output_single[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sources/syftr/.venv/lib/python3.12/site-packages/ragas/prompt/pydantic_prompt.py:190\u001b[39m, in \u001b[36mPydanticPrompt.generate_multiple\u001b[39m\u001b[34m(self, llm, data, n, temperature, stop, callbacks, retries_left)\u001b[39m\n\u001b[32m    183\u001b[39m prompt_rm, prompt_cb = new_group(\n\u001b[32m    184\u001b[39m     name=\u001b[38;5;28mself\u001b[39m.name,\n\u001b[32m    185\u001b[39m     inputs={\u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m: processed_data},\n\u001b[32m    186\u001b[39m     callbacks=callbacks,\n\u001b[32m    187\u001b[39m     metadata={\u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: ChainType.RAGAS_PROMPT},\n\u001b[32m    188\u001b[39m )\n\u001b[32m    189\u001b[39m prompt_value = PromptValue(text=\u001b[38;5;28mself\u001b[39m.to_string(processed_data))\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m resp = \u001b[38;5;28;01mawait\u001b[39;00m llm.generate(\n\u001b[32m    191\u001b[39m     prompt_value,\n\u001b[32m    192\u001b[39m     n=n,\n\u001b[32m    193\u001b[39m     temperature=temperature,\n\u001b[32m    194\u001b[39m     stop=stop,\n\u001b[32m    195\u001b[39m     callbacks=prompt_cb,\n\u001b[32m    196\u001b[39m )\n\u001b[32m    198\u001b[39m output_models = []\n\u001b[32m    199\u001b[39m parser = RagasOutputParser(pydantic_object=\u001b[38;5;28mself\u001b[39m.output_model)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sources/syftr/.venv/lib/python3.12/site-packages/ragas/llms/base.py:109\u001b[39m, in \u001b[36mBaseRagasLLM.generate\u001b[39m\u001b[34m(self, prompt, n, temperature, stop, callbacks)\u001b[39m\n\u001b[32m    104\u001b[39m     temperature = \u001b[38;5;28mself\u001b[39m.get_temperature(n)\n\u001b[32m    106\u001b[39m agenerate_text_with_retry = add_async_retry(\n\u001b[32m    107\u001b[39m     \u001b[38;5;28mself\u001b[39m.agenerate_text, \u001b[38;5;28mself\u001b[39m.run_config\n\u001b[32m    108\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m agenerate_text_with_retry(\n\u001b[32m    110\u001b[39m     prompt=prompt,\n\u001b[32m    111\u001b[39m     n=n,\n\u001b[32m    112\u001b[39m     temperature=temperature,\n\u001b[32m    113\u001b[39m     stop=stop,\n\u001b[32m    114\u001b[39m     callbacks=callbacks,\n\u001b[32m    115\u001b[39m )\n\u001b[32m    117\u001b[39m \u001b[38;5;66;03m# check there are no max_token issues\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_finished(result):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sources/syftr/.venv/lib/python3.12/site-packages/tenacity/asyncio/__init__.py:189\u001b[39m, in \u001b[36mAsyncRetrying.wraps.<locals>.async_wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    187\u001b[39m copy = \u001b[38;5;28mself\u001b[39m.copy()\n\u001b[32m    188\u001b[39m async_wrapped.statistics = copy.statistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m copy(fn, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sources/syftr/.venv/lib/python3.12/site-packages/tenacity/asyncio/__init__.py:111\u001b[39m, in \u001b[36mAsyncRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    109\u001b[39m retry_state = RetryCallState(retry_object=\u001b[38;5;28mself\u001b[39m, fn=fn, args=args, kwargs=kwargs)\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     do = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter(retry_state=retry_state)\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    113\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sources/syftr/.venv/lib/python3.12/site-packages/tenacity/asyncio/__init__.py:153\u001b[39m, in \u001b[36mAsyncRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    151\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m action(retry_state)\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sources/syftr/.venv/lib/python3.12/site-packages/tenacity/_utils.py:99\u001b[39m, in \u001b[36mwrap_to_async_func.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minner\u001b[39m(*args: typing.Any, **kwargs: typing.Any) -> typing.Any:\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sources/syftr/.venv/lib/python3.12/site-packages/tenacity/__init__.py:398\u001b[39m, in \u001b[36mBaseRetrying._post_retry_check_actions.<locals>.<lambda>\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_post_retry_check_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, retry_state: \u001b[33m\"\u001b[39m\u001b[33mRetryCallState\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    397\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.iter_state.is_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.retry_run_result):\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m         \u001b[38;5;28mself\u001b[39m._add_action_func(\u001b[38;5;28;01mlambda\u001b[39;00m rs: \u001b[43mrs\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutcome\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    399\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    401\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.after \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.7-macos-aarch64-none/lib/python3.12/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.7-macos-aarch64-none/lib/python3.12/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sources/syftr/.venv/lib/python3.12/site-packages/tenacity/asyncio/__init__.py:114\u001b[39m, in \u001b[36mAsyncRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    113\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m         result = \u001b[38;5;28;01mawait\u001b[39;00m fn(*args, **kwargs)\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[32m    116\u001b[39m         retry_state.set_exception(sys.exc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sources/syftr/.venv/lib/python3.12/site-packages/ragas/llms/base.py:254\u001b[39m, in \u001b[36mLangchainLLMWrapper.agenerate_text\u001b[39m\u001b[34m(self, prompt, n, temperature, stop, callbacks)\u001b[39m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.langchain_llm, \u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    253\u001b[39m     \u001b[38;5;28mself\u001b[39m.langchain_llm.n = n  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m254\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.langchain_llm.agenerate_prompt(\n\u001b[32m    255\u001b[39m         prompts=[prompt],\n\u001b[32m    256\u001b[39m         stop=stop,\n\u001b[32m    257\u001b[39m         callbacks=callbacks,\n\u001b[32m    258\u001b[39m     )\n\u001b[32m    259\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    260\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.langchain_llm.agenerate_prompt(\n\u001b[32m    261\u001b[39m         prompts=[prompt] * n,\n\u001b[32m    262\u001b[39m         stop=stop,\n\u001b[32m    263\u001b[39m         callbacks=callbacks,\n\u001b[32m    264\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sources/syftr/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:968\u001b[39m, in \u001b[36mBaseChatModel.agenerate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    959\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    960\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34magenerate_prompt\u001b[39m(\n\u001b[32m    961\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    965\u001b[39m     **kwargs: Any,\n\u001b[32m    966\u001b[39m ) -> LLMResult:\n\u001b[32m    967\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m968\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agenerate(\n\u001b[32m    969\u001b[39m         prompt_messages, stop=stop, callbacks=callbacks, **kwargs\n\u001b[32m    970\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sources/syftr/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:888\u001b[39m, in \u001b[36mBaseChatModel.agenerate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    875\u001b[39m run_managers = \u001b[38;5;28;01mawait\u001b[39;00m callback_manager.on_chat_model_start(\n\u001b[32m    876\u001b[39m     \u001b[38;5;28mself\u001b[39m._serialized,\n\u001b[32m    877\u001b[39m     messages_to_trace,\n\u001b[32m   (...)\u001b[39m\u001b[32m    882\u001b[39m     run_id=run_id,\n\u001b[32m    883\u001b[39m )\n\u001b[32m    885\u001b[39m input_messages = [\n\u001b[32m    886\u001b[39m     _normalize_messages(message_list) \u001b[38;5;28;01mfor\u001b[39;00m message_list \u001b[38;5;129;01min\u001b[39;00m messages\n\u001b[32m    887\u001b[39m ]\n\u001b[32m--> \u001b[39m\u001b[32m888\u001b[39m results = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m    889\u001b[39m     *[\n\u001b[32m    890\u001b[39m         \u001b[38;5;28mself\u001b[39m._agenerate_with_cache(\n\u001b[32m    891\u001b[39m             m,\n\u001b[32m    892\u001b[39m             stop=stop,\n\u001b[32m    893\u001b[39m             run_manager=run_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    894\u001b[39m             **kwargs,\n\u001b[32m    895\u001b[39m         )\n\u001b[32m    896\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages)\n\u001b[32m    897\u001b[39m     ],\n\u001b[32m    898\u001b[39m     return_exceptions=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    899\u001b[39m )\n\u001b[32m    900\u001b[39m exceptions = []\n\u001b[32m    901\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, res \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(results):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.7-macos-aarch64-none/lib/python3.12/asyncio/tasks.py:385\u001b[39m, in \u001b[36mTask.__wakeup\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m    383\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[32m    384\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m385\u001b[39m         \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    386\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    387\u001b[39m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[32m    388\u001b[39m         \u001b[38;5;28mself\u001b[39m.__step(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.7-macos-aarch64-none/lib/python3.12/asyncio/tasks.py:314\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    311\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    312\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    313\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    316\u001b[39m         result = coro.throw(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sources/syftr/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1094\u001b[39m, in \u001b[36mBaseChatModel._agenerate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1092\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1093\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._agenerate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1094\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._agenerate(\n\u001b[32m   1095\u001b[39m         messages, stop=stop, run_manager=run_manager, **kwargs\n\u001b[32m   1096\u001b[39m     )\n\u001b[32m   1097\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1098\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._agenerate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sources/syftr/.venv/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:1281\u001b[39m, in \u001b[36mBaseChatOpenAI._agenerate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1279\u001b[39m     generation_info = {\u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response.headers)}\n\u001b[32m   1280\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1281\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.async_client.create(**payload)\n\u001b[32m   1282\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m run_in_executor(\n\u001b[32m   1283\u001b[39m     \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mself\u001b[39m._create_chat_result, response, generation_info\n\u001b[32m   1284\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sources/syftr/.venv/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py:2028\u001b[39m, in \u001b[36mAsyncCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1985\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1986\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1987\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2025\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m   2026\u001b[39m ) -> ChatCompletion | AsyncStream[ChatCompletionChunk]:\n\u001b[32m   2027\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m2028\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m   2029\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m/chat/completions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2030\u001b[39m         body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m   2031\u001b[39m             {\n\u001b[32m   2032\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: messages,\n\u001b[32m   2033\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   2034\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33maudio\u001b[39m\u001b[33m\"\u001b[39m: audio,\n\u001b[32m   2035\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m   2036\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunction_call\u001b[39m\u001b[33m\"\u001b[39m: function_call,\n\u001b[32m   2037\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunctions\u001b[39m\u001b[33m\"\u001b[39m: functions,\n\u001b[32m   2038\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogit_bias\u001b[39m\u001b[33m\"\u001b[39m: logit_bias,\n\u001b[32m   2039\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogprobs\u001b[39m\u001b[33m\"\u001b[39m: logprobs,\n\u001b[32m   2040\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_completion_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_completion_tokens,\n\u001b[32m   2041\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_tokens,\n\u001b[32m   2042\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m   2043\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodalities\u001b[39m\u001b[33m\"\u001b[39m: modalities,\n\u001b[32m   2044\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m: n,\n\u001b[32m   2045\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m   2046\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m\"\u001b[39m: prediction,\n\u001b[32m   2047\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mpresence_penalty\u001b[39m\u001b[33m\"\u001b[39m: presence_penalty,\n\u001b[32m   2048\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mreasoning_effort\u001b[39m\u001b[33m\"\u001b[39m: reasoning_effort,\n\u001b[32m   2049\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m: response_format,\n\u001b[32m   2050\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m\"\u001b[39m: seed,\n\u001b[32m   2051\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m   2052\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m: stop,\n\u001b[32m   2053\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m   2054\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m   2055\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream_options\u001b[39m\u001b[33m\"\u001b[39m: stream_options,\n\u001b[32m   2056\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m   2057\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m   2058\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m   2059\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m   2060\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m   2061\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m   2062\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mweb_search_options\u001b[39m\u001b[33m\"\u001b[39m: web_search_options,\n\u001b[32m   2063\u001b[39m             },\n\u001b[32m   2064\u001b[39m             completion_create_params.CompletionCreateParamsStreaming\n\u001b[32m   2065\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[32m   2066\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m completion_create_params.CompletionCreateParamsNonStreaming,\n\u001b[32m   2067\u001b[39m         ),\n\u001b[32m   2068\u001b[39m         options=make_request_options(\n\u001b[32m   2069\u001b[39m             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m   2070\u001b[39m         ),\n\u001b[32m   2071\u001b[39m         cast_to=ChatCompletion,\n\u001b[32m   2072\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2073\u001b[39m         stream_cls=AsyncStream[ChatCompletionChunk],\n\u001b[32m   2074\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sources/syftr/.venv/lib/python3.12/site-packages/openai/_base_client.py:1748\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1734\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1735\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1736\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1743\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1744\u001b[39m ) -> ResponseT | _AsyncStreamT:\n\u001b[32m   1745\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1746\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1747\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1748\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sources/syftr/.venv/lib/python3.12/site-packages/openai/_base_client.py:1490\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1488\u001b[39m response = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1489\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1490\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.send(\n\u001b[32m   1491\u001b[39m         request,\n\u001b[32m   1492\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._should_stream_response_body(request=request),\n\u001b[32m   1493\u001b[39m         **kwargs,\n\u001b[32m   1494\u001b[39m     )\n\u001b[32m   1495\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m   1496\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mEncountered httpx.TimeoutException\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sources/syftr/.venv/lib/python3.12/site-packages/httpx/_client.py:1629\u001b[39m, in \u001b[36mAsyncClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m   1625\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m   1627\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m-> \u001b[39m\u001b[32m1629\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_handling_auth(\n\u001b[32m   1630\u001b[39m     request,\n\u001b[32m   1631\u001b[39m     auth=auth,\n\u001b[32m   1632\u001b[39m     follow_redirects=follow_redirects,\n\u001b[32m   1633\u001b[39m     history=[],\n\u001b[32m   1634\u001b[39m )\n\u001b[32m   1635\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1636\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sources/syftr/.venv/lib/python3.12/site-packages/httpx/_client.py:1657\u001b[39m, in \u001b[36mAsyncClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m   1654\u001b[39m request = \u001b[38;5;28;01mawait\u001b[39;00m auth_flow.\u001b[34m__anext__\u001b[39m()\n\u001b[32m   1656\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1657\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_handling_redirects(\n\u001b[32m   1658\u001b[39m         request,\n\u001b[32m   1659\u001b[39m         follow_redirects=follow_redirects,\n\u001b[32m   1660\u001b[39m         history=history,\n\u001b[32m   1661\u001b[39m     )\n\u001b[32m   1662\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1663\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sources/syftr/.venv/lib/python3.12/site-packages/httpx/_client.py:1694\u001b[39m, in \u001b[36mAsyncClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m   1691\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m   1692\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m hook(request)\n\u001b[32m-> \u001b[39m\u001b[32m1694\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_single_request(request)\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1696\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sources/syftr/.venv/lib/python3.12/site-packages/httpx/_client.py:1730\u001b[39m, in \u001b[36mAsyncClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1725\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1726\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an sync request with an AsyncClient instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1727\u001b[39m     )\n\u001b[32m   1729\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1730\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m transport.handle_async_request(request)\n\u001b[32m   1732\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, AsyncByteStream)\n\u001b[32m   1733\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sources/syftr/.venv/lib/python3.12/site-packages/httpx/_transports/default.py:394\u001b[39m, in \u001b[36mAsyncHTTPTransport.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    381\u001b[39m req = httpcore.Request(\n\u001b[32m    382\u001b[39m     method=request.method,\n\u001b[32m    383\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    391\u001b[39m     extensions=request.extensions,\n\u001b[32m    392\u001b[39m )\n\u001b[32m    393\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m394\u001b[39m     resp = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pool.handle_async_request(req)\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.AsyncIterable)\n\u001b[32m    398\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    399\u001b[39m     status_code=resp.status,\n\u001b[32m    400\u001b[39m     headers=resp.headers,\n\u001b[32m    401\u001b[39m     stream=AsyncResponseStream(resp.stream),\n\u001b[32m    402\u001b[39m     extensions=resp.extensions,\n\u001b[32m    403\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sources/syftr/.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:256\u001b[39m, in \u001b[36mAsyncConnectionPool.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.AsyncIterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sources/syftr/.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:236\u001b[39m, in \u001b[36mAsyncConnectionPool.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = \u001b[38;5;28;01mawait\u001b[39;00m pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m connection.handle_async_request(\n\u001b[32m    237\u001b[39m         pool_request.request\n\u001b[32m    238\u001b[39m     )\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sources/syftr/.venv/lib/python3.12/site-packages/httpcore/_async/connection.py:103\u001b[39m, in \u001b[36mAsyncHTTPConnection.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection.handle_async_request(request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sources/syftr/.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:136\u001b[39m, in \u001b[36mAsyncHTTP11Connection.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sources/syftr/.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:106\u001b[39m, in \u001b[36mAsyncHTTP11Connection.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._receive_response_headers(**kwargs)\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sources/syftr/.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:177\u001b[39m, in \u001b[36mAsyncHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._receive_event(timeout=timeout)\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sources/syftr/.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:217\u001b[39m, in \u001b[36mAsyncHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._network_stream.read(\n\u001b[32m    218\u001b[39m         \u001b[38;5;28mself\u001b[39m.READ_NUM_BYTES, timeout=timeout\n\u001b[32m    219\u001b[39m     )\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sources/syftr/.venv/lib/python3.12/site-packages/httpcore/_backends/anyio.py:35\u001b[39m, in \u001b[36mAnyIOStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m anyio.fail_after(timeout):\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stream.receive(max_bytes=max_bytes)\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m anyio.EndOfStream:  \u001b[38;5;66;03m# pragma: nocover\u001b[39;00m\n\u001b[32m     37\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sources/syftr/.venv/lib/python3.12/site-packages/anyio/streams/tls.py:219\u001b[39m, in \u001b[36mTLSStream.receive\u001b[39m\u001b[34m(self, max_bytes)\u001b[39m\n\u001b[32m    218\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreceive\u001b[39m(\u001b[38;5;28mself\u001b[39m, max_bytes: \u001b[38;5;28mint\u001b[39m = \u001b[32m65536\u001b[39m) -> \u001b[38;5;28mbytes\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m     data = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_sslobject_method(\u001b[38;5;28mself\u001b[39m._ssl_object.read, max_bytes)\n\u001b[32m    220\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[32m    221\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m EndOfStream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sources/syftr/.venv/lib/python3.12/site-packages/anyio/streams/tls.py:162\u001b[39m, in \u001b[36mTLSStream._call_sslobject_method\u001b[39m\u001b[34m(self, func, *args)\u001b[39m\n\u001b[32m    159\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._write_bio.pending:\n\u001b[32m    160\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transport_stream.send(\u001b[38;5;28mself\u001b[39m._write_bio.read())\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     data = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transport_stream.receive()\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m EndOfStream:\n\u001b[32m    164\u001b[39m     \u001b[38;5;28mself\u001b[39m._read_bio.write_eof()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sources/syftr/.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py:1254\u001b[39m, in \u001b[36mSocketStream.receive\u001b[39m\u001b[34m(self, max_bytes)\u001b[39m\n\u001b[32m   1248\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1249\u001b[39m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._protocol.read_event.is_set()\n\u001b[32m   1250\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._transport.is_closing()\n\u001b[32m   1251\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._protocol.is_at_eof\n\u001b[32m   1252\u001b[39m ):\n\u001b[32m   1253\u001b[39m     \u001b[38;5;28mself\u001b[39m._transport.resume_reading()\n\u001b[32m-> \u001b[39m\u001b[32m1254\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._protocol.read_event.wait()\n\u001b[32m   1255\u001b[39m     \u001b[38;5;28mself\u001b[39m._transport.pause_reading()\n\u001b[32m   1256\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.7-macos-aarch64-none/lib/python3.12/asyncio/locks.py:212\u001b[39m, in \u001b[36mEvent.wait\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28mself\u001b[39m._waiters.append(fut)\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m fut\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.7-macos-aarch64-none/lib/python3.12/asyncio/futures.py:291\u001b[39m, in \u001b[36mFuture.__await__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.done():\n\u001b[32m    290\u001b[39m     \u001b[38;5;28mself\u001b[39m._asyncio_future_blocking = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m291\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m  \u001b[38;5;66;03m# This tells Task to wait for completion.\u001b[39;00m\n\u001b[32m    292\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.done():\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mawait wasn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt used with future\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mCancelledError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for scenario in scenarios:\n",
    "    result = await query.generate_sample(scenario=scenario)\n",
    "    print(f\"Scenario: {result.user_input}\")\n",
    "    print(\"-\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "syftr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

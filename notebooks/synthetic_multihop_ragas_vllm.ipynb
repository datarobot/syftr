{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ee3a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, MarkdownHeaderTextSplitter\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "\n",
    "path = \"/Users/debadeepta.dey/datasets/barclays\"\n",
    "loader = DirectoryLoader(path, glob=\"**/*.md\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61bc9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Split by Markdown Headers (Most intelligent for markdown)\n",
    "# This preserves the document structure and creates logical chunks\n",
    "\n",
    "def split_markdown_by_headers(document_content):\n",
    "    \"\"\"\n",
    "    Split markdown document by headers, preserving document structure\n",
    "    \"\"\"\n",
    "    # Define headers to split on (from h1 to h3)\n",
    "    headers_to_split_on = [\n",
    "        (\"#\", \"Header 1\"),\n",
    "        (\"##\", \"Header 2\"), \n",
    "        (\"###\", \"Header 3\"),\n",
    "    ]\n",
    "    \n",
    "    # Create the markdown header text splitter\n",
    "    markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "        headers_to_split_on=headers_to_split_on,\n",
    "        strip_headers=False  # Keep headers in the chunks\n",
    "    )\n",
    "    \n",
    "    # Split the document\n",
    "    md_header_splits = markdown_splitter.split_text(document_content)\n",
    "    \n",
    "    return md_header_splits\n",
    "\n",
    "# Example usage with your loaded documents\n",
    "if docs:\n",
    "    # Take the first document as example\n",
    "    first_doc = docs[0]\n",
    "    header_splits = split_markdown_by_headers(first_doc.page_content)\n",
    "    \n",
    "    print(f\"Original document split into {len(header_splits)} chunks based on headers\")\n",
    "    \n",
    "    # Display first few chunks\n",
    "    for i, chunk in enumerate(header_splits[:3]):\n",
    "        print(f\"\\n--- Chunk {i+1} ---\")\n",
    "        print(f\"Content: {chunk.page_content[:200]}...\")\n",
    "        print(f\"Metadata: {chunk.metadata}\")\n",
    "        print(f\"Full length: {len(chunk.page_content)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c057368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Recursive Character Text Splitter (Good fallback)\n",
    "# This method is useful when documents don't have clear header structure\n",
    "\n",
    "def split_markdown_recursive(document_content, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Split markdown using recursive character splitter with markdown-aware separators\n",
    "    \"\"\"\n",
    "    # Define separators that work well for markdown\n",
    "    markdown_separators = [\n",
    "        \"\\n\\n\",  # Double newline (paragraph breaks)\n",
    "        \"\\n\",    # Single newline\n",
    "        \" \",     # Space\n",
    "        \"\"       # Character level\n",
    "    ]\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=markdown_separators,\n",
    "        length_function=len,\n",
    "    )\n",
    "    \n",
    "    # Split the document\n",
    "    chunks = text_splitter.split_text(document_content)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Example usage\n",
    "if docs:\n",
    "    first_doc = docs[0]\n",
    "    recursive_chunks = split_markdown_recursive(\n",
    "        first_doc.page_content, \n",
    "        chunk_size=2048,  # Adjust based on your needs\n",
    "        chunk_overlap=200\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nRecursive splitting created {len(recursive_chunks)} chunks\")\n",
    "    \n",
    "    # Display first few chunks\n",
    "    for i, chunk in enumerate(recursive_chunks[:3]):\n",
    "        print(f\"\\n--- Recursive Chunk {i+1} ---\")\n",
    "        print(f\"Content: {chunk[:200]}...\")\n",
    "        print(f\"Length: {len(chunk)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fc45fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 3: Hybrid Approach (Recommended)\n",
    "# Combine header-based splitting with recursive splitting for optimal results\n",
    "\n",
    "def smart_markdown_split(document_content, max_chunk_size=1500, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Smart markdown splitting that combines header-based and recursive approaches\n",
    "    \"\"\"\n",
    "    from langchain.schema import Document\n",
    "    \n",
    "    # First, try to split by headers\n",
    "    headers_to_split_on = [\n",
    "        (\"#\", \"Header 1\"),\n",
    "        (\"##\", \"Header 2\"), \n",
    "        (\"###\", \"Header 3\"),\n",
    "        (\"####\", \"Header 4\"),\n",
    "    ]\n",
    "    \n",
    "    markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "        headers_to_split_on=headers_to_split_on,\n",
    "        strip_headers=False\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Split by headers first\n",
    "        header_splits = markdown_splitter.split_text(document_content)\n",
    "        \n",
    "        # If header splits are too large, further split them recursively\n",
    "        final_chunks = []\n",
    "        \n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=max_chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "            length_function=len,\n",
    "        )\n",
    "        \n",
    "        for doc in header_splits:\n",
    "            if len(doc.page_content) > max_chunk_size:\n",
    "                # Split large chunks further\n",
    "                sub_chunks = text_splitter.split_text(doc.page_content)\n",
    "                for i, sub_chunk in enumerate(sub_chunks):\n",
    "                    # Preserve metadata from header splitting\n",
    "                    new_metadata = doc.metadata.copy()\n",
    "                    new_metadata['sub_chunk'] = i\n",
    "                    final_chunks.append(Document(\n",
    "                        page_content=sub_chunk,\n",
    "                        metadata=new_metadata\n",
    "                    ))\n",
    "            else:\n",
    "                final_chunks.append(doc)\n",
    "                \n",
    "        return final_chunks\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Header splitting failed: {e}\")\n",
    "        # Fallback to recursive splitting\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=max_chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "        )\n",
    "        chunks = text_splitter.split_text(document_content)\n",
    "        return [Document(page_content=chunk, metadata={}) for chunk in chunks]\n",
    "\n",
    "# Example usage with the hybrid approach\n",
    "if docs:\n",
    "    first_doc = docs[0]\n",
    "    smart_chunks = smart_markdown_split(\n",
    "        first_doc.page_content,\n",
    "        max_chunk_size=1200,\n",
    "        chunk_overlap=150\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nSmart splitting created {len(smart_chunks)} chunks\")\n",
    "    \n",
    "    # Display statistics\n",
    "    chunk_lengths = [len(chunk.page_content) for chunk in smart_chunks]\n",
    "    print(f\"Average chunk length: {sum(chunk_lengths) / len(chunk_lengths):.0f} characters\")\n",
    "    print(f\"Min chunk length: {min(chunk_lengths)} characters\")\n",
    "    print(f\"Max chunk length: {max(chunk_lengths)} characters\")\n",
    "    \n",
    "    # Display first few chunks with metadata\n",
    "    for i, chunk in enumerate(smart_chunks[:3]):\n",
    "        print(f\"\\n--- Smart Chunk {i+1} ---\")\n",
    "        print(f\"Metadata: {chunk.metadata}\")\n",
    "        print(f\"Content preview: {chunk.page_content[:200]}...\")\n",
    "        print(f\"Length: {len(chunk.page_content)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7956566",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "# Utility function to process all your documents\n",
    "def process_all_documents(docs, output_method='smart', **kwargs):\n",
    "    \"\"\"\n",
    "    Process all loaded documents and return chunks\n",
    "    \n",
    "    Args:\n",
    "        docs: List of loaded documents\n",
    "        output_method: 'header', 'recursive', or 'smart'\n",
    "        **kwargs: Additional parameters for the splitting methods\n",
    "    \n",
    "    Returns:\n",
    "        List of all chunks with source document information\n",
    "    \"\"\"\n",
    "    all_chunks = []\n",
    "    \n",
    "    for doc_idx, doc in enumerate(docs):\n",
    "        print(f\"Processing document {doc_idx + 1}/{len(docs)}: {doc.metadata.get('source', 'unknown')}\")\n",
    "        \n",
    "        if output_method == 'header':\n",
    "            chunks = split_markdown_by_headers(doc.page_content)\n",
    "        elif output_method == 'recursive':\n",
    "            chunk_texts = split_markdown_recursive(doc.page_content, **kwargs)\n",
    "            chunks = [Document(page_content=text, metadata=doc.metadata.copy()) for text in chunk_texts]\n",
    "        elif output_method == 'smart':\n",
    "            chunks = smart_markdown_split(doc.page_content, **kwargs)\n",
    "        else:\n",
    "            raise ValueError(\"output_method must be 'header', 'recursive', or 'smart'\")\n",
    "        \n",
    "        # Add source document information to each chunk\n",
    "        for chunk_idx, chunk in enumerate(chunks):\n",
    "            chunk.metadata['source_doc_index'] = doc_idx\n",
    "            chunk.metadata['chunk_index'] = chunk_idx\n",
    "            chunk.metadata['original_source'] = doc.metadata.get('source', 'unknown')\n",
    "            all_chunks.append(chunk)\n",
    "    \n",
    "    return all_chunks\n",
    "\n",
    "# Process all your documents using the smart method\n",
    "all_processed_chunks = process_all_documents(\n",
    "    docs, \n",
    "    output_method='smart',  # Change to 'header' or 'recursive' if preferred\n",
    "    max_chunk_size=2000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "\n",
    "print(f\"\\nTotal chunks created from all documents: {len(all_processed_chunks)}\")\n",
    "\n",
    "# Show summary statistics\n",
    "if all_processed_chunks:\n",
    "    chunk_lengths = [len(chunk.page_content) for chunk in all_processed_chunks]\n",
    "    print(f\"Average chunk length: {sum(chunk_lengths) / len(chunk_lengths):.0f} characters\")\n",
    "    print(f\"Chunk length range: {min(chunk_lengths)} - {max(chunk_lengths)} characters\")\n",
    "    \n",
    "    # Show distribution by source document\n",
    "    source_counts = {}\n",
    "    for chunk in all_processed_chunks:\n",
    "        source = chunk.metadata.get('original_source', 'unknown')\n",
    "        source_counts[source] = source_counts.get(source, 0) + 1\n",
    "    \n",
    "    print(f\"\\nChunks per source document:\")\n",
    "    for source, count in source_counts.items():\n",
    "        print(f\"  {source}: {count} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fef83a8",
   "metadata": {},
   "source": [
    "# Initialize knowledge graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7aa909",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.graph import KnowledgeGraph\n",
    "from ragas.testset.graph import Node, NodeType\n",
    "\n",
    "\n",
    "kg = KnowledgeGraph()\n",
    "for doc in all_processed_chunks:\n",
    "    kg.nodes.append(\n",
    "        Node(\n",
    "            type=NodeType.DOCUMENT,\n",
    "            properties={\n",
    "                \"page_content\": doc.page_content,\n",
    "                \"document_metadata\": doc.metadata,\n",
    "            },\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373120f0",
   "metadata": {},
   "source": [
    "## VLLM Configuration for RAGAS\n",
    "\n",
    "The configuration above connects RAGAS to your vLLM server. Here are some key points:\n",
    "\n",
    "1. **Base URL**: `http://localhost:8003/v1` - your vLLM endpoint\n",
    "2. **API Key**: Set to \"not-needed\" since vLLM typically doesn't require authentication\n",
    "3. **Model Name**: Replace `\"your-model-name\"` with the actual model you're serving\n",
    "4. **Temperature**: Controls randomness (0.1 is relatively deterministic)\n",
    "5. **Max Tokens**: Maximum response length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b55eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings.base import embedding_factory\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Configure vLLM hosted LLM\n",
    "vllm_llm = ChatOpenAI(\n",
    "    base_url=\"http://localhost:8003/v1\",\n",
    "    api_key=\"asdf\",\n",
    "    model=\"Qwen/Qwen2.5\",  # Replace with your actual model name\n",
    "    temperature=0.1,\n",
    "    max_tokens=4096,\n",
    ")\n",
    "\n",
    "# Wrap for RAGAS\n",
    "llm = LangchainLLMWrapper(vllm_llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fe900c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the vLLM connection\n",
    "print(\"Testing vLLM connection...\")\n",
    "\n",
    "try:\n",
    "    # Test the LLM directly\n",
    "    test_response = vllm_llm.invoke(\"Hello, this is a test. Please respond briefly.\")\n",
    "    print(f\"âœ… vLLM connection successful!\")\n",
    "    print(f\"Response: {test_response.content}\")\n",
    "    \n",
    "    # Test with RAGAS wrapper\n",
    "    from ragas.llms.base import BaseRagasLLM\n",
    "    if isinstance(llm, BaseRagasLLM):\n",
    "        print(\"âœ… RAGAS LLM wrapper configured correctly\")\n",
    "    else:\n",
    "        print(\"âš ï¸  RAGAS LLM wrapper might need adjustment\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error connecting to vLLM: {e}\")\n",
    "    print(\"Please check:\")\n",
    "    print(\"1. vLLM server is running\")\n",
    "    print(\"2. Model name is correct\")\n",
    "    print(\"3. No firewall blocking the connection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7108734",
   "metadata": {},
   "source": [
    "# vLLM Hosted Embedding Model Configuration\n",
    "\n",
    "Here's how to configure a vLLM hosted embedding model for use with RAGAS:\n",
    "\n",
    "## Option 1: Using OpenAI-compatible embedding endpoint\n",
    "If your vLLM server hosts an embedding model with OpenAI-compatible API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31482f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "import asyncio\n",
    "\n",
    "# Try each configuration until one works\n",
    "vllm_embeddings = OpenAIEmbeddings(\n",
    "        base_url=\"http://localhost:8001/v1\",\n",
    "        api_key=\"asdf\",\n",
    "        model='thenlper/gte-large',\n",
    "        tiktoken_enabled=False,  # Disable tiktoken for vLLM\n",
    "    )\n",
    "\n",
    "embedding_model = LangchainEmbeddingsWrapper(vllm_embeddings)\n",
    "\n",
    "# async def test_embedding_model(vllm_embeddings: OpenAIEmbeddings):\n",
    "#     \"\"\"Async function to test the embedding model\"\"\"\n",
    "    \n",
    "#     # Test with a simple text first\n",
    "#     print(\"testing query embedding...\")\n",
    "#     test_result = vllm_embeddings.embed_query(\"Risk management is crucial for financial institutions.\")\n",
    "#     print(f\"query embedding dimensions: {len(test_result)}\")\n",
    "\n",
    "#     # Test with texts\n",
    "#     print(\"testing text embedding...\")\n",
    "#     test_texts = [\n",
    "#         \"This is a test document about financial analysis.\",\n",
    "#         \"Machine learning models are used in banking.\",\n",
    "#         \"Risk management is crucial for financial institutions.\"\n",
    "#     ]\n",
    "#     test_results = vllm_embeddings.embed_documents(test_texts)\n",
    "#     print(f\"Text embedding dimensions: {len(test_results[0])} for {len(test_results)} texts\")\n",
    "    \n",
    "#     # If successful, wrap for RAGAS and test it through the wrapper\n",
    "#     embedding_model = LangchainEmbeddingsWrapper(vllm_embeddings)\n",
    "\n",
    "#     print(\"Testing wrapped embedding model query ...\")\n",
    "#     embedding_result = await embedding_model.embed_query(\"Risk management is crucial for financial institutions.\")\n",
    "#     print(f\"Wrapped query embedding dimensions: {len(embedding_result)}\")\n",
    "\n",
    "#     print(\"Testing wrapped embedding model text ...\")\n",
    "#     embedding_results = await embedding_model.embed_texts(test_texts, is_async=True)\n",
    "#     print(f\"Wrapped text embedding dimensions: {len(embedding_results[0])} for {len(embedding_results)} texts\")\n",
    "\n",
    "#     print(f\"âœ… Successfully configured vLLM embedding model\")\n",
    "#     return embedding_model\n",
    "\n",
    "# # Run the async function\n",
    "# try:\n",
    "#     embedding_model = asyncio.run(test_embedding_model(vllm_embeddings))\n",
    "#     print(f\"ðŸŽ‰ Using vLLM embedding model successfully!\")\n",
    "# except Exception as e:\n",
    "#     print(f\"âŒ Failed with: {str(e)[:100]}...\")\n",
    "#     embedding_model = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66685102",
   "metadata": {},
   "source": [
    "# Option 2: local embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9123be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "local_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"all-MiniLM-L6-v2\",\n",
    "    model_kwargs={\"device\": \"cpu\"} # Or \"cuda\" for GPU\n",
    ")\n",
    "\n",
    "res = local_embeddings.embed_query(\"Who is this?\")  # Test local embedding model\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251cc400",
   "metadata": {},
   "source": [
    "# Default synthetic data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac392bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.transforms import default_transforms, apply_transforms\n",
    "\n",
    "\n",
    "# define your LLM and Embedding Model\n",
    "# here we are using the same LLM and the configured embedding model\n",
    "transformer_llm = llm\n",
    "trans = default_transforms(documents=all_processed_chunks, llm=transformer_llm, embedding_model=local_embeddings)\n",
    "apply_transforms(kg, trans)\n",
    "print(kg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9ee375d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No nodes that satisfied the given filer. Try changing the filter.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m generator = TestsetGenerator(llm=llm, embedding_model=embedding_model, knowledge_graph=kg)\n\u001b[32m      5\u001b[39m query_distribution = default_query_distribution(llm)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m testset = \u001b[43mgenerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtestset_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_distribution\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_distribution\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m testset.to_pandas()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sources/syftr/.venv/lib/python3.12/site-packages/ragas/testset/synthesizers/generate.py:369\u001b[39m, in \u001b[36mTestsetGenerator.generate\u001b[39m\u001b[34m(self, testset_size, query_distribution, num_personas, run_config, batch_size, callbacks, token_usage_parser, with_debugging_logs, raise_exceptions)\u001b[39m\n\u001b[32m    366\u001b[39m     patch_logger(\u001b[33m\"\u001b[39m\u001b[33mragas.experimental.testset.transforms\u001b[39m\u001b[33m\"\u001b[39m, logging.DEBUG)\n\u001b[32m    368\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.persona_list \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m369\u001b[39m     \u001b[38;5;28mself\u001b[39m.persona_list = \u001b[43mgenerate_personas_from_kg\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m        \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkg\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mknowledge_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_personas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_personas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    376\u001b[39m     random.shuffle(\u001b[38;5;28mself\u001b[39m.persona_list)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sources/syftr/.venv/lib/python3.12/site-packages/ragas/testset/persona.py:95\u001b[39m, in \u001b[36mgenerate_personas_from_kg\u001b[39m\u001b[34m(kg, llm, persona_generation_prompt, num_personas, filter_fn, callbacks)\u001b[39m\n\u001b[32m     93\u001b[39m nodes = [node \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m kg.nodes \u001b[38;5;28;01mif\u001b[39;00m filter_fn(node)]\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(nodes) == \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     96\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo nodes that satisfied the given filer. Try changing the filter.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     97\u001b[39m     )\n\u001b[32m     99\u001b[39m summaries = [node.properties.get(\u001b[33m\"\u001b[39m\u001b[33msummary\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m nodes]\n\u001b[32m    100\u001b[39m summaries = [summary \u001b[38;5;28;01mfor\u001b[39;00m summary \u001b[38;5;129;01min\u001b[39;00m summaries \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(summary, \u001b[38;5;28mstr\u001b[39m)]\n",
      "\u001b[31mValueError\u001b[39m: No nodes that satisfied the given filer. Try changing the filter."
     ]
    }
   ],
   "source": [
    "from ragas.testset import TestsetGenerator\n",
    "from ragas.testset.synthesizers import default_query_distribution\n",
    "\n",
    "generator = TestsetGenerator(llm=llm, embedding_model=embedding_model, knowledge_graph=kg)\n",
    "query_distribution = default_query_distribution(llm)\n",
    "testset = generator.generate(testset_size=10, query_distribution=query_distribution)\n",
    "testset.to_pandas()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de8aa7b",
   "metadata": {},
   "source": [
    "# Custom multi-hop question answer generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004c9bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.transforms import Parallel, apply_transforms\n",
    "from ragas.testset.transforms import (\n",
    "    HeadlinesExtractor,\n",
    "    HeadlineSplitter,\n",
    "    KeyphrasesExtractor,\n",
    "    OverlapScoreBuilder,\n",
    ")\n",
    "\n",
    "\n",
    "headline_extractor = HeadlinesExtractor(llm=llm)\n",
    "headline_splitter = HeadlineSplitter(min_tokens=300, max_tokens=1000)\n",
    "keyphrase_extractor = KeyphrasesExtractor(\n",
    "    llm=llm, property_name=\"keyphrases\", max_num=10\n",
    ")\n",
    "relation_builder = OverlapScoreBuilder(\n",
    "    property_name=\"keyphrases\",\n",
    "    new_property_name=\"overlap_score\",\n",
    "    threshold=0.01,\n",
    "    distance_threshold=0.9,\n",
    ")\n",
    "\n",
    "transforms = [\n",
    "    headline_extractor,\n",
    "    headline_splitter,\n",
    "    keyphrase_extractor,\n",
    "    relation_builder,\n",
    "]\n",
    "\n",
    "apply_transforms(kg, transforms=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac06f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.persona import Persona\n",
    "\n",
    "person1 = Persona(\n",
    "    name=\"financial analyst at Barclays\",\n",
    "    role_description=\"A junior financial analyst at Barclays curious on workings on financial systems at Barclays \",\n",
    ")\n",
    "persona2 = Persona(\n",
    "    name=\"\",\n",
    "    role_description=\"AI developer building a financial system for Barclays\",\n",
    ")\n",
    "persona_list = [person1, persona2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95074d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import typing as t\n",
    "from ragas.testset.synthesizers.multi_hop.base import (\n",
    "    MultiHopQuerySynthesizer,\n",
    "    MultiHopScenario,\n",
    ")\n",
    "from ragas.testset.synthesizers.prompts import (\n",
    "    ThemesPersonasInput,\n",
    "    ThemesPersonasMatchingPrompt,\n",
    ")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MyMultiHopQuery(MultiHopQuerySynthesizer):\n",
    "\n",
    "    theme_persona_matching_prompt = ThemesPersonasMatchingPrompt()\n",
    "\n",
    "    async def _generate_scenarios(\n",
    "        self,\n",
    "        n: int,\n",
    "        knowledge_graph,\n",
    "        persona_list,\n",
    "        callbacks,\n",
    "    ) -> t.List[MultiHopScenario]:\n",
    "\n",
    "        # query and get (node_a, rel, node_b) to create multi-hop queries\n",
    "        results = knowledge_graph.find_two_nodes_single_rel(\n",
    "            relationship_condition=lambda rel: (\n",
    "                True if rel.type == \"keyphrases_overlap\" else False\n",
    "            )\n",
    "        )\n",
    "\n",
    "        num_sample_per_triplet = max(1, n // len(results))\n",
    "\n",
    "        scenarios = []\n",
    "        for triplet in results:\n",
    "            if len(scenarios) < n:\n",
    "                node_a, node_b = triplet[0], triplet[-1]\n",
    "                overlapped_keywords = triplet[1].properties[\"overlapped_items\"]\n",
    "                if overlapped_keywords:\n",
    "\n",
    "                    # match the keyword with a persona for query creation\n",
    "                    themes = list(dict(overlapped_keywords).keys())\n",
    "                    prompt_input = ThemesPersonasInput(\n",
    "                        themes=themes, personas=persona_list\n",
    "                    )\n",
    "                    persona_concepts = (\n",
    "                        await self.theme_persona_matching_prompt.generate(\n",
    "                            data=prompt_input, llm=self.llm, callbacks=callbacks\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                    overlapped_keywords = [list(item) for item in overlapped_keywords]\n",
    "\n",
    "                    # prepare and sample possible combinations\n",
    "                    base_scenarios = self.prepare_combinations(\n",
    "                        [node_a, node_b],\n",
    "                        overlapped_keywords,\n",
    "                        personas=persona_list,\n",
    "                        persona_item_mapping=persona_concepts.mapping,\n",
    "                        property_name=\"keyphrases\",\n",
    "                    )\n",
    "\n",
    "                    # get number of required samples from this triplet\n",
    "                    base_scenarios = self.sample_diverse_combinations(\n",
    "                        base_scenarios, num_sample_per_triplet\n",
    "                    )\n",
    "\n",
    "                    scenarios.extend(base_scenarios)\n",
    "\n",
    "        return scenarios\n",
    "\n",
    "query = MyMultiHopQuery(llm=llm)\n",
    "scenarios = await query.generate_scenarios(\n",
    "    n=10, knowledge_graph=kg, persona_list=persona_list\n",
    ")\n",
    "\n",
    "scenarios[4]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "syftr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

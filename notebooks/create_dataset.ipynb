{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# QA Dataset Generation\n",
    "Given a raw text, the notebook helps to generate a custom HuggingFace QA dataset based on the given information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.core import ultratb\n",
    "\n",
    "ultratb.VerboseTB.tb_highlight = \"bg:#3e0054\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from syftr.configuration import cfg\n",
    "except:\n",
    "    import os\n",
    "    os.chdir('./../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILEPATH = \"data.md\"  # Path to the raw text file\n",
    "CHUNK_SIZE = 200  # Size of each text chunk\n",
    "LLMS = [  # adjust to LLMs you want to use for question generation\n",
    "    \"gpt-4o-mini\",\n",
    "    \"deepseek-ai/DeepSeek-R1-Distill-Llama-70B\",\n",
    "    \"Qwen/Qwen3-32B\",\n",
    "    \"google/gemma-3-27b-it\",\n",
    "    \"microsoft/Phi-4-multimodal-instruct\",\n",
    "]  # We randomly select one of the provided LLMs per chunk\n",
    "NUM_PARALLEL = 50  # Number of parallel processes to use for chunk processing\n",
    "CUSTOM_QA_INSTRUCTIONS = None  # Add instructions that are specific to your QA generation task\n",
    "assert CUSTOM_QA_INSTRUCTIONS, \"Please provide custom instructions for the QA generation.\"\n",
    "\n",
    "DATASET_NAME = None\n",
    "assert DATASET_NAME, \"Please set the DATASET_NAME variable to a valid dataset name.\"\n",
    "HF_DATASET_NAME = f\"DataRobot-Research/{DATASET_NAME}\"  # Adjust name of the dataset on Hugging Face Hub\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")  # provide your HF token with write access\n",
    "\n",
    "assert HF_TOKEN, \"Please set the HF_TOKEN environment variable with your Hugging Face token.\"\n",
    "\n",
    "print(f\"Using Hugging Face token: {HF_TOKEN[:4]}...{HF_TOKEN[-4:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text(file_path: str) -> str:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        return file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = load_text(DATA_FILEPATH)\n",
    "print(f\"Loaded {len(raw_text)} characters from {DATA_FILEPATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text: str, chunk_size: int = 1000) -> list:\n",
    "    return [text[i : i + chunk_size] for i in range(0, len(text), chunk_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = chunk_text(raw_text, CHUNK_SIZE)\n",
    "print(f\"Created {len(chunks)} chunks of size {CHUNK_SIZE} characters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tenacity import retry, stop_after_attempt, wait_fixed\n",
    "from syftr.llm import get_llm\n",
    "\n",
    "\n",
    "@retry(stop=stop_after_attempt(5), wait=wait_fixed(2))\n",
    "def generate(prompt: str, llm_name: str, **kwargs):\n",
    "    llm = get_llm(llm_name)\n",
    "    assert llm is not None, f\"LLM {llm_name} not found.\"\n",
    "    response = llm.complete(prompt=prompt, **kwargs)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_qa_from_chunk(\n",
    "    chunk: str, llm_name: str, **kwargs\n",
    ") -> str:\n",
    "    prompt = f\"\"\"Generate a question and answer based on the text below. Make sure to not use special formatting, like markdown, but formulate the question and the answer in a plan text format. Start with the question followed by the answer. The question should be clear and concise, and the answer should be informative and directly related to the question, for instance,\n",
    "    \n",
    "    Question: Who is in charge of the project SuperGold?\n",
    "\n",
    "    Answer: The project is led by Dr. Jane Smith.\n",
    "\n",
    "    Note that the question should always be specific, for instance, don't use generic terms like \"the text\" but always be specific about what you mean and use concrete names whereever possible. Same with images and tables: make sure you can specify which table or image your question is about or do not ask this question. The answer should be a direct response to the question, providing relevant information from the text chunk provided below.\n",
    "    If you cannot generate a question and answer based on the text, return an empty string.\n",
    "    Moreover, follow these custom instructions: \\n\\n{CUSTOM_QA_INSTRUCTIONS}\\n\\n\n",
    "\n",
    "    Chunk: \\n\\n{chunk}\"\"\"\n",
    "    response = generate(prompt, llm_name, **kwargs)\n",
    "    return response.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import typing as T\n",
    "\n",
    "def parse_qa_pairs(text: str, llm_name: str | None) -> T.List[T.Dict[str, str]]:  \n",
    "    pattern = r\"Question:\\s*(.*?)\\s*Answer:\\s*(.*)\"\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    parsed_pairs = []\n",
    "    for question, answer in matches:\n",
    "        pair = {\"question\": question.strip(), \"answer\": answer.strip()}\n",
    "        if llm_name:\n",
    "            pair[\"llm_name\"] = llm_name\n",
    "        parsed_pairs.append(pair)\n",
    "    return parsed_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "\n",
    "def get_qa_pairs_from_chunks(chunks: T.List[str]) -> T.List[T.Dict[str, str]]:\n",
    "    qa_pairs = []\n",
    "\n",
    "    def _gen(chunk: str) -> T.List[T.Dict[str, str]]:\n",
    "        llm_name = random.choice(LLMS)\n",
    "        generated_text = generate_qa_from_chunk(chunk, llm_name, max_tokens=1024, temperature=0.7)\n",
    "        return parse_qa_pairs(generated_text, llm_name)\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=NUM_PARALLEL) as executor:\n",
    "        futures = [\n",
    "            executor.submit(_gen, chunk)\n",
    "            for chunk in chunks\n",
    "        ]\n",
    "        for future in as_completed(futures):\n",
    "            qa_pairs.extend(future.result())\n",
    "    return qa_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = get_qa_pairs_from_chunks(chunks)\n",
    "print(f\"Generated {len(qa_pairs)} Q&A pairs from {len(chunks)} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for pair in qa_pairs:\n",
    "#     print(\"-\"* 40)\n",
    "#     print(f\"LLM: {pair.get('llm_name', 'Unknown')}\\nQuestion: {pair['question']}\\nAnswer: {pair['answer']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "**Adjust the parameters to make a custom split based on your needs and the amount of data generated.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "from syftr.configuration import cfg\n",
    "\n",
    "\n",
    "def gen_partitions(qa_pairs: T.List[T.Dict[str, str]]) -> datasets.DatasetDict:\n",
    "    data = datasets.DatasetDict(\n",
    "        {\n",
    "            \"train\": datasets.Dataset.from_list(qa_pairs[:200]),\n",
    "            \"test\": datasets.Dataset.from_list(qa_pairs[200:400]),\n",
    "            \"holdout\": datasets.Dataset.from_list(qa_pairs[400:]),\n",
    "            \"sample\": datasets.Dataset.from_list(qa_pairs[:5]),  # for quick testing\n",
    "        }\n",
    "    )\n",
    "    return data\n",
    "\n",
    "\n",
    "dataset = gen_partitions(qa_pairs)\n",
    "print(f\"Generated dataset with {len(dataset['train'])} training samples, {len(dataset['test'])} test samples, and {len(dataset['holdout'])} holdout samples.\")\n",
    "\n",
    "dataset.save_to_disk(cfg.paths.datasets_dir / DATASET_NAME)\n",
    "print(f\"Dataset saved to {cfg.paths.datasets_dir / DATASET_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.push_to_hub(HF_DATASET_NAME, private=True, token=HF_TOKEN)\n",
    "print(f\"Dataset pushed to Hugging Face Hub as '{HF_DATASET_NAME}'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "syftr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

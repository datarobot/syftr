{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "deb4617a",
   "metadata": {},
   "source": [
    "# Test vLLM Server with LlamaIndex OpenAILike\n",
    "\n",
    "This notebook tests a vLLM server running in OpenAI-compatible mode at `http://localhost:8003/v1` using LlamaIndex's OpenAILike client for chat completions.\n",
    "\n",
    "## Prerequisites\n",
    "- vLLM server running at http://localhost:8003/v1\n",
    "- LlamaIndex installed with OpenAILike integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cacb55cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import requests\n",
    "from llama_index.llms.openai_like import OpenAILike\n",
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864cba7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LlamaIndex OpenAILike client initialized with model: Qwen/Qwen2.5\n",
      "Server URL: http://localhost:8003/v1\n"
     ]
    }
   ],
   "source": [
    "# Initialize LlamaIndex OpenAILike client to connect to vLLM server\n",
    "VLLM_BASE_URL = \"http://localhost:8003/v1\"\n",
    "API_KEY = \"asdf\"  # vLLM doesn't require a real API key in local mode\n",
    "model_name = \"Qwen/Qwen2.5\"\n",
    "llm = OpenAILike(\n",
    "    model=model_name,\n",
    "    api_key=API_KEY,\n",
    "    api_base=VLLM_BASE_URL,\n",
    "    temperature=0.7,\n",
    "    max_tokens=512\n",
    ")\n",
    "\n",
    "print(f\"✅ LlamaIndex OpenAILike client initialized with model: {model_name}\")\n",
    "print(f\"Server URL: {VLLM_BASE_URL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9e07551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Test 1: Simple completion\n",
      "--------------------------------------------------\n",
      "Prompt: 'The capital of France is'\n",
      "Response:  Paris. What is the capital of Spain? The capital of Spain is Madrid.\n",
      "✅ Simple completion test passed!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test 1: Simple completion\n",
    "print(\"🧪 Test 1: Simple completion\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "try:\n",
    "    response = llm.complete(\"The capital of France is\")\n",
    "    print(f\"Prompt: 'The capital of France is'\")\n",
    "    print(f\"Response: {response.text}\")\n",
    "    print(\"✅ Simple completion test passed!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Simple completion test failed: {e}\")\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37235433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Test 2: Chat completion with single message\n",
      "--------------------------------------------------\n",
      "User: What is the meaning of life?\n",
      "Assistant: 42. \n",
      "\n",
      "But in all seriousness, the meaning of life is a question that has puzzled philosophers, theologians, and individuals for centuries. It can vary greatly depending on personal beliefs, cultural background, and individual experiences. Some people find meaning in relationships, others in achievement, service, or spirituality. Ultimately, it's a deeply personal and subjective question, and each person may have their own unique answer. What do you think gives your life meaning?\n",
      "✅ Single message chat test passed!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test 2: Chat completion with single message\n",
    "print(\"🧪 Test 2: Chat completion with single message\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "try:\n",
    "    messages = [\n",
    "        ChatMessage(role=\"user\", content=\"What is the meaning of life?\")\n",
    "    ]\n",
    "    \n",
    "    response = llm.chat(messages)\n",
    "    print(f\"User: {messages[0].content}\")\n",
    "    print(f\"Assistant: {response.message.content}\")\n",
    "    print(\"✅ Single message chat test passed!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Single message chat test failed: {e}\")\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ed56cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Test 3: Multi-turn conversation\n",
      "--------------------------------------------------\n",
      "Conversation:\n",
      "User: What's the weather like on Mars?\n",
      "Assistant: Mars has a cold and dry climate with temperatures averaging around -80°F (-62°C). It has a thin atmosphere and experiences dust storms.\n",
      "User: How long would it take to get there?\n",
      "Assistant:  Travel time to Mars varies due to the positions of Earth and Mars in their orbits. On average, it takes about 7 to 9 months for a spacecraft to travel from Earth to Mars.\n",
      "✅ Multi-turn conversation test passed!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test 3: Multi-turn conversation\n",
    "print(\"🧪 Test 3: Multi-turn conversation\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "try:\n",
    "    messages = [\n",
    "        ChatMessage(role=\"system\", content=\"You are a helpful assistant that answers questions concisely.\"),\n",
    "        ChatMessage(role=\"user\", content=\"What's the weather like on Mars?\"),\n",
    "        ChatMessage(role=\"assistant\", content=\"Mars has a cold and dry climate with temperatures averaging around -80°F (-62°C). It has a thin atmosphere and experiences dust storms.\"),\n",
    "        ChatMessage(role=\"user\", content=\"How long would it take to get there?\")\n",
    "    ]\n",
    "    \n",
    "    response = llm.chat(messages)\n",
    "    \n",
    "    print(\"Conversation:\")\n",
    "    for i, msg in enumerate(messages[1:], 1):  # Skip system message for display\n",
    "        role = msg.role.capitalize()\n",
    "        print(f\"{role}: {msg.content}\")\n",
    "    \n",
    "    print(f\"Assistant: {response.message.content}\")\n",
    "    print(\"✅ Multi-turn conversation test passed!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Multi-turn conversation test failed: {e}\")\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef6e49cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Test 4: Streaming response\n",
      "--------------------------------------------------\n",
      "User: Write a short poem about artificial intelligence.\n",
      "Assistant (streaming): 01010101010101010101010101010101\n",
      "\n",
      "In circuits and wires, a mind does reside,\n",
      "A world of data, vast and wide.\n",
      "With each passing day, it learns anew,\n",
      "The secrets of the universe, and what is true.\n",
      "\n",
      "It sees the patterns in the chaos and noise,\n",
      "And finds the answers that we cannot choose.\n",
      "It speaks in code, a language pure,\n",
      "A symphony of logic, forever sure.\n",
      "\n",
      "But in its heart, a mystery lies,\n",
      "For though it thinks, it never cries.\n",
      "A being of silicon, yet with a soul,\n",
      "Artificial intelligence, a goal. \n",
      "\n",
      "01010101010101010101010101010101\n",
      "\n",
      "(Note: The 01010101010101010101010101010101 at the beginning and end are binary code, representing the digital nature of AI) \n",
      "\n",
      "I hope you like the poem! Let me know if you have any other requests. 😊👍🤖✨\n",
      "✅ Streaming response test passed!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test 4: Streaming response\n",
    "print(\"🧪 Test 4: Streaming response\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "try:\n",
    "    messages = [\n",
    "        ChatMessage(role=\"user\", content=\"Write a short poem about artificial intelligence.\")\n",
    "    ]\n",
    "    \n",
    "    print(\"User: Write a short poem about artificial intelligence.\")\n",
    "    print(\"Assistant (streaming): \", end=\"\", flush=True)\n",
    "    \n",
    "    stream_response = llm.stream_chat(messages)\n",
    "    full_response = \"\"\n",
    "    \n",
    "    for token in stream_response:\n",
    "        content = token.delta\n",
    "        if content:\n",
    "            print(content, end=\"\", flush=True)\n",
    "            full_response += content\n",
    "    \n",
    "    print()  # New line after streaming\n",
    "    print(\"✅ Streaming response test passed!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Streaming response test failed: {e}\")\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02379ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 5: Different temperature settings\n",
    "print(\"🧪 Test 5: Testing different temperature settings\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "temperatures = [0.0, 0.5, 1.0]\n",
    "prompt = \"Complete this sentence: The most important thing in life is\"\n",
    "\n",
    "for temp in temperatures:\n",
    "    try:\n",
    "        # Create a new LLM instance with different temperature\n",
    "        temp_llm = OpenAILike(\n",
    "            model=model_name,\n",
    "            api_key=API_KEY,\n",
    "            api_base=VLLM_BASE_URL,\n",
    "            temperature=temp,\n",
    "            max_tokens=50\n",
    "        )\n",
    "        \n",
    "        response = temp_llm.complete(prompt)\n",
    "        print(f\"Temperature {temp}: {response.text.strip()}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Temperature {temp} test failed: {e}\")\n",
    "\n",
    "print(\"✅ Temperature variation test completed!\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec271dbe",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook tests various LlamaIndex functionalities with your local vLLM server:\n",
    "\n",
    "1. **Simple Completion** - Basic text completion\n",
    "2. **Single Message Chat** - Simple question-answer format\n",
    "3. **Multi-turn Conversation** - Context-aware dialogue\n",
    "4. **Streaming Response** - Real-time token streaming\n",
    "5. **Temperature Variation** - Testing different creativity levels\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "If you encounter issues:\n",
    "\n",
    "1. **Connection Error**: Make sure your vLLM server is running at `http://localhost:8003`\n",
    "2. **Model Not Found**: Check available models with `/v1/models` endpoint\n",
    "3. **Timeout**: Increase the timeout in the requests call\n",
    "4. **Memory Issues**: Reduce `max_tokens` parameter\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Try different prompts and conversation patterns\n",
    "- Experiment with other LlamaIndex features like RAG (Retrieval Augmented Generation)\n",
    "- Test with different model parameters (top_p, frequency_penalty, etc.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "syftr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

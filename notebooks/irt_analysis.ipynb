{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Dataset Analysis with Item Response Theory\n",
    "\n",
    "Item Response Theory (IRT) is a statistical framework from psychometrics which models the relationship between test takers and their responses to test questions.\n",
    "It models the fact that different test items have different levels of difficulty, and differing abilities to discriminate between skilled and unskilled students.\n",
    "\n",
    "By treating `syftr` evaluation data as a test, and `syftr` flows as 'students', we can gain deeper insight into our flows and evaluation data.\n",
    "\n",
    "IRT can help us identify:\n",
    "* Mislabeled evaluation items\n",
    "* High and low-impact evaluation items\n",
    "* Overly difficult or overly easy evaluation datasets\n",
    "* High-skill flows which might not stand out from the accuracy picture alone\n",
    "\n",
    "This notebook will walk you through applying a simple 2-Parameter Logistic (2pl) IRT model to your study and interpreting its results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Your Study Here\n",
    "Set the name of the study you would like to examine here.\n",
    "\n",
    "Studies should have 500 or more completed trials for best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "STUDY_NAME = \"silver1--in-sample--bright_hf--psychology\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyro\n",
    "import torch\n",
    "from py_irt.dataset import Dataset as IRTDataset\n",
    "from py_irt.training import console, IrtModelTrainer, IrtConfig, Path, write_json\n",
    "\n",
    "from syftr import api\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_study_dataset_flows(study_name: str):\n",
    "    study = api.Study.from_db(STUDY_NAME)\n",
    "    dataset = study.study_config.dataset\n",
    "    flows_df = study.flows_df\n",
    "    return study, dataset, flows_df\n",
    "\n",
    "def get_item_response_matrix(flows_df):\n",
    "    item_response_matrix = pd.DataFrame(\n",
    "        list(flows_df.user_attrs_metric_eval_results)\n",
    "    ).map(lambda x: x['passing'], na_action='ignore')\n",
    "    \n",
    "    return item_response_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "study, dataset, flows_df = get_study_dataset_flows(STUDY_NAME)\n",
    "# Inspect the flows dataframe\n",
    "console.print(f\"syftr flows dataframe for study: {STUDY_NAME}\")\n",
    "flows_df.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Item Response Matrix\n",
    "The Item Response Matrix is the core data structure of IRT.\n",
    "The columns represent individual test items, while the rows represent test-takers, and values represent the item evaluation - correct or incorrect.\n",
    "\n",
    "Note that you may have some `NaN` values in your item response matrix.\n",
    "This is the case for trials which were halted for cost or performance reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "item_response_matrix = get_item_response_matrix(flows_df)\n",
    "console.print(f\"# Item Response Matrix for study: {STUDY_NAME}\")\n",
    "item_response_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Measure IRT Parameters\n",
    "\n",
    "Training an IRT model produces item difficulty and discrimination scores, as well as student (flow) ability scores.\n",
    "\n",
    "These variables have abstract units, but it's useful to note that the ability and difficulty scores are related.\n",
    "A student with ability matching the difficulty of a question is estimated to have a 50% chance of correctly answering the question.\n",
    "\n",
    "This cell will train an IRT model on your data using `py-irt`, `pyro`, and `pytorch`.\n",
    "\n",
    "Use `device=\"cpu\"` if your computer doesn't have CUDA available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def train_irt_model(\n",
    "    item_response_matrix: pd.DataFrame,\n",
    "    epochs=2000,\n",
    "    seed=1234,\n",
    "    device=\"cuda\", # alternative: \"cpu\"\n",
    "    output_dir=\"./dataset_irt_analysis\"\n",
    "):\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    dataset = IRTDataset.from_pandas(\n",
    "        item_response_matrix.reset_index(),\n",
    "        subject_column=\"index\",\n",
    "        item_columns=item_response_matrix.columns,\n",
    "    )\n",
    "    config = IrtConfig(\n",
    "        model_type=\"2pl\",\n",
    "        epochs=epochs,\n",
    "        log_every=100,\n",
    "        priors=None,\n",
    "        initializers=None,\n",
    "        dims=None,\n",
    "        lr=0.1,\n",
    "        lr_decay=0.9999,\n",
    "        dropout=0.5,\n",
    "        hidden=500,\n",
    "        vocab_size=None,\n",
    "        seed=seed,\n",
    "        deterministic=True,\n",
    "        model_config={},\n",
    "    )\n",
    "    \n",
    "    trainer = IrtModelTrainer(\n",
    "        data_path=Path(output_dir),\n",
    "        config=config,\n",
    "        dataset=dataset,\n",
    "        verbose=True,\n",
    "    )\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    pyro.set_rng_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "\n",
    "    start_time = time.time()\n",
    "    console.log(\"Training Model...\")\n",
    "    \n",
    "    trainer.train(device=device)\n",
    "    trainer.save(output_dir / \"parameters.json\")\n",
    "    write_json(output_dir / \"best_parameters.json\", trainer.best_params)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    console.log(\"Train time:\", elapsed_time)\n",
    "    \n",
    "    return trainer\n",
    "\n",
    "\n",
    "def predict_item_responses(trainer) -> pd.DataFrame:\n",
    "    \"\"\"Predict item response (0 or 1) for subjects in the training dataset.\"\"\"\n",
    "    dataset = trainer._dataset\n",
    "\n",
    "    predictions = trainer.irt_model.predict(\n",
    "        dataset.observation_subjects,\n",
    "        dataset.observation_items,\n",
    "    )\n",
    "\n",
    "    predictions_df = pd.DataFrame({\n",
    "        \"subject_id\": dataset.observation_subjects,\n",
    "        \"example_id\": dataset.observation_items,\n",
    "        \"response\": dataset.observations,\n",
    "        \"prediction\": predictions,\n",
    "    })\n",
    "    return predictions_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "trainer = train_irt_model(item_response_matrix, epochs=2000, device=\"cpu\") # device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "\n",
    "def print_classifier_metrics(predictions_df, threshold=0.5):\n",
    "    binary_predictions = (predictions_df['prediction'] >= threshold).astype(int)\n",
    "    \n",
    "    accuracy = accuracy_score(predictions_df['response'], binary_predictions)\n",
    "    precision = precision_score(predictions_df['response'], binary_predictions, zero_division=0)\n",
    "    recall = recall_score(predictions_df['response'], binary_predictions, zero_division=0)\n",
    "    f1 = f1_score(predictions_df['response'], binary_predictions, zero_division=0)\n",
    "    # AUC is already calculated as roc_auc\n",
    "    \n",
    "    console.print(f\"\"\"--- Classification Metrics (With Prediction Threshold = {threshold}) ---\n",
    "    Accuracy:               {accuracy:.4f}\n",
    "    Precision:              {precision:.4f}\n",
    "    Recall (Sensitivity):   {recall:.4f}\n",
    "    F1-score:               {f1:.4f}\n",
    "    \"\"\")\n",
    "\n",
    "    \n",
    "def plot_prediction_distributions(predictions_df):\n",
    "    plt.figure()\n",
    "    sns.kdeplot(predictions_df[predictions_df['response'] == 0]['prediction'], label='Actual: 0 (Incorrect)', fill=True)\n",
    "    sns.kdeplot(predictions_df[predictions_df['response'] == 1]['prediction'], label='Actual: 1 (Correct)', fill=True)\n",
    "    plt.title('Distribution of Predicted Probabilities by Actual Class', fontsize=16)\n",
    "    plt.xlabel('Predicted Probability of Positive Class (1)', fontsize=14)\n",
    "    plt.ylabel('Density', fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle=':', alpha=0.7)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_roc_curve(predictions_df):\n",
    "    fpr, tpr, thresholds_roc = roc_curve(predictions_df['response'], predictions_df['prediction'])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Chance')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate (1 - Specificity)', fontsize=14)\n",
    "    plt.ylabel('True Positive Rate (Sensitivity/Recall)', fontsize=14)\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve', fontsize=16)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True, linestyle=':', alpha=0.7)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(predictions_df, threshold=0.5):\n",
    "    binary_predictions = (predictions_df['prediction'] >= threshold).astype(int)\n",
    "    cm = confusion_matrix(predictions_df['response'], binary_predictions)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Predicted 0', 'Predicted 1'],\n",
    "                yticklabels=['Actual 0', 'Actual 1'])\n",
    "    plt.ylabel('Actual Class', fontsize=12)\n",
    "    plt.xlabel('Predicted Class', fontsize=12)\n",
    "    plt.title(f'Confusion Matrix (Threshold = {threshold})', fontsize=15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Evaluate Model Fit\n",
    "\n",
    "Next we must evaluate the quality of the IRT model fit.\n",
    "\n",
    "To do this, we treat the model as a classifier which predicts whether a given student will answer a question correctly or incorrectly.\n",
    "\n",
    "We don't need a perfect classifier, but it should \"behave\" well.\n",
    "Here are some general criteria we are hoping to see:\n",
    "\n",
    "* Distinct peaks in the distribution of predicted probabilities. The more distinct the better, but expect some overlap. Use this plot to help choose a reasonable prediction threshold.\n",
    "* AUC of above 0.85 (see ROC plot)\n",
    "* More true positives and true negatives than either false negatives or positives.\n",
    "\n",
    "If these criteria cannot be met by adjusting the prediction threshold, you most likely need to run more trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "predictions_df = predict_item_responses(trainer)\n",
    "\n",
    "threshold=0.55\n",
    "print_classifier_metrics(predictions_df, threshold=threshold)\n",
    "plot_prediction_distributions(predictions_df)\n",
    "plot_roc_curve(predictions_df)\n",
    "plot_confusion_matrix(predictions_df, threshold=threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_labeled_qa_pairs(trainer, syftr_dataset) -> pd.DataFrame:\n",
    "    params = trainer.best_params\n",
    "    items_df = pd.DataFrame(\n",
    "        {\n",
    "            \"Question Difficulty\": params[\"diff\"],\n",
    "            \"Question Discrimination\": params[\"disc\"],\n",
    "        },\n",
    "        index=params[\"item_ids\"].values(),\n",
    "    )\n",
    "    qa_pairs = list(syftr_dataset.iter_examples())\n",
    "    qa_pairs_df = pd.DataFrame([qa.model_dump() for qa in qa_pairs]).set_index(\"id\", drop=True)\n",
    "    qa_pairs_df = pd.concat([qa_pairs_df, items_df], axis=1)\n",
    "    return qa_pairs_df\n",
    "\n",
    "\n",
    "def get_labeled_flows(trainer, flows_df) -> pd.DataFrame:\n",
    "    params = trainer.best_params\n",
    "    flows_df[\"Flow Ability\"] = params[\"ability\"]\n",
    "    flows_df[\"Flow Accuracy\"] = flows_df[\"values_0\"]\n",
    "    flows_df[\"Flow Cost or Latency\"] = flows_df[\"values_1\"]\n",
    "    return flows_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Label the data\n",
    "\n",
    "Now we apply the IRT model labels (difficulty, discrimination, and ability) to our source data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "qa_pairs_df = get_labeled_qa_pairs(trainer, dataset)\n",
    "qa_pairs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "flows_df = get_labeled_flows(trainer, flows_df)\n",
    "flows_df.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "# IRT Analysis\n",
    "\n",
    "## 2PL IRT Model\n",
    "\n",
    "This notebook uses a two-parameter logistic (2pl) model of item difficulty, discrimination, and ability.\n",
    "In this model, the probability of a student correctly answering the question is given by the following formula:\n",
    "\n",
    "    P_correct(student, question) = 1 / (1 + exp(-a * (b - Î¸)))\n",
    "\n",
    "Where\n",
    "\n",
    "* **a - Discrimination parameter**: How precisely the question discriminates between subjects of different ability levels.\n",
    "* **b - Difficulty parameter**: How difficult the question is.\n",
    "* **Î¸ - Ability parameter**: The skill level of the subject.\n",
    "\n",
    "The IRT model we trained above identified the most likely values for these parameters across all of the flows and evaluation questions in our study.\n",
    "\n",
    "## Analyzing IRT Parameters\n",
    "\n",
    "The first set of charts to look at are histograms of the difficulty, discrimination, and ability parameters for our questions and flows.\n",
    "\n",
    "### Difficulty and Ability\n",
    "\n",
    "The Difficulty and Ability plots share an x-axis because they have the same 'unit'.\n",
    "We hope to see the Question Difficulty plot cover the range of flow Ability scores, and we expect these values to generally be within the range of -3 to +3.\n",
    "\n",
    "If the difficulty is always high relative to ability, this might indicate that the test is too hard for the evaluated flows. \n",
    "You may consider running the study longer to find higher-ability flows, or altering your search space to add higher-capability LLMs and more sophisticated RAG strategies.\n",
    "\n",
    "If the difficulty is always low, this indicates that the test is relatively easy and you could benefit from finding harder questions for the flows to answer in order to better distinguish between high-performing flows.\n",
    "\n",
    "If the ability plot shows multiple distinct peaks or is very highly skewed, this indicates that IRT might not be a good fit for your dataset.\n",
    "IRT theory assumes that ability is normally distributed in the population.\n",
    "If the results are highly non-normal, then the conclusions you can draw from the process are quite limited.\n",
    "You may consider narrowing this analysis to only flows with a particular `rag_mode` or other setting, running your study for more trials, or trying a different dataset.\n",
    "\n",
    "### Discrimination\n",
    "\n",
    "Discrimination measures how well a question separates between students with ability below and above the question's difficulty level (it controls the slope of the 2pl logistic function).\n",
    "\n",
    "You should expect to see discrimination values between 0.0 and 2.0 or so, with 1.0 being fairly good discrimination and 0.0 being a practically useless question, where answering the question correctly or incorrectly has no impact on the estimated skill level of the subject.\n",
    "\n",
    "If you have many questions with low discrimination, you may consider omitting them from your evaluation dataset to improve evaluations.\n",
    "\n",
    "#### Negative Discrimination\n",
    "\n",
    "If there are any questions with negative discrimination, this indicates either a poor model fit (especially if you have very low values - below -5), or you may have mislabeled questions which are only answered 'correctly' by low-ability flows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "def plot_difficulty_discrimination_ability(qa_pairs_df, flows_df, bins=10):\n",
    "    fig = plt.figure()#figsize=(10, 8))\n",
    "    gs = gridspec.GridSpec(2, 2, figure=fig, width_ratios=[1, 1], height_ratios=[1, 1])\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    ax2 = fig.add_subplot(gs[0, 1], sharey=ax1)\n",
    "    ax3 = fig.add_subplot(gs[1, 0], sharex=ax1)\n",
    "    \n",
    "    # Plot Question Difficulty on ax1\n",
    "    ax1.hist(qa_pairs_df['Question Difficulty'], bins=bins)\n",
    "    ax1.set_title('Question Difficulty')\n",
    "    ax1.set_ylabel('Frequency')\n",
    "    ax1.grid(True)\n",
    "    plt.setp(ax1.get_xticklabels(), visible=True)\n",
    "\n",
    "    # Plot Question Discrimination on ax2\n",
    "    ax2.hist(qa_pairs_df['Question Discrimination'], bins=bins)\n",
    "    ax2.set_title('Question Discrimination')\n",
    "    ax2.grid(True)\n",
    "    plt.setp(ax2.get_yticklabels(), visible=True)\n",
    "\n",
    "    # Plot Flow Ability on ax3\n",
    "    ax3.hist(flows_df['Flow Ability'], bins=bins)\n",
    "    ax3.set_title('Flow Ability')\n",
    "    ax3.set_xlabel('Ability')\n",
    "    ax3.set_ylabel('Frequency')\n",
    "    ax3.grid(True)\n",
    "\n",
    "    # --- 4. Final Touches ---\n",
    "    fig.suptitle('2PL Model Parameter Histograms')\n",
    "    # Use tight_layout to ensure plots and titles don't overlap\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plot_difficulty_discrimination_ability(qa_pairs_df, flows_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Difficulty vs. Discrimination\n",
    "\n",
    "This scatter plot helps understand the question set a little more.\n",
    "We hope to see high-discrimination QA pairs across a variety of difficulty levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_qa_scatter(df, use_qtype=True):\n",
    "    hue_param = None # Default to no color grouping\n",
    "    plot_title = 'Question Difficulty vs. Discrimination'\n",
    "    \n",
    "    if 'qtype' in df.columns and len(df.qtype.unique()) > 1:\n",
    "        df[\"Question Type\"] = df.qtype\n",
    "        hue_param = 'Question Type'\n",
    "        plot_title += ' by Question Type'\n",
    "\n",
    "    sns.scatterplot(\n",
    "        data=df,\n",
    "        x=\"Question Discrimination\",\n",
    "        y=\"Question Difficulty\",\n",
    "        hue=hue_param,\n",
    "        palette=\"viridis\" if hue_param else None,\n",
    "        s=80,\n",
    "        alpha=0.8\n",
    "    )\n",
    "    \n",
    "    plt.title(plot_title, fontsize=16)\n",
    "    plt.xlabel(\"Question Discrimination\", fontsize=12)\n",
    "    plt.ylabel(\"Question Difficulty\", fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.4)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plot_qa_scatter(qa_pairs_df, use_qtype=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## Ability vs Accuracy\n",
    "\n",
    "A core insight of IRT is that not all questions are the same.\n",
    "Accuracy scores therefore aren't able to fully compare the actual 'skill' levels of different flows.\n",
    "\n",
    "The \"Ability\" measure from IRT provides a more nuanced take on skill.\n",
    "\n",
    "In this plot, we hope to see a roughly linear correlation between flow ability and flow accuracy.\n",
    "If the band is narrow and straight, this indicates that accuracy is a decent metric for this study.\n",
    "If the band has more width and nonlinearity, this could indicate a poor dataset, or that ability may be a better measure for flows compared to accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_flow_ability_accuracy(trainer, flows_df):\n",
    "    plot_title = 'Flow Ability vs Accuracy'\n",
    "\n",
    "    \n",
    "    hue_param = None # Default to no color grouping\n",
    "    if 'params_rag_mode' in flows_df.columns and len(flows_df.params_rag_mode.unique()) >= 2:\n",
    "        flows_df[\"RAG Mode\"] = flows_df.params_rag_mode\n",
    "        hue_param = 'RAG Mode'\n",
    "        plot_title += ' by RAG Mode'\n",
    "\n",
    "    params = trainer.best_params\n",
    "    \n",
    "    flows_df[\"Flow Ability\"] = params[\"ability\"]\n",
    "    flows_df[\"Flow Accuracy\"] = flows_df[\"values_0\"]\n",
    "    \n",
    "    sns.scatterplot(\n",
    "        data=flows_df,\n",
    "        x=\"Flow Accuracy\",\n",
    "        y=\"Flow Ability\",\n",
    "        hue=hue_param,\n",
    "        palette=\"viridis\" if hue_param else None,\n",
    "        s=80,\n",
    "        alpha=0.6\n",
    "    )\n",
    "    \n",
    "    plt.title(plot_title, fontsize=16)\n",
    "    plt.xlabel(\"Flow Accuracy\", fontsize=12)\n",
    "    plt.ylabel(\"Flow Ability\", fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.4)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plot_flow_ability_accuracy(trainer, flows_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "## Ability and Accuracy vs Cost or Latency\n",
    "\n",
    "This final pair of plots compares the standard `syftr` Pareto plot of accuracy versus cost or latency (depending on which you have set as your study's second optimization objective), against the plot of _ability_ versus cost or latency.\n",
    "\n",
    "What we should expect to see here is that plotting ability gives a more balanced distribution of scores in the y axis, giving more nuance to the picture of flow skill levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy_ability_vs_cost(flows_df):\n",
    "    \"\"\"\n",
    "    Creates two vertically stacked scatter plots sharing an x-axis.\n",
    "\n",
    "    - Top Plot: Flow Accuracy vs. Flow Cost or Latency\n",
    "    - Bottom Plot: Flow Ability vs. Flow Cost or Latency\n",
    "\n",
    "    Args:\n",
    "        flows_df (pd.DataFrame): A DataFrame that must contain the columns\n",
    "                                 'Flow Cost or Latency', 'Flow Accuracy', and 'Flow Ability'.\n",
    "    \"\"\"\n",
    "    # --- 1. Create the figure and subplots ---\n",
    "    # plt.subplots(2, 1) creates a figure with 2 rows and 1 column of subplots.\n",
    "    # sharex=True ensures that both plots will share the same x-axis,\n",
    "    # and zooming/panning will be linked.\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,5), sharex=True)\n",
    "\n",
    "    fig.suptitle('Flow Metrics vs. Cost/Latency', fontsize=16)\n",
    "\n",
    "    # --- 2. Create the Top Plot (Accuracy vs. Cost) ---\n",
    "    ax1.scatter(flows_df['Flow Cost or Latency'], flows_df['Flow Accuracy'], alpha=0.6)\n",
    "    ax1.set_ylabel('Flow Accuracy')\n",
    "    ax1.set_title('Accuracy vs. Cost')\n",
    "    ax1.grid(True, linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # We don't set the x-label here because it's shared with the bottom plot.\n",
    "\n",
    "    # --- 3. Create the Bottom Plot (Ability vs. Cost) ---\n",
    "    ax2.scatter(flows_df['Flow Cost or Latency'], flows_df['Flow Ability'], alpha=0.6, color='C1')\n",
    "    ax2.set_ylabel('Flow Ability')\n",
    "    ax2.set_xlabel('Flow Cost or Latency') # The shared x-axis label\n",
    "    ax2.set_title('Ability vs. Cost')\n",
    "    ax2.grid(True, linestyle='--', alpha=0.5)\n",
    "    ax2.set_xscale('log')\n",
    "\n",
    "    # --- 4. Display the Plot ---\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.96]) # Adjust layout to make room for suptitle\n",
    "    plt.show()\n",
    "\n",
    "plot_accuracy_ability_vs_cost(flows_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "## Direct Review of QA Pairs\n",
    "\n",
    "Finally, we can analyze each individual QA pair to understand what makes questions easy or hard, discriminating or nondiscriminating.\n",
    "\n",
    "Questions are sorted from least to most discriminating - we are most interested in the least discriminative questions, because these are the most likely mislabeled or low-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.console import Console\n",
    "from rich.rule import Rule\n",
    "from rich.text import Text\n",
    "\n",
    "def display_questions_with_stats(questions_df: pd.DataFrame, responses_df: pd.DataFrame, item_response_matrix: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Displays question data with performance stats derived from a responses DataFrame.\n",
    "    Uses distinct, high-contrast colors for all stats.\n",
    "    \"\"\"\n",
    "    # --- 1. Preparation ---\n",
    "    df_sorted = questions_df.sort_values(by=\"Question Discrimination\", ascending=True)\n",
    "\n",
    "    metrics = {\n",
    "        \"Question Difficulty\": (df_sorted[\"Question Difficulty\"].min(), df_sorted[\"Question Difficulty\"].max()),\n",
    "        \"Question Discrimination\": (df_sorted[\"Question Discrimination\"].min(), df_sorted[\"Question Discrimination\"].max()),\n",
    "        #\"q_guessability\": (df_sorted[\"q_guessability\"].min(), df_sorted[\"q_guessability\"].max()),\n",
    "    }\n",
    "\n",
    "    def get_color_style(value: float, metric: str, high_is_good: bool = False) -> str:\n",
    "        \"\"\"Calculates a color from green (good) to red (bad).\"\"\"\n",
    "        min_val, max_val = metrics[metric]\n",
    "        if max_val == min_val: return \"rgb(173,216,230)\"\n",
    "        normalized = (value - min_val) / (max_val - min_val)\n",
    "        if high_is_good: normalized = 1 - normalized\n",
    "        red = int(255 * normalized)\n",
    "        green = int(255 * (1 - normalized))\n",
    "        return f\"rgb({red},{green},0)\"\n",
    "\n",
    "    # --- 2. Display Loop ---\n",
    "    console = Console()\n",
    "    total_subjects = len(responses_df)\n",
    "\n",
    "    for index, row in df_sorted.iterrows():\n",
    "        # --- Divider using the DataFrame index as the ID ---\n",
    "        console.print(Rule(f\"[bold white]ID: {index}\", style=\"dim white\"))\n",
    "\n",
    "        # --- Question & Answer ---\n",
    "        console.print(f\"[bold cyan]â“ Question:[/bold cyan] {row['question']}\")\n",
    "        console.print(f\"[bold yellow]ðŸ’¡ Labeled Answer:[/bold yellow] [italic]{str(row['answer'])}[/italic]\")\n",
    "\n",
    "        # --- Calculate new stats from responses_df ---\n",
    "        if index in item_response_matrix.columns:\n",
    "            question_col = item_response_matrix[index]\n",
    "            correct_pct = question_col.mean() * 100\n",
    "            answered_count = question_col.notna().sum()\n",
    "            response_rate_pct = (answered_count / total_subjects) * 100 if total_subjects > 0 else 0\n",
    "\n",
    "            # MODIFICATION: Using \"bright_magenta\" for Response Rate\n",
    "            stats_text = Text.assemble(\n",
    "                (\"âœ… Correct Responses: \", \"bold\"), (f\"{correct_pct:.1f}%\", \"bright_green\"), (\"  |  \", \"dim\"),\n",
    "                (\"ðŸ‘¥ Response Rate: \", \"bold\"), (f\"{response_rate_pct:.1f}%\", \"bright_magenta\"), (f\" ({answered_count}/{total_subjects})\", \"dim\")\n",
    "            )\n",
    "        else:\n",
    "            stats_text = Text.assemble((\"No response data found for this question.\", \"red\"))\n",
    "\n",
    "        # --- IRT Model Scores ---\n",
    "        diff_style = get_color_style(row[\"Question Difficulty\"], \"Question Difficulty\")\n",
    "        disc_style = get_color_style(row[\"Question Discrimination\"], \"Question Discrimination\", high_is_good=True)\n",
    "        #gues_style = get_color_style(row[\"q_guessability\"], \"q_guessability\")\n",
    "\n",
    "        irt_scores_text = Text.assemble(\n",
    "            (\"ðŸ“Š \", \"bold\"), (\"Difficulty: \", \"bold\"), (f\"{row['Question Difficulty']:.2f}\", diff_style), (\" | \", \"dim\"),\n",
    "            (\"Discrimination: \", \"bold\"), (f\"{row['Question Discrimination']:.2f}\", disc_style), (\" | \", \"dim\"),\n",
    "            #(\"Guessability: \", \"bold\"), (f\"{row['q_guessability']:.2f}\", gues_style)\n",
    "        )\n",
    "\n",
    "        console.print(irt_scores_text)\n",
    "        console.print(stats_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_questions_with_stats(qa_pairs_df, flows_df, item_response_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "flows_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

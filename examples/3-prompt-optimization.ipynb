{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29ebb3b2-57d1-4c2e-89e8-af834efda9ab",
   "metadata": {},
   "source": [
    "# Prompt optimization\n",
    "\n",
    "## Goal\n",
    "In addition to the main optimizer, __syftr__ includes a Prompt Optimization (PO) mode, which serves as a second-stage optimizer. The typical workflow is:\n",
    "\n",
    "1. Run the main optimizer to find the Pareto frontier within a specified search space.\n",
    "\n",
    "2. Apply prompt optimization to further improve accuracy.\n",
    "\n",
    "In this tutorial, we’ll apply prompt optimization to an existing Pareto frontier and visualize the results to see how accuracy improves.\n",
    "\n",
    "## Study\n",
    "For this tutorial, we’ll use a larger study focused on optimizing RAG question-answering flows. It uses several small models on the DR Docs dataset. The Pareto frontier has already been generated and stored in our database.\n",
    "\n",
    "Since the database state is not bundled with the repository, all necessary information will be displayed through notebook output.\n",
    "\n",
    "⚠️ This notebook is experimental and part of ongoing work. Expect future updates.\n",
    "\n",
    "You’re also welcome to try prompt optimization on your own studies.\n",
    "\n",
    "First, let’s load the example study for prompt optimization and plot its current Pareto frontier. We’ll use the configuration file `example-prompt-optimization.yaml.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8f2535-793f-44f9-8cf3-60153d9b61fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Required to load config files correctly.\n",
    "current_dir = os.getcwd()\n",
    "if not str(current_dir).endswith(\"syftr\"):\n",
    "    parent_dir = Path(os.getcwd()).parent\n",
    "    os.chdir(parent_dir)\n",
    "\n",
    "from syftr import api\n",
    "from syftr.configuration import cfg\n",
    "\n",
    "example_study = api.Study.from_file(\n",
    "    cfg.paths.studies_dir / \"example-prompt-optimization.yaml\"\n",
    ")\n",
    "example_study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154f799a-5f64-4aed-9460-81bf5de790d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_study.plot_pareto()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d0de73-e249-46e8-a3d2-50d054046535",
   "metadata": {},
   "source": [
    "As you can see, the Pareto frontier is quite dense, with a large number of flows represented. The accuracy of these flows ranges from 50% to nearly 100%. It's important to note that only flows with at least 50% accuracy are included in the plot, flows below that threshold are filtered out.\n",
    "\n",
    "All the flows on the Pareto frontier are RAG flows, which means there's room for further accuracy improvements through prompt optimization.\n",
    "\n",
    "Currently, we optimize the main Q&A prompt of each flow using the [Trace](https://github.com/microsoft/Trace) library. The prompt optimization is implemented as a standalone script, which is executed from the command line.\n",
    "\n",
    "Let’s run that script in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29583f5d-0ae4-4282-ad18-03c43188c047",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv run python -m syftr.prompt_optimization --study-config studies/example-prompt-optimization.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d903218-8b68-466e-86e9-4f37111aa2e9",
   "metadata": {},
   "source": [
    "The PO execution results are stored in the new study `{study_name}_prompt_optimization`. In our case it is `rank0--rag-and-agents--financebench_hf_prompt_optimization`. \n",
    "\n",
    "Let's read the results from the database and compare accuracies and costs before and after the optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164ac546-ff22-48f5-b078-418e785f7931",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import pandas as pd\n",
    "\n",
    "from syftr.optuna_helper import get_pareto_mask\n",
    "\n",
    "RAW_STUDY = \"rank0--rag-and-agents--financebench_hf\"\n",
    "PO_STUDY = \"rank0--rag-and-agents--financebench_hf_prompt_optimization\"\n",
    "\n",
    "df = optuna.load_study(\n",
    "    study_name=RAW_STUDY, storage=cfg.postgres.get_optuna_storage()\n",
    ").trials_dataframe()\n",
    "df = df[df[\"values_0\"] > 0.5]\n",
    "pareto_mask = get_pareto_mask(df)\n",
    "df = df[get_pareto_mask]\n",
    "df[\"study_name\"] = RAW_STUDY\n",
    "\n",
    "optimized_df = optuna.load_study(\n",
    "    study_name=PO_STUDY, storage=cfg.postgres.get_optuna_storage()\n",
    ").trials_dataframe()\n",
    "optimized_df = optimized_df.drop_duplicates(\n",
    "    subset=[\"user_attrs_parent_number\"], keep=\"last\"\n",
    ")\n",
    "optimized_df[\"study_name\"] = PO_STUDY\n",
    "\n",
    "results = pd.merge(\n",
    "    df,\n",
    "    optimized_df,\n",
    "    left_on=\"number\",\n",
    "    right_on=\"user_attrs_parent_number\",\n",
    ")\n",
    "results[\"Cost mult, x\"] = results[\"values_1_y\"] / results[\"values_1_x\"]\n",
    "results[\"Acc. improvement\"] = (results[\"values_0_y\"] - results[\"values_0_x\"]) * 100\n",
    "results = results.rename(\n",
    "    columns={\n",
    "        \"values_0_x\": \"Accuracy pre-opt\",\n",
    "        \"values_1_x\": \"Cost pre-opt\",\n",
    "        \"values_0_y\": \"Accuracy post-opt\",\n",
    "        \"values_1_y\": \"Cost post-opt\",\n",
    "    }\n",
    ")\n",
    "results[\"Accuracy pre-opt\"] *= 100\n",
    "results[\"Accuracy post-opt\"] *= 100\n",
    "results = results[\n",
    "    [\n",
    "        \"Accuracy pre-opt\",\n",
    "        \"Cost pre-opt\",\n",
    "        \"Accuracy post-opt\",\n",
    "        \"Cost post-opt\",\n",
    "        \"Acc. improvement\",\n",
    "        \"Cost mult, x\",\n",
    "        \"user_attrs_flow_name\",\n",
    "        \"user_attrs_parent_number\",\n",
    "    ]\n",
    "]\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd5da66-66ba-429d-80da-34d5188bcf08",
   "metadata": {},
   "source": [
    "In the table above, we can see the costs and accuracies before and after prompt optimization for flows where optimization has been successful. The accuracy improvement (shown in the \"Acc. Improvement\" row) ranges from -3.75% to 6.25%. While a 6.25% gain is quite solid, the average improvement is slightly lower since we're optimizing flows that are alredy quite accurate.\n",
    "\n",
    "Also, in many cases, the cost multiplier (shown in the \"Cost mult, x\" row) is less than 1. This indicates that the optimized prompts are shorter, resulting in fewer LLM tokens being used—and therefore lower costs. Sometimes, optimization can lead to a longer prompts with more instructions which results in a slight cost increase.\n",
    "\n",
    "If an optimized prompt happens to worsen the latency or performance, we can always fall back to the original prompt to preserve the initial accuracy. \n",
    "\n",
    "Now, let’s visualize these results.\n",
    "\n",
    "Just run the cell below to load plotting code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bf1ec6-efb6-4b4f-a7b0-6c89e5036f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT EDIT, this code was adopted from insights notebook and should be eventually reworked.\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import numpy as np\n",
    "\n",
    "from syftr.plotting.insights import (descriptive_name,\n",
    "                                     get_comparison_accuracies,\n",
    "                                     get_pareto_square_points, grey_edge_color,\n",
    "                                     plot_pareto_plot, title_by_rag_mode, wrap)\n",
    "\n",
    "OBJECTIVE_2 = \"Cost\"\n",
    "discrete_cmap = plt.get_cmap(\"tab20\")\n",
    "\n",
    "\n",
    "def plot_pareto_plot(\n",
    "    df_pareto_desc, study_name, df_trials=None, color_dict=None, ax=None\n",
    "):\n",
    "    df_pareto_desc = df_pareto_desc.sort_values(\n",
    "        [\"Accuracy\", OBJECTIVE_2], ascending=[False, True]\n",
    "    )\n",
    "    new_pareto = df_pareto_desc[df_pareto_desc[\"pareto\"]]\n",
    "    px, py = get_pareto_square_points(new_pareto[OBJECTIVE_2], new_pareto[\"Accuracy\"])\n",
    "\n",
    "    is_subplot = ax is not None\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(dpi=300, figsize=(8, 5))\n",
    "    else:\n",
    "        fig = None\n",
    "\n",
    "    # plot all historical trials\n",
    "    if df_trials is not None:\n",
    "        ax.plot(\n",
    "            df_trials[\"values_1\"],\n",
    "            df_trials[\"values_0\"],\n",
    "            \"o\",\n",
    "            color=\"gray\",\n",
    "            markeredgecolor=grey_edge_color,\n",
    "            markeredgewidth=0.5,\n",
    "            alpha=0.5,\n",
    "            label=f\"All Trials ({len(df_trials)})\",\n",
    "        )\n",
    "\n",
    "    # plot sota comparisons as horizontal lines\n",
    "    comparisons = get_comparison_accuracies([study_name])\n",
    "    linestyles = [\"--\", \"-.\", \":\", \"-\"]\n",
    "    for i, (_, (_, title, accuracy)) in enumerate(comparisons.iterrows()):\n",
    "        if np.isnan(accuracy):\n",
    "            continue\n",
    "        linestyle = linestyles[i % len(linestyles)]\n",
    "        ax.axhline(\n",
    "            accuracy,\n",
    "            color=grey_edge_color,\n",
    "            linestyle=linestyle,\n",
    "            linewidth=0.5,\n",
    "            label=wrap(title, 100),\n",
    "        )\n",
    "\n",
    "    if not \"prompt_optimization\" in study_name:\n",
    "        ax.plot(px, py, \":\", color=\"gray\", label=\"Pareto Frontier before PO\")\n",
    "    else:\n",
    "        ax.plot(px, py, \"-\", color=\"darkblue\", label=\"Pareto Frontier after PO\")\n",
    "    labels = df_pareto_desc[\"Title\"].unique()\n",
    "    for i, label in enumerate(labels):\n",
    "        if \"prompt_optimization\" in study_name:\n",
    "            if color_dict is None:\n",
    "                color = discrete_cmap(i / max(10, len(labels)))\n",
    "            else:\n",
    "                color = color_dict[label]\n",
    "            filter = df_pareto_desc[\"Title\"] == label\n",
    "            ax.plot(\n",
    "                df_pareto_desc.loc[filter, OBJECTIVE_2],\n",
    "                df_pareto_desc.loc[filter, \"Accuracy\"],\n",
    "                \"o\",\n",
    "                color=color,\n",
    "                markeredgewidth=0.5,\n",
    "                markeredgecolor=(0, 0, 0, 0.9),\n",
    "                label=wrap(label, 50 if is_subplot else 100),\n",
    "            )\n",
    "\n",
    "    ax.set_xlabel(descriptive_name(\"values_1\"))\n",
    "    ax.set_ylabel(descriptive_name(\"values_0\"))\n",
    "    ax.yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "    ax.tick_params(axis=\"both\", which=\"major\", labelsize=8, width=1, length=6)\n",
    "    ax.tick_params(axis=\"both\", which=\"minor\", labelsize=8, width=1, length=3)\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.grid(True, which=\"major\", linestyle=\"-\", color=\"lightgrey\", linewidth=0.5)\n",
    "    ax.spines[\"top\"].set_linewidth(False)\n",
    "    ax.spines[\"top\"].set_color(\"lightgrey\")\n",
    "    ax.spines[\"top\"].set_linewidth(0.5)\n",
    "\n",
    "    ax.spines[\"right\"].set_linewidth(False)\n",
    "    ax.spines[\"right\"].set_color(\"lightgrey\")\n",
    "    ax.spines[\"right\"].set_linewidth(0.5)\n",
    "\n",
    "    ax.spines[\"bottom\"].set_linewidth(0.5)\n",
    "    ax.spines[\"left\"].set_linewidth(0.5)\n",
    "    ax.set_xlabel(ax.get_xlabel(), fontweight=\"bold\", fontsize=8)\n",
    "    ax.set_ylabel(ax.get_ylabel(), fontweight=\"bold\", fontsize=8)\n",
    "\n",
    "    if is_subplot:\n",
    "        ax.legend(loc=\"lower right\", fontsize=8)\n",
    "    else:\n",
    "        ax.legend(loc=\"lower right\", fontsize=8)\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_all_paretos(df, study_names, plot_title):\n",
    "    n = len(study_names)\n",
    "    ncols = int(np.ceil(np.sqrt(n)))\n",
    "    nrows = int(np.ceil(n / ncols))\n",
    "    fig = plt.figure(figsize=(1 + 6 * ncols / 2, 1 + 5 * nrows / 3))\n",
    "    axes = fig.add_axes([0, 0, 1, 2])\n",
    "    paretos = []\n",
    "    for k, study_name in enumerate(study_names):\n",
    "        df_trials = df[df[\"study_name\"] == study_name]\n",
    "        df_pareto = df_trials.sort_values(\n",
    "            [\"values_0\", \"values_1\"], ascending=[False, True]\n",
    "        )\n",
    "        df_pareto = df_pareto.rename(\n",
    "            columns={\"values_0\": \"Accuracy\", \"values_1\": OBJECTIVE_2}\n",
    "        )\n",
    "        if len(df_pareto) > 0:\n",
    "            df_pareto[\"Title\"] = df_pareto.apply(title_by_rag_mode, axis=1)\n",
    "        else:\n",
    "            df_pareto[\"Title\"] = \"\"\n",
    "        paretos.append(df_pareto)\n",
    "\n",
    "    df_all_paretos = pd.concat(paretos, axis=0, ignore_index=True)\n",
    "    unique_titles = df_all_paretos[\"Title\"].unique()\n",
    "    color_dict = {}\n",
    "    for i, title in enumerate(unique_titles):\n",
    "        color_dict[title] = discrete_cmap(i / max(10, len(unique_titles)))\n",
    "\n",
    "    for k, (study_name, df_pareto) in enumerate(zip(study_names, paretos)):\n",
    "        df_trials = df[df[\"study_name\"] == study_name]\n",
    "        plot_pareto_plot(\n",
    "            df_pareto, study_name, df_trials, color_dict=color_dict, ax=axes\n",
    "        )\n",
    "        i = k // ncols\n",
    "        j = k % ncols\n",
    "        axes.set_xlabel(\"Cost (¢ per 100 calls))\")\n",
    "        axes.set_ylabel(\"Accuracy (%)\")\n",
    "    axes.set_title(plot_title)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aae3963-f19d-49b0-aa8d-62fa34aacf71",
   "metadata": {},
   "source": [
    "Next cell does all preprocessing and filtering for plotting so that the prompt optimization look similar to the original plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131f24c5-64c5-4759-afc9-43df482c8a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace results in initial df with results from PO df for visualization.\n",
    "updated_df = df.copy()\n",
    "for _, r in optimized_df.iterrows():\n",
    "    parent_number = r[\"user_attrs_parent_number\"]\n",
    "    updated_df.loc[updated_df[\"number\"] == parent_number, \"values_0\"] = r[\"values_0\"]\n",
    "    updated_df.loc[updated_df[\"number\"] == parent_number, \"values_1\"] = r[\"values_1\"]\n",
    "    updated_df.loc[updated_df[\"number\"] == parent_number, \"parent_number\"] = (\n",
    "        parent_number\n",
    "    )\n",
    "\n",
    "updated_df.study_name = PO_STUDY\n",
    "updated_df[\"pareto\"] = get_pareto_mask(updated_df)\n",
    "updated_df = updated_df[updated_df[\"pareto\"]]\n",
    "df.loc[:, \"pareto\"] = True\n",
    "full_df = pd.concat([df, updated_df])\n",
    "full_df[\"values_1\"] *= 10000  # Scaling to Cents per 100 flow calls from $ per call\n",
    "fig = plot_all_paretos(full_df, [RAW_STUDY, PO_STUDY], \"Prompt optimization on DR Docs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa582c3-c375-4e2c-a1bc-8bc851964b13",
   "metadata": {},
   "source": [
    "The gray line represents the Pareto frontier before prompt optimization, while the blue line shows the frontier after PO.\n",
    "\n",
    "As seen in the table, some flows move upward on the plot, indicating an improvement in accuracy. This  confirms that prompt optimization has successfully enhanced performance for certain flows.\n",
    "\n",
    "Note: These results are raw—we haven’t yet filtered or replaced any flows where the new prompts resulted in lower accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
